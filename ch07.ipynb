{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 60;\n                var nbb_unformatted_code = \"%load_ext nb_black\\nimport numpy as np \\nimport matplotlib.pyplot as plt\\nimport pickle\";\n                var nbb_formatted_code = \"%load_ext nb_black\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pickle\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 61;\n                var nbb_unformatted_code = \"def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\\n    N, C, H, W = input_data.shape \\n    out_h = (H + 2 * pad - filter_h) // stride + 1 \\n    out_w = (W + 2 * pad - filter_w) // stride + 1 \\n\\n    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\\n\\n    for y in range(filter_h):\\n        y_max = y + stride * out_h\\n        for x in range(filter_w):\\n            x_max = x + stride * out_w \\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\\n\\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\\n    return col \";\n                var nbb_formatted_code = \"def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\\n    N, C, H, W = input_data.shape\\n    out_h = (H + 2 * pad - filter_h) // stride + 1\\n    out_w = (W + 2 * pad - filter_w) // stride + 1\\n\\n    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], \\\"constant\\\")\\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\\n\\n    for y in range(filter_h):\\n        y_max = y + stride * out_h\\n        for x in range(filter_w):\\n            x_max = x + stride * out_w\\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\\n\\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\\n    return col\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_data.shape \n",
    "    out_h = (H + 2 * pad - filter_h) // stride + 1 \n",
    "    out_w = (W + 2 * pad - filter_w) // stride + 1 \n",
    "\n",
    "    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w \n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "    return col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.  5.  6.  7.  9. 10. 11.]\n",
      " [ 2.  3.  4.  6.  7.  8. 10. 11. 12.]\n",
      " [ 5.  6.  7.  9. 10. 11. 13. 14. 15.]\n",
      " [ 6.  7.  8. 10. 11. 12. 14. 15. 16.]]\n",
      "[[-4. -3. -2. -1.]]\n",
      "[[ -26.  -36.  -46.  -66.  -76.  -86. -106. -116. -126.]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 62;\n                var nbb_unformatted_code = \"def naive_im2col(image, filter_h, filter_w):\\n    img_h, img_w = image.shape \\n    out_h = img_h - filter_h + 1 \\n    out_w = img_w - filter_w + 1\\n    col = np.zeros((filter_h * filter_w, out_h * out_w))\\n    for h in range(out_h):\\n        for w in range(out_w):\\n            col[:, w + h * out_w] = image[h : h + filter_w, w : w + filter_w].reshape(-1)\\n    return col\\n\\nx = np.arange(1, 17).reshape(4, 4)\\nf = np.arange(-4, 0).reshape(2, 2)\\nprint(naive_im2col(x, 2, 2))\\nprint(naive_im2col(f, 2, 2).T)\\nprint(naive_im2col(f, 2, 2).T @ naive_im2col(x, 2, 2))\";\n                var nbb_formatted_code = \"def naive_im2col(image, filter_h, filter_w):\\n    img_h, img_w = image.shape\\n    out_h = img_h - filter_h + 1\\n    out_w = img_w - filter_w + 1\\n    col = np.zeros((filter_h * filter_w, out_h * out_w))\\n    for h in range(out_h):\\n        for w in range(out_w):\\n            col[:, w + h * out_w] = image[h : h + filter_w, w : w + filter_w].reshape(\\n                -1\\n            )\\n    return col\\n\\n\\nx = np.arange(1, 17).reshape(4, 4)\\nf = np.arange(-4, 0).reshape(2, 2)\\nprint(naive_im2col(x, 2, 2))\\nprint(naive_im2col(f, 2, 2).T)\\nprint(naive_im2col(f, 2, 2).T @ naive_im2col(x, 2, 2))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def naive_im2col(image, filter_h, filter_w):\n",
    "    img_h, img_w = image.shape \n",
    "    out_h = img_h - filter_h + 1 \n",
    "    out_w = img_w - filter_w + 1\n",
    "    col = np.zeros((filter_h * filter_w, out_h * out_w))\n",
    "    for h in range(out_h):\n",
    "        for w in range(out_w):\n",
    "            col[:, w + h * out_w] = image[h : h + filter_w, w : w + filter_w].reshape(-1)\n",
    "    return col\n",
    "\n",
    "x = np.arange(1, 17).reshape(4, 4)\n",
    "f = np.arange(-4, 0).reshape(2, 2)\n",
    "print(naive_im2col(x, 2, 2))\n",
    "print(naive_im2col(f, 2, 2).T)\n",
    "print(naive_im2col(f, 2, 2).T @ naive_im2col(x, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.6777710914611816\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 63;\n                var nbb_unformatted_code = \"import time \\ny = np.zeros((28, 28))\\nstart = time.time()\\nfor i in range(1000):\\n    naive_im2col(y, 2, 2)\\nend = time.time()\\nprint(f\\\"time: {end - start}\\\")\";\n                var nbb_formatted_code = \"import time\\n\\ny = np.zeros((28, 28))\\nstart = time.time()\\nfor i in range(1000):\\n    naive_im2col(y, 2, 2)\\nend = time.time()\\nprint(f\\\"time: {end - start}\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time \n",
    "y = np.zeros((28, 28))\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    naive_im2col(y, 2, 2)\n",
    "end = time.time()\n",
    "print(f\"time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.  5.  6.  7.  9. 10. 11.]\n",
      " [ 2.  3.  4.  6.  7.  8. 10. 11. 12.]\n",
      " [ 5.  6.  7.  9. 10. 11. 13. 14. 15.]\n",
      " [ 6.  7.  8. 10. 11. 12. 14. 15. 16.]]\n",
      "[[-4. -3. -2. -1.]]\n",
      "[[ -26.  -36.  -46.  -66.  -76.  -86. -106. -116. -126.]]\n",
      "time: 0.004269838333129883\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 64;\n                var nbb_unformatted_code = \"def naive_im2col_v2(image, filter_h, filter_w):\\n    img_h, img_w = image.shape \\n    out_h = img_h - filter_h + 1\\n    out_w = img_w - filter_w + 1\\n    col = np.zeros((filter_h, filter_w, out_h, out_w))\\n    for h in range(filter_h):\\n        for w in range(filter_w):\\n            col[h, w, :, :] = image[h : h + out_h, w : w + out_w]\\n    return col.reshape(filter_h * filter_w, out_h * out_w)\\n\\nx = np.arange(1, 17).reshape(4, 4)\\nf = np.arange(-4, 0).reshape(2, 2)\\nprint(naive_im2col_v2(x, 2, 2))\\nprint(naive_im2col_v2(f, 2, 2).T)\\nprint(naive_im2col_v2(f, 2, 2).T @ naive_im2col_v2(x, 2, 2))\\n\\ny = np.zeros((28, 28))\\nstart = time.time()\\nfor i in range(1000):\\n    naive_im2col_v2(y, 2, 2)\\nend = time.time()\\nprint(\\\"time: {}\\\".format(end - start))\";\n                var nbb_formatted_code = \"def naive_im2col_v2(image, filter_h, filter_w):\\n    img_h, img_w = image.shape\\n    out_h = img_h - filter_h + 1\\n    out_w = img_w - filter_w + 1\\n    col = np.zeros((filter_h, filter_w, out_h, out_w))\\n    for h in range(filter_h):\\n        for w in range(filter_w):\\n            col[h, w, :, :] = image[h : h + out_h, w : w + out_w]\\n    return col.reshape(filter_h * filter_w, out_h * out_w)\\n\\n\\nx = np.arange(1, 17).reshape(4, 4)\\nf = np.arange(-4, 0).reshape(2, 2)\\nprint(naive_im2col_v2(x, 2, 2))\\nprint(naive_im2col_v2(f, 2, 2).T)\\nprint(naive_im2col_v2(f, 2, 2).T @ naive_im2col_v2(x, 2, 2))\\n\\ny = np.zeros((28, 28))\\nstart = time.time()\\nfor i in range(1000):\\n    naive_im2col_v2(y, 2, 2)\\nend = time.time()\\nprint(\\\"time: {}\\\".format(end - start))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def naive_im2col_v2(image, filter_h, filter_w):\n",
    "    img_h, img_w = image.shape \n",
    "    out_h = img_h - filter_h + 1\n",
    "    out_w = img_w - filter_w + 1\n",
    "    col = np.zeros((filter_h, filter_w, out_h, out_w))\n",
    "    for h in range(filter_h):\n",
    "        for w in range(filter_w):\n",
    "            col[h, w, :, :] = image[h : h + out_h, w : w + out_w]\n",
    "    return col.reshape(filter_h * filter_w, out_h * out_w)\n",
    "\n",
    "x = np.arange(1, 17).reshape(4, 4)\n",
    "f = np.arange(-4, 0).reshape(2, 2)\n",
    "print(naive_im2col_v2(x, 2, 2))\n",
    "print(naive_im2col_v2(f, 2, 2).T)\n",
    "print(naive_im2col_v2(f, 2, 2).T @ naive_im2col_v2(x, 2, 2))\n",
    "\n",
    "y = np.zeros((28, 28))\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    naive_im2col_v2(y, 2, 2)\n",
    "end = time.time()\n",
    "print(\"time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 65;\n                var nbb_unformatted_code = \"x1 = np.random.rand(1, 3, 7, 7)\\ncol1 = im2col(x1, 5, 5, stride=1, pad=0)\\nprint(col1.shape)\\n\\nx2 = np.random.rand(10, 3, 7, 7)\\ncol2 = im2col(x2, 5, 5, stride=1, pad=0)\\nprint(col2.shape)\";\n                var nbb_formatted_code = \"x1 = np.random.rand(1, 3, 7, 7)\\ncol1 = im2col(x1, 5, 5, stride=1, pad=0)\\nprint(col1.shape)\\n\\nx2 = np.random.rand(10, 3, 7, 7)\\ncol2 = im2col(x2, 5, 5, stride=1, pad=0)\\nprint(col2.shape)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 96;\n                var nbb_unformatted_code = \"def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\\n    N, C, H, W = input_shape \\n    out_h = (H + 2 * pad - filter_h) // stride + 1\\n    out_w = (W + 2 * pad - filter_w) // stride + 1\\n    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\\n\\n    img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\\n    for y in range(filter_h):\\n        y_max = y + stride * out_h\\n        for x in range(filter_w):\\n            x_max = x + stride * out_w\\n            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\\n\\n    return img[:, :, pad:H + pad, pad:W + pad]\";\n                var nbb_formatted_code = \"def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\\n    N, C, H, W = input_shape\\n    out_h = (H + 2 * pad - filter_h) // stride + 1\\n    out_w = (W + 2 * pad - filter_w) // stride + 1\\n    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(\\n        0, 3, 4, 5, 1, 2\\n    )\\n\\n    img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\\n    for y in range(filter_h):\\n        y_max = y + stride * out_h\\n        for x in range(filter_w):\\n            x_max = x + stride * out_w\\n            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\\n\\n    return img[:, :, pad : H + pad, pad : W + pad]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_shape \n",
    "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 101;\n                var nbb_unformatted_code = \"class Convolution:\\n    def __init__(self, w, b, stride=1, pad=0):\\n        self.w = w \\n        self.b = b \\n        self.stride = stride\\n        self.pad = pad \\n\\n        self.x = None \\n        self.col = None \\n        self.col_w = None \\n        self.dw = None \\n        self.db = None\\n\\n    def forward(self, x):\\n        FN, C, FH, FW = self.w.shape \\n        N, C, H, W = x.shape \\n        out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)\\n        out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)\\n\\n        col = im2col(x, FH, FW, self.stride, self.pad)\\n        col_w = self.w.reshape(FN, -1).T \\n        out = np.dot(col, col_w) + self.b \\n\\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\\n\\n        self.x = x \\n        self.col = col \\n        self.col_w = col_w\\n        return out\\n\\n    def backward(self, dout):\\n        FN, C, FH, FW = self.w.shape \\n        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\\n\\n        self.db = np.sum(dout, axis=0)\\n        self.dw = np.dot(self.col.T, dout)\\n        self.dw = self.dw.transpose(1, 0).reshape(FN, C, FH, FW)\\n\\n        dcol = np.dot(dout, self.col_w.T)\\n        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\\n        return\";\n                var nbb_formatted_code = \"class Convolution:\\n    def __init__(self, w, b, stride=1, pad=0):\\n        self.w = w\\n        self.b = b\\n        self.stride = stride\\n        self.pad = pad\\n\\n        self.x = None\\n        self.col = None\\n        self.col_w = None\\n        self.dw = None\\n        self.db = None\\n\\n    def forward(self, x):\\n        FN, C, FH, FW = self.w.shape\\n        N, C, H, W = x.shape\\n        out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)\\n        out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)\\n\\n        col = im2col(x, FH, FW, self.stride, self.pad)\\n        col_w = self.w.reshape(FN, -1).T\\n        out = np.dot(col, col_w) + self.b\\n\\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\\n\\n        self.x = x\\n        self.col = col\\n        self.col_w = col_w\\n        return out\\n\\n    def backward(self, dout):\\n        FN, C, FH, FW = self.w.shape\\n        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\\n\\n        self.db = np.sum(dout, axis=0)\\n        self.dw = np.dot(self.col.T, dout)\\n        self.dw = self.dw.transpose(1, 0).reshape(FN, C, FH, FW)\\n\\n        dcol = np.dot(dout, self.col_w.T)\\n        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\\n        return\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, w, b, stride=1, pad=0):\n",
    "        self.w = w \n",
    "        self.b = b \n",
    "        self.stride = stride\n",
    "        self.pad = pad \n",
    "\n",
    "        self.x = None \n",
    "        self.col = None \n",
    "        self.col_w = None \n",
    "        self.dw = None \n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.w.shape \n",
    "        N, C, H, W = x.shape \n",
    "        out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_w = self.w.reshape(FN, -1).T \n",
    "        out = np.dot(col, col_w) + self.b \n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x \n",
    "        self.col = col \n",
    "        self.col_w = col_w\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.w.shape \n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dw = np.dot(self.col.T, dout)\n",
    "        self.dw = self.dw.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_w.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 102;\n                var nbb_unformatted_code = \"class Pooling:\\n    def __init__(self, pool_h, pool_w, stride=2, pad=0):\\n        self.pool_h = pool_h\\n        self.pool_w = pool_w\\n        self.stride = stride \\n        self.pad = pad \\n\\n        self.x = None \\n        self.arg_max = None \\n\\n    def forward(self, x):\\n        N, C, H, W = x.shape \\n        out_h = int(1 + (H - self.pool_h) / self.stride)\\n        out_w = int(1 + (W - self.pool_w) / self.stride)\\n        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\\n        # (N, C, OH, OW)\\n        col = col.reshape(-1, self.pool_h * self.pool_w)\\n\\n        arg_max = np.argmax(col, axis=1)\\n        out = np.max(col, axis=1)\\n        # (N, C, OH, OW)\\n        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\\n\\n        self.x = x \\n        self.arg_max = arg_max\\n        return out\\n\\n    def backward(self, dout):\\n        dout = dout.transpose(0, 2, 3, 1)\\n\\n        pool_size = self.pool_h * self.pool_w\\n        dmax = np.zeros((dout.size, pool_size))\\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\\n        dmax = dmax.reshape(dout.shape + (pool_size, ))\\n\\n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\\n        return dx   \";\n                var nbb_formatted_code = \"class Pooling:\\n    def __init__(self, pool_h, pool_w, stride=2, pad=0):\\n        self.pool_h = pool_h\\n        self.pool_w = pool_w\\n        self.stride = stride\\n        self.pad = pad\\n\\n        self.x = None\\n        self.arg_max = None\\n\\n    def forward(self, x):\\n        N, C, H, W = x.shape\\n        out_h = int(1 + (H - self.pool_h) / self.stride)\\n        out_w = int(1 + (W - self.pool_w) / self.stride)\\n        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\\n        # (N, C, OH, OW)\\n        col = col.reshape(-1, self.pool_h * self.pool_w)\\n\\n        arg_max = np.argmax(col, axis=1)\\n        out = np.max(col, axis=1)\\n        # (N, C, OH, OW)\\n        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\\n\\n        self.x = x\\n        self.arg_max = arg_max\\n        return out\\n\\n    def backward(self, dout):\\n        dout = dout.transpose(0, 2, 3, 1)\\n\\n        pool_size = self.pool_h * self.pool_w\\n        dmax = np.zeros((dout.size, pool_size))\\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\\n        dmax = dmax.reshape(dout.shape + (pool_size,))\\n\\n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\\n        return dx\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride \n",
    "        self.pad = pad \n",
    "\n",
    "        self.x = None \n",
    "        self.arg_max = None \n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape \n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        # (N, C, OH, OW)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        # (N, C, OH, OW)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x \n",
    "        self.arg_max = arg_max\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size, ))\n",
    "\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        return dx   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 107;\n                var nbb_unformatted_code = \"from layers import Relu, Affine, SoftmaxWithLoss\\nfrom collections import OrderedDict\\n\\n\\nclass SimpleConvNet:\\n    def __init__(\\n        self,\\n        input_dim=(1, 28, 28),\\n        conv_param={\\\"filter_num\\\": 30, \\\"filter_size\\\": 5, \\\"pad\\\": 0, \\\"stride\\\": 1},\\n        hidden_size=100,\\n        output_size=10,\\n        weight_init_std=0.01,\\n    ):\\n        filter_num = conv_param[\\\"filter_num\\\"]\\n        filter_size = conv_param[\\\"filter_size\\\"]\\n        filter_pad = conv_param[\\\"pad\\\"]\\n        filter_stride = conv_param[\\\"stride\\\"]\\n        input_size = input_dim[1]\\n        conv_output_size = (\\n            input_size - filter_size + 2 * filter_pad\\n        ) // filter_stride + 1\\n        pool_output_size = (\\n            filter_num * (conv_output_size // 2) * (conv_output_size // 2)\\n        )\\n\\n        self.params = {\\n            \\\"w1\\\": weight_init_std\\n            * np.random.randn(filter_num, input_dim[0], filter_size, filter_size),\\n            \\\"b1\\\": np.zeros(filter_num),\\n            \\\"w2\\\": weight_init_std * np.random.randn(pool_output_size, hidden_size),\\n            \\\"b2\\\": np.zeros(hidden_size),\\n            \\\"w3\\\": weight_init_std * np.random.randn(hidden_size, output_size),\\n            \\\"b3\\\": np.zeros(output_size),\\n        }\\n\\n        self.layers = OrderedDict()\\n        self.layers[\\\"Conv1\\\"] = Convolution(\\n            self.params[\\\"w1\\\"],\\n            self.params[\\\"b1\\\"],\\n            conv_param[\\\"stride\\\"],\\n            conv_param[\\\"pad\\\"],\\n        )\\n        self.layers[\\\"Relu1\\\"] = Relu()\\n        self.layers[\\\"Pool1\\\"] = Pooling(pool_h=2, pool_w=2, stride=2)\\n        self.layers[\\\"Affine1\\\"] = Affine(self.params[\\\"w2\\\"], self.params[\\\"b2\\\"])\\n        self.layers[\\\"Relu2\\\"] = Relu()\\n        self.layers[\\\"Affine2\\\"] = Affine(self.params[\\\"w3\\\"], self.params[\\\"b3\\\"])\\n\\n        self.last_layer = SoftmaxWithLoss()\\n\\n    def predict(self, x):\\n        for layer in self.layers.values():\\n            x = layer.forward(x)\\n        return x\\n\\n    def loss(self, x, t):\\n        y = self.predict(x)\\n        return self.last_layer.forward(y, t)\\n\\n    def gradient(self, x, t):\\n        # forward\\n        self.loss(x, t)\\n\\n        # backward\\n        dout = 1\\n        dout = self.last_layer.backward(dout)\\n\\n        layers = list(self.layers.values())\\n        layers.reverse()\\n        for layer in layers:\\n            dout = layer.backward(dout)\\n\\n        grads = {\\n            \\\"w1\\\": self.layers[\\\"Conv1\\\"].dw,\\n            \\\"b1\\\": self.layers[\\\"Conv1\\\"].db,\\n            \\\"w2\\\": self.layers[\\\"Affine1\\\"].dw,\\n            \\\"b2\\\": self.layers[\\\"Affine1\\\"].db,\\n            \\\"w3\\\": self.layers[\\\"Affine2\\\"].dw,\\n            \\\"b3\\\": self.layers[\\\"Affine2\\\"].db,\\n        }\\n        return grads\\n\\n    def accuracy(self, x, t, batch_size=100):\\n        if t.ndim != 1:\\n            t = np.argmax(t, axis=1)\\n\\n        acc = 0.0 \\n        for i in range(int(x.shape[0] / batch_size)):\\n            tx = x[i * batch_size : (i + 1) * batch_size]\\n            tt = t[i * batch_size : (i + 1) * batch_size]\\n            y = self.predict(tx)\\n            y = np.argmax(y, axis=1)\\n            acc += np.sum(y == tt)\\n\\n        return acc / x.shape[0]\";\n                var nbb_formatted_code = \"from layers import Relu, Affine, SoftmaxWithLoss\\nfrom collections import OrderedDict\\n\\n\\nclass SimpleConvNet:\\n    def __init__(\\n        self,\\n        input_dim=(1, 28, 28),\\n        conv_param={\\\"filter_num\\\": 30, \\\"filter_size\\\": 5, \\\"pad\\\": 0, \\\"stride\\\": 1},\\n        hidden_size=100,\\n        output_size=10,\\n        weight_init_std=0.01,\\n    ):\\n        filter_num = conv_param[\\\"filter_num\\\"]\\n        filter_size = conv_param[\\\"filter_size\\\"]\\n        filter_pad = conv_param[\\\"pad\\\"]\\n        filter_stride = conv_param[\\\"stride\\\"]\\n        input_size = input_dim[1]\\n        conv_output_size = (\\n            input_size - filter_size + 2 * filter_pad\\n        ) // filter_stride + 1\\n        pool_output_size = (\\n            filter_num * (conv_output_size // 2) * (conv_output_size // 2)\\n        )\\n\\n        self.params = {\\n            \\\"w1\\\": weight_init_std\\n            * np.random.randn(filter_num, input_dim[0], filter_size, filter_size),\\n            \\\"b1\\\": np.zeros(filter_num),\\n            \\\"w2\\\": weight_init_std * np.random.randn(pool_output_size, hidden_size),\\n            \\\"b2\\\": np.zeros(hidden_size),\\n            \\\"w3\\\": weight_init_std * np.random.randn(hidden_size, output_size),\\n            \\\"b3\\\": np.zeros(output_size),\\n        }\\n\\n        self.layers = OrderedDict()\\n        self.layers[\\\"Conv1\\\"] = Convolution(\\n            self.params[\\\"w1\\\"],\\n            self.params[\\\"b1\\\"],\\n            conv_param[\\\"stride\\\"],\\n            conv_param[\\\"pad\\\"],\\n        )\\n        self.layers[\\\"Relu1\\\"] = Relu()\\n        self.layers[\\\"Pool1\\\"] = Pooling(pool_h=2, pool_w=2, stride=2)\\n        self.layers[\\\"Affine1\\\"] = Affine(self.params[\\\"w2\\\"], self.params[\\\"b2\\\"])\\n        self.layers[\\\"Relu2\\\"] = Relu()\\n        self.layers[\\\"Affine2\\\"] = Affine(self.params[\\\"w3\\\"], self.params[\\\"b3\\\"])\\n\\n        self.last_layer = SoftmaxWithLoss()\\n\\n    def predict(self, x):\\n        for layer in self.layers.values():\\n            x = layer.forward(x)\\n        return x\\n\\n    def loss(self, x, t):\\n        y = self.predict(x)\\n        return self.last_layer.forward(y, t)\\n\\n    def gradient(self, x, t):\\n        # forward\\n        self.loss(x, t)\\n\\n        # backward\\n        dout = 1\\n        dout = self.last_layer.backward(dout)\\n\\n        layers = list(self.layers.values())\\n        layers.reverse()\\n        for layer in layers:\\n            dout = layer.backward(dout)\\n\\n        grads = {\\n            \\\"w1\\\": self.layers[\\\"Conv1\\\"].dw,\\n            \\\"b1\\\": self.layers[\\\"Conv1\\\"].db,\\n            \\\"w2\\\": self.layers[\\\"Affine1\\\"].dw,\\n            \\\"b2\\\": self.layers[\\\"Affine1\\\"].db,\\n            \\\"w3\\\": self.layers[\\\"Affine2\\\"].dw,\\n            \\\"b3\\\": self.layers[\\\"Affine2\\\"].db,\\n        }\\n        return grads\\n\\n    def accuracy(self, x, t, batch_size=100):\\n        if t.ndim != 1:\\n            t = np.argmax(t, axis=1)\\n\\n        acc = 0.0\\n        for i in range(int(x.shape[0] / batch_size)):\\n            tx = x[i * batch_size : (i + 1) * batch_size]\\n            tt = t[i * batch_size : (i + 1) * batch_size]\\n            y = self.predict(tx)\\n            y = np.argmax(y, axis=1)\\n            acc += np.sum(y == tt)\\n\\n        return acc / x.shape[0]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from layers import Relu, Affine, SoftmaxWithLoss\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=(1, 28, 28),\n",
    "        conv_param={\"filter_num\": 30, \"filter_size\": 5, \"pad\": 0, \"stride\": 1},\n",
    "        hidden_size=100,\n",
    "        output_size=10,\n",
    "        weight_init_std=0.01,\n",
    "    ):\n",
    "        filter_num = conv_param[\"filter_num\"]\n",
    "        filter_size = conv_param[\"filter_size\"]\n",
    "        filter_pad = conv_param[\"pad\"]\n",
    "        filter_stride = conv_param[\"stride\"]\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (\n",
    "            input_size - filter_size + 2 * filter_pad\n",
    "        ) // filter_stride + 1\n",
    "        pool_output_size = (\n",
    "            filter_num * (conv_output_size // 2) * (conv_output_size // 2)\n",
    "        )\n",
    "\n",
    "        self.params = {\n",
    "            \"w1\": weight_init_std\n",
    "            * np.random.randn(filter_num, input_dim[0], filter_size, filter_size),\n",
    "            \"b1\": np.zeros(filter_num),\n",
    "            \"w2\": weight_init_std * np.random.randn(pool_output_size, hidden_size),\n",
    "            \"b2\": np.zeros(hidden_size),\n",
    "            \"w3\": weight_init_std * np.random.randn(hidden_size, output_size),\n",
    "            \"b3\": np.zeros(output_size),\n",
    "        }\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Conv1\"] = Convolution(\n",
    "            self.params[\"w1\"],\n",
    "            self.params[\"b1\"],\n",
    "            conv_param[\"stride\"],\n",
    "            conv_param[\"pad\"],\n",
    "        )\n",
    "        self.layers[\"Relu1\"] = Relu()\n",
    "        self.layers[\"Pool1\"] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"w2\"], self.params[\"b2\"])\n",
    "        self.layers[\"Relu2\"] = Relu()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"w3\"], self.params[\"b3\"])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {\n",
    "            \"w1\": self.layers[\"Conv1\"].dw,\n",
    "            \"b1\": self.layers[\"Conv1\"].db,\n",
    "            \"w2\": self.layers[\"Affine1\"].dw,\n",
    "            \"b2\": self.layers[\"Affine1\"].db,\n",
    "            \"w3\": self.layers[\"Affine2\"].dw,\n",
    "            \"b3\": self.layers[\"Affine2\"].db,\n",
    "        }\n",
    "        return grads\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0 \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i * batch_size : (i + 1) * batch_size]\n",
    "            tt = t[i * batch_size : (i + 1) * batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.298772258651825\n",
      "=== epoch:1, train acc:0.106, test acc:0.119 ===\n",
      "train loss:2.297002099195513\n",
      "train loss:2.290245401962719\n",
      "train loss:2.2866673517201126\n",
      "train loss:2.275383265058307\n",
      "train loss:2.2640594985212137\n",
      "train loss:2.2316374006854813\n",
      "train loss:2.230562924773169\n",
      "train loss:2.2223972775229694\n",
      "train loss:2.1919069192170446\n",
      "train loss:2.1712525072088606\n",
      "train loss:2.112856131360257\n",
      "train loss:2.063302403161611\n",
      "train loss:2.0201835747270196\n",
      "train loss:1.9971041498102111\n",
      "train loss:1.9270913809393482\n",
      "train loss:1.8359504403913645\n",
      "train loss:1.7340271013031947\n",
      "train loss:1.6768525156879373\n",
      "train loss:1.5257421006906504\n",
      "train loss:1.6545627443878272\n",
      "train loss:1.4433314596370965\n",
      "train loss:1.3637016312278056\n",
      "train loss:1.2659147138523712\n",
      "train loss:1.1964755562477545\n",
      "train loss:1.219196164782566\n",
      "train loss:1.0821448764038564\n",
      "train loss:0.9947774342868521\n",
      "train loss:0.936988184246169\n",
      "train loss:0.8531012305427171\n",
      "train loss:0.7600107506024506\n",
      "train loss:0.9157025912466469\n",
      "train loss:0.6834555223047204\n",
      "train loss:0.6639215595905921\n",
      "train loss:0.796175301347511\n",
      "train loss:0.8258107887576727\n",
      "train loss:0.7868936435589486\n",
      "train loss:0.56440895564996\n",
      "train loss:0.6232189635327428\n",
      "train loss:0.6873693699981911\n",
      "train loss:0.7501882435516104\n",
      "train loss:0.45969906896692087\n",
      "train loss:0.6083762551170934\n",
      "train loss:0.7006543842850376\n",
      "train loss:0.5139618244774841\n",
      "train loss:0.698322694834773\n",
      "train loss:0.6421752785481436\n",
      "train loss:0.5947538714690829\n",
      "train loss:0.4326953578228319\n",
      "train loss:0.5725892744031953\n",
      "train loss:0.6365722409492613\n",
      "train loss:0.5747211170157704\n",
      "train loss:0.657693236721919\n",
      "train loss:0.5004117733539034\n",
      "train loss:0.5518621160736644\n",
      "train loss:0.4127571823869808\n",
      "train loss:0.33313348207507504\n",
      "train loss:0.40642958159546033\n",
      "train loss:0.4726099847266857\n",
      "train loss:0.4384841235278402\n",
      "train loss:0.5502402832684234\n",
      "train loss:0.44651971794279727\n",
      "train loss:0.35051701262971674\n",
      "train loss:0.45240179195390856\n",
      "train loss:0.5521249365541927\n",
      "train loss:0.5990638382858725\n",
      "train loss:0.4592716949206253\n",
      "train loss:0.3663031053134707\n",
      "train loss:0.5429091234831602\n",
      "train loss:0.6384178876330246\n",
      "train loss:0.38221863900735475\n",
      "train loss:0.45651089188396454\n",
      "train loss:0.45626504408633856\n",
      "train loss:0.47209720629629204\n",
      "train loss:0.33260999421550175\n",
      "train loss:0.6047627495369844\n",
      "train loss:0.5394524324403528\n",
      "train loss:0.38626747989518484\n",
      "train loss:0.4040846004800852\n",
      "train loss:0.4577935174294192\n",
      "train loss:0.40875178389401035\n",
      "train loss:0.3674718595954723\n",
      "train loss:0.593738790069393\n",
      "train loss:0.5005444600543345\n",
      "train loss:0.5120315688143041\n",
      "train loss:0.5006022030800403\n",
      "train loss:0.35064210467881934\n",
      "train loss:0.4602924787969391\n",
      "train loss:0.44531627120587863\n",
      "train loss:0.3517419415575123\n",
      "train loss:0.3049861728136566\n",
      "train loss:0.3892460120356185\n",
      "train loss:0.43500829548711506\n",
      "train loss:0.3880189509447067\n",
      "train loss:0.41065374163268586\n",
      "train loss:0.4039813796855074\n",
      "train loss:0.20639091040026059\n",
      "train loss:0.38856266263928907\n",
      "train loss:0.46890862388209315\n",
      "train loss:0.4275075925111301\n",
      "train loss:0.41188862707128465\n",
      "train loss:0.5272788946408872\n",
      "train loss:0.48083323910063747\n",
      "train loss:0.3081896588611753\n",
      "train loss:0.3047240203349417\n",
      "train loss:0.33301325038825497\n",
      "train loss:0.2885617720334728\n",
      "train loss:0.4638010116877874\n",
      "train loss:0.3384851187553764\n",
      "train loss:0.545203118015676\n",
      "train loss:0.3326202910339763\n",
      "train loss:0.29191613125472954\n",
      "train loss:0.26805231851802447\n",
      "train loss:0.42144480180059185\n",
      "train loss:0.46990305965766843\n",
      "train loss:0.2916712960863802\n",
      "train loss:0.23031888906349274\n",
      "train loss:0.36913144364444656\n",
      "train loss:0.24027308469931666\n",
      "train loss:0.4058325741269304\n",
      "train loss:0.31501349618675906\n",
      "train loss:0.4627552407781668\n",
      "train loss:0.3116953577175301\n",
      "train loss:0.289936968024209\n",
      "train loss:0.2903961918195534\n",
      "train loss:0.17970996685650586\n",
      "train loss:0.2687684248732186\n",
      "train loss:0.3122242544921548\n",
      "train loss:0.25652802435918376\n",
      "train loss:0.3031098037076335\n",
      "train loss:0.2994026831929054\n",
      "train loss:0.44611236513898034\n",
      "train loss:0.4574791049157341\n",
      "train loss:0.36462859515396917\n",
      "train loss:0.32475505405287164\n",
      "train loss:0.232315307503461\n",
      "train loss:0.18542335237696342\n",
      "train loss:0.4125566455186627\n",
      "train loss:0.290143510259918\n",
      "train loss:0.3818686326925368\n",
      "train loss:0.6346363897921105\n",
      "train loss:0.29366274057216507\n",
      "train loss:0.5317709200596648\n",
      "train loss:0.2874051140290999\n",
      "train loss:0.31873196362294925\n",
      "train loss:0.24644379449142265\n",
      "train loss:0.34902132521470974\n",
      "train loss:0.31261979686227015\n",
      "train loss:0.3673973925299466\n",
      "train loss:0.27855350605079787\n",
      "train loss:0.19097726772257828\n",
      "train loss:0.4437697179912805\n",
      "train loss:0.319595495899433\n",
      "train loss:0.2808878973870555\n",
      "train loss:0.40231488293278866\n",
      "train loss:0.35510247002954537\n",
      "train loss:0.2988750943220502\n",
      "train loss:0.30539704021246517\n",
      "train loss:0.3344501646894846\n",
      "train loss:0.38682106080205453\n",
      "train loss:0.27284461667203463\n",
      "train loss:0.30891070660492603\n",
      "train loss:0.3116785296336067\n",
      "train loss:0.24660269335685756\n",
      "train loss:0.39247036715389944\n",
      "train loss:0.31425501828076496\n",
      "train loss:0.21156651792105638\n",
      "train loss:0.29045167636913694\n",
      "train loss:0.25355573356549915\n",
      "train loss:0.4997118831066173\n",
      "train loss:0.37994213903520213\n",
      "train loss:0.2845222287228228\n",
      "train loss:0.19277677019904568\n",
      "train loss:0.3156592683225221\n",
      "train loss:0.2842164446702376\n",
      "train loss:0.18563867680122093\n",
      "train loss:0.41485811821178364\n",
      "train loss:0.31733257619852767\n",
      "train loss:0.26569341673802005\n",
      "train loss:0.4491592051634109\n",
      "train loss:0.24462572028498486\n",
      "train loss:0.2541995230445059\n",
      "train loss:0.36948981968367983\n",
      "train loss:0.33921780128652573\n",
      "train loss:0.2364694312266324\n",
      "train loss:0.2940975693005181\n",
      "train loss:0.2678772688179441\n",
      "train loss:0.2836745668986008\n",
      "train loss:0.4692220099513552\n",
      "train loss:0.3450650645196649\n",
      "train loss:0.19486053924422728\n",
      "train loss:0.34105659709451464\n",
      "train loss:0.2174086271084602\n",
      "train loss:0.3273785569562812\n",
      "train loss:0.3834845305965677\n",
      "train loss:0.2623446456672423\n",
      "train loss:0.350506071691603\n",
      "train loss:0.3600407070145773\n",
      "train loss:0.2149803650392714\n",
      "train loss:0.28523917528016574\n",
      "train loss:0.19267774970477358\n",
      "train loss:0.5632091893045017\n",
      "train loss:0.21710108512126314\n",
      "train loss:0.21134213405386837\n",
      "train loss:0.5238751718044442\n",
      "train loss:0.3072441783361984\n",
      "train loss:0.2828528450705149\n",
      "train loss:0.21782932945079259\n",
      "train loss:0.3231659670356513\n",
      "train loss:0.27666763044524567\n",
      "train loss:0.2791498839660425\n",
      "train loss:0.2199305447504361\n",
      "train loss:0.20730883475094497\n",
      "train loss:0.18755682652976763\n",
      "train loss:0.2227245839961714\n",
      "train loss:0.2761076327040588\n",
      "train loss:0.26193482558319464\n",
      "train loss:0.2634028950060743\n",
      "train loss:0.2962258194205643\n",
      "train loss:0.26999472764026167\n",
      "train loss:0.41943492424745893\n",
      "train loss:0.2685144554419989\n",
      "train loss:0.33250775968827895\n",
      "train loss:0.3858161673335818\n",
      "train loss:0.3097877366050371\n",
      "train loss:0.29607222757804624\n",
      "train loss:0.17018710807912982\n",
      "train loss:0.31672596785227713\n",
      "train loss:0.24364299064281272\n",
      "train loss:0.3810045781341907\n",
      "train loss:0.30753600035729983\n",
      "train loss:0.21337447228907613\n",
      "train loss:0.24857063614292374\n",
      "train loss:0.36111073070567146\n",
      "train loss:0.22846291694390203\n",
      "train loss:0.31747796696607616\n",
      "train loss:0.2758683039546804\n",
      "train loss:0.32240474179005096\n",
      "train loss:0.3743479523489114\n",
      "train loss:0.2917452860882361\n",
      "train loss:0.24658450245446556\n",
      "train loss:0.33414004274562314\n",
      "train loss:0.34913371463322873\n",
      "train loss:0.39213845153602656\n",
      "train loss:0.20899343503323212\n",
      "train loss:0.22427047985918303\n",
      "train loss:0.20138350650190082\n",
      "train loss:0.21005202686939786\n",
      "train loss:0.2096605094615502\n",
      "train loss:0.2796081972324582\n",
      "train loss:0.2858120896705402\n",
      "train loss:0.3121190552086388\n",
      "train loss:0.2860470270129318\n",
      "train loss:0.32899475306700565\n",
      "train loss:0.32616744028579236\n",
      "train loss:0.1880011624624133\n",
      "train loss:0.27254593606640265\n",
      "train loss:0.27122989636494593\n",
      "train loss:0.25592525238289876\n",
      "train loss:0.2710078689383016\n",
      "train loss:0.2505852058635178\n",
      "train loss:0.20342892748966002\n",
      "train loss:0.3236299709761243\n",
      "train loss:0.21797822070164005\n",
      "train loss:0.3660471672952099\n",
      "train loss:0.2591635128969882\n",
      "train loss:0.21882825925242497\n",
      "train loss:0.2835657962148891\n",
      "train loss:0.4246995823556855\n",
      "train loss:0.2365044836175316\n",
      "train loss:0.26848914830521\n",
      "train loss:0.1830178392160058\n",
      "train loss:0.2288479562224919\n",
      "train loss:0.32210003807730175\n",
      "train loss:0.22345938978217897\n",
      "train loss:0.19553576298074085\n",
      "train loss:0.2572944611618649\n",
      "train loss:0.20381011184855594\n",
      "train loss:0.2961727010988599\n",
      "train loss:0.1410040973681311\n",
      "train loss:0.4004731310566702\n",
      "train loss:0.2437458891668614\n",
      "train loss:0.24038032233459525\n",
      "train loss:0.2279438452518875\n",
      "train loss:0.24314556106507626\n",
      "train loss:0.2837262261914803\n",
      "train loss:0.16860090753691176\n",
      "train loss:0.17595385988774206\n",
      "train loss:0.36256373278874626\n",
      "train loss:0.3266614222519242\n",
      "train loss:0.23426421086277752\n",
      "train loss:0.14970120276114135\n",
      "train loss:0.17290733771369904\n",
      "train loss:0.1811381094498228\n",
      "train loss:0.16848653197503247\n",
      "train loss:0.1429259009115812\n",
      "train loss:0.1448961859124607\n",
      "train loss:0.20307927354412886\n",
      "train loss:0.1511682726111767\n",
      "train loss:0.1158281672213441\n",
      "train loss:0.226416150425906\n",
      "train loss:0.11612007766233795\n",
      "train loss:0.24816165364424186\n",
      "train loss:0.12867800780978195\n",
      "train loss:0.1672243883872267\n",
      "train loss:0.2080066040178287\n",
      "train loss:0.19048281014941876\n",
      "train loss:0.2124508248205256\n",
      "train loss:0.31344217301938343\n",
      "train loss:0.33519509971807365\n",
      "train loss:0.24157713886052284\n",
      "train loss:0.11135200723568843\n",
      "train loss:0.19528571185034924\n",
      "train loss:0.21444669858898652\n",
      "train loss:0.29762342801774244\n",
      "train loss:0.18091327501045407\n",
      "train loss:0.18417706456782618\n",
      "train loss:0.28592673854617306\n",
      "train loss:0.15493942358320564\n",
      "train loss:0.23282550971502503\n",
      "train loss:0.1154561566741376\n",
      "train loss:0.2796619763918997\n",
      "train loss:0.19264574916067267\n",
      "train loss:0.2403360040982109\n",
      "train loss:0.1744699990941052\n",
      "train loss:0.21501578577729194\n",
      "train loss:0.21975464126816166\n",
      "train loss:0.29452629803190394\n",
      "train loss:0.25726995553064463\n",
      "train loss:0.16195338666812703\n",
      "train loss:0.07264814040831545\n",
      "train loss:0.26289985610911126\n",
      "train loss:0.2009112060071933\n",
      "train loss:0.20037425019132424\n",
      "train loss:0.11311093964960563\n",
      "train loss:0.26893911609360166\n",
      "train loss:0.1300386599755238\n",
      "train loss:0.43482460620207386\n",
      "train loss:0.19118097903933434\n",
      "train loss:0.16170543686075564\n",
      "train loss:0.10864612760616903\n",
      "train loss:0.16818476989123773\n",
      "train loss:0.1195265799982995\n",
      "train loss:0.15066831529606584\n",
      "train loss:0.10502222022876294\n",
      "train loss:0.11814860681717614\n",
      "train loss:0.33635754190067507\n",
      "train loss:0.18771008219843005\n",
      "train loss:0.3471602522097392\n",
      "train loss:0.21496202312751145\n",
      "train loss:0.2243521111542445\n",
      "train loss:0.23004310081906237\n",
      "train loss:0.09423773814273635\n",
      "train loss:0.15972445609217228\n",
      "train loss:0.11253501556542704\n",
      "train loss:0.2625528249357363\n",
      "train loss:0.2379365677584682\n",
      "train loss:0.10935676521219706\n",
      "train loss:0.3047215204051227\n",
      "train loss:0.1559346330154963\n",
      "train loss:0.1490304712896505\n",
      "train loss:0.14416958523957463\n",
      "train loss:0.11067952994978537\n",
      "train loss:0.14820126900372269\n",
      "train loss:0.2755235239497163\n",
      "train loss:0.18561000498735974\n",
      "train loss:0.15982481782003644\n",
      "train loss:0.17926063404340617\n",
      "train loss:0.11366908084119633\n",
      "train loss:0.11244919383486067\n",
      "train loss:0.22285819492589912\n",
      "train loss:0.1706245454637812\n",
      "train loss:0.26338086644536896\n",
      "train loss:0.18214230869473166\n",
      "train loss:0.13662161146830049\n",
      "train loss:0.18816045082954108\n",
      "train loss:0.1374467702091033\n",
      "train loss:0.12900916874456186\n",
      "train loss:0.19784720565573827\n",
      "train loss:0.31755876141333833\n",
      "train loss:0.1727962199744258\n",
      "train loss:0.3405036500586061\n",
      "train loss:0.24638469630436272\n",
      "train loss:0.2647701590299445\n",
      "train loss:0.15188294411221254\n",
      "train loss:0.12131027616753387\n",
      "train loss:0.16704711114992377\n",
      "train loss:0.3554833385642901\n",
      "train loss:0.2938835084981111\n",
      "train loss:0.24839620082177322\n",
      "train loss:0.15669759550952875\n",
      "train loss:0.12009123413898004\n",
      "train loss:0.22191739768721847\n",
      "train loss:0.12034403766877659\n",
      "train loss:0.25247261075574784\n",
      "train loss:0.30603677517365135\n",
      "train loss:0.15008004843769085\n",
      "train loss:0.10869916354617148\n",
      "train loss:0.23178245393572613\n",
      "train loss:0.17401188390310868\n",
      "train loss:0.10352661221866163\n",
      "train loss:0.26659973799173214\n",
      "train loss:0.17911817194022728\n",
      "train loss:0.21271030234850985\n",
      "train loss:0.12057614612734889\n",
      "train loss:0.1911314143312057\n",
      "train loss:0.19263698606139545\n",
      "train loss:0.16070297467847477\n",
      "train loss:0.1430625039022327\n",
      "train loss:0.17498090433755387\n",
      "train loss:0.22103505932389164\n",
      "train loss:0.20230467027226232\n",
      "train loss:0.16915744126783092\n",
      "train loss:0.1874377579058812\n",
      "train loss:0.13118572819945934\n",
      "train loss:0.11348198889430355\n",
      "train loss:0.1381581499742515\n",
      "train loss:0.29918894350088004\n",
      "train loss:0.1242681387117725\n",
      "train loss:0.12770847732330856\n",
      "train loss:0.13214825302584057\n",
      "train loss:0.08683090085436128\n",
      "train loss:0.1877124545782055\n",
      "train loss:0.19374297405749427\n",
      "train loss:0.2214902710236677\n",
      "train loss:0.21181831325113007\n",
      "train loss:0.16970006465754545\n",
      "train loss:0.12883560790184842\n",
      "train loss:0.24017328862307866\n",
      "train loss:0.10818056376165977\n",
      "train loss:0.12857099897013152\n",
      "train loss:0.21594921835038408\n",
      "train loss:0.08891183881783622\n",
      "train loss:0.14618954808392173\n",
      "train loss:0.08141216220313047\n",
      "train loss:0.17023580551298495\n",
      "train loss:0.15370186277743744\n",
      "train loss:0.08680545541492375\n",
      "train loss:0.0893420012125334\n",
      "train loss:0.125632235101611\n",
      "train loss:0.18558444000934696\n",
      "train loss:0.1635006742452697\n",
      "train loss:0.16271472559039876\n",
      "train loss:0.13457817730287944\n",
      "train loss:0.16908032718953298\n",
      "train loss:0.14899789356553395\n",
      "train loss:0.21749677630936756\n",
      "train loss:0.2170452888595883\n",
      "train loss:0.10570392857348075\n",
      "train loss:0.19926209015860866\n",
      "train loss:0.17249226538113596\n",
      "train loss:0.05879464279937756\n",
      "train loss:0.1875944028106769\n",
      "train loss:0.2268260437557823\n",
      "train loss:0.20667811163576066\n",
      "train loss:0.16983389177158348\n",
      "train loss:0.21835736748629475\n",
      "train loss:0.13395990337880045\n",
      "train loss:0.2650339396871445\n",
      "train loss:0.22598452435050176\n",
      "train loss:0.13475517819228666\n",
      "train loss:0.18130674767754906\n",
      "train loss:0.1518024298769571\n",
      "train loss:0.11967166524377763\n",
      "train loss:0.09670238646717644\n",
      "train loss:0.1611380251036888\n",
      "train loss:0.2056099190214168\n",
      "train loss:0.07203559797082879\n",
      "train loss:0.11390165408378201\n",
      "train loss:0.18211652362392422\n",
      "train loss:0.15468206568038956\n",
      "train loss:0.14658447293929433\n",
      "train loss:0.061482863449345636\n",
      "train loss:0.15793618093014056\n",
      "train loss:0.09809343834414784\n",
      "train loss:0.1402346232695644\n",
      "train loss:0.14358175308143772\n",
      "train loss:0.13375451575276331\n",
      "train loss:0.22807133083175465\n",
      "train loss:0.3087176614548037\n",
      "train loss:0.08774346750521878\n",
      "train loss:0.11089014813895468\n",
      "train loss:0.11006040640644113\n",
      "train loss:0.20943944088130503\n",
      "train loss:0.1559261578137725\n",
      "train loss:0.13620060680691315\n",
      "train loss:0.1593840684904322\n",
      "train loss:0.17926233979976447\n",
      "train loss:0.2570818220007514\n",
      "train loss:0.04354900309832243\n",
      "train loss:0.15243105652817923\n",
      "train loss:0.1450955743612104\n",
      "train loss:0.16825115157959022\n",
      "train loss:0.13452057050561148\n",
      "train loss:0.22654834895257397\n",
      "train loss:0.1855233854752885\n",
      "train loss:0.2065851217704175\n",
      "train loss:0.09854712705859842\n",
      "train loss:0.13582760950942255\n",
      "train loss:0.12325592183439317\n",
      "train loss:0.07854964536161418\n",
      "train loss:0.16644267632181428\n",
      "train loss:0.1400282071415004\n",
      "train loss:0.10111700129288378\n",
      "train loss:0.15705630990454358\n",
      "train loss:0.061808351760596035\n",
      "train loss:0.14878213736752596\n",
      "train loss:0.13448029570096554\n",
      "train loss:0.0709140728721258\n",
      "train loss:0.1996795980205727\n",
      "train loss:0.06818474359773756\n",
      "train loss:0.11352637675414771\n",
      "train loss:0.22023689562426513\n",
      "train loss:0.30345599604872114\n",
      "train loss:0.10463737937513155\n",
      "train loss:0.08882271920176117\n",
      "train loss:0.11338617254274005\n",
      "train loss:0.15542055939661176\n",
      "train loss:0.10138031489870626\n",
      "train loss:0.19162012681873095\n",
      "train loss:0.08126694695159811\n",
      "train loss:0.08490168079683573\n",
      "train loss:0.10778148612111271\n",
      "train loss:0.212582127107548\n",
      "train loss:0.1571516413374466\n",
      "train loss:0.114411359163546\n",
      "train loss:0.23600591409386215\n",
      "train loss:0.18726144490600788\n",
      "train loss:0.1626782500039136\n",
      "train loss:0.06591499808375971\n",
      "train loss:0.0887471345716152\n",
      "train loss:0.19347183083845515\n",
      "train loss:0.11988012963284782\n",
      "train loss:0.14964578565739034\n",
      "train loss:0.27397840705875043\n",
      "train loss:0.16707065916809807\n",
      "train loss:0.09504999835868194\n",
      "train loss:0.18699021907526275\n",
      "train loss:0.26809054899904966\n",
      "train loss:0.18477713276645072\n",
      "train loss:0.11980240780640809\n",
      "train loss:0.15241527730836515\n",
      "train loss:0.2351484143023207\n",
      "train loss:0.17250762535627115\n",
      "train loss:0.16023310397652768\n",
      "train loss:0.1958758436885254\n",
      "train loss:0.21222391277635963\n",
      "train loss:0.12281753489351974\n",
      "train loss:0.16026771671257492\n",
      "train loss:0.07193465050555727\n",
      "train loss:0.1997413960639211\n",
      "train loss:0.0673654666528653\n",
      "train loss:0.17208938208707278\n",
      "train loss:0.17341908961852048\n",
      "train loss:0.22307196975287963\n",
      "train loss:0.18135117132299272\n",
      "train loss:0.1489158560797745\n",
      "train loss:0.1330235013557764\n",
      "train loss:0.21068991586184688\n",
      "train loss:0.08267425388572733\n",
      "train loss:0.06162881760440298\n",
      "train loss:0.06778286229956229\n",
      "train loss:0.23307302513603234\n",
      "train loss:0.08505455328575277\n",
      "train loss:0.1697114718798773\n",
      "train loss:0.3219737045994355\n",
      "train loss:0.17669596922941902\n",
      "train loss:0.13079364735193194\n",
      "train loss:0.1491170889233382\n",
      "train loss:0.0943997441902523\n",
      "train loss:0.12464148263862695\n",
      "train loss:0.1675753605236768\n",
      "train loss:0.11982517918812068\n",
      "train loss:0.21215661906527633\n",
      "train loss:0.09336963513582557\n",
      "train loss:0.11279046493236225\n",
      "train loss:0.15654564770099233\n",
      "train loss:0.179987149272687\n",
      "train loss:0.05353815025113117\n",
      "train loss:0.1384697390937588\n",
      "train loss:0.13207678590337038\n",
      "train loss:0.1174219098221196\n",
      "train loss:0.12294802989676185\n",
      "train loss:0.12170613714411466\n",
      "train loss:0.13006836075311096\n",
      "train loss:0.19970209787111246\n",
      "train loss:0.14146423333040203\n",
      "train loss:0.11391851308051232\n",
      "train loss:0.2084595313395191\n",
      "train loss:0.2381441476988311\n",
      "train loss:0.11900470291972917\n",
      "train loss:0.2763160834267106\n",
      "train loss:0.06992940009257385\n",
      "train loss:0.09656273612754368\n",
      "train loss:0.19331188126672214\n",
      "train loss:0.1081159253438222\n",
      "train loss:0.1169878495731669\n",
      "train loss:0.09488409247038662\n",
      "train loss:0.110827112656549\n",
      "train loss:0.07390341152903408\n",
      "train loss:0.0676957672631452\n",
      "=== epoch:2, train acc:0.962, test acc:0.959 ===\n",
      "train loss:0.08409459755166292\n",
      "train loss:0.11933310461894689\n",
      "train loss:0.12648413446206389\n",
      "train loss:0.08356283513683817\n",
      "train loss:0.17006647900644112\n",
      "train loss:0.1607537400339599\n",
      "train loss:0.14654949384278784\n",
      "train loss:0.11508979486944072\n",
      "train loss:0.19063007809163188\n",
      "train loss:0.1142196238692171\n",
      "train loss:0.14611117060029624\n",
      "train loss:0.13879745223894724\n",
      "train loss:0.15685375476022953\n",
      "train loss:0.18704943913435493\n",
      "train loss:0.09254696972013\n",
      "train loss:0.2481377247707508\n",
      "train loss:0.11194737750988348\n",
      "train loss:0.26302512689177293\n",
      "train loss:0.11043563531009129\n",
      "train loss:0.12880500294073285\n",
      "train loss:0.04610025577314727\n",
      "train loss:0.0865845213872759\n",
      "train loss:0.144893317269773\n",
      "train loss:0.19702272664044387\n",
      "train loss:0.1705459594721777\n",
      "train loss:0.06619763574464661\n",
      "train loss:0.131169774259329\n",
      "train loss:0.14379950526280644\n",
      "train loss:0.13003794465112806\n",
      "train loss:0.06676506644144974\n",
      "train loss:0.08746281875704164\n",
      "train loss:0.07957409601784733\n",
      "train loss:0.13576391955058187\n",
      "train loss:0.10105890103865713\n",
      "train loss:0.149396701945508\n",
      "train loss:0.1688846842672473\n",
      "train loss:0.06653892736809365\n",
      "train loss:0.05250306329864454\n",
      "train loss:0.16609611421302717\n",
      "train loss:0.1006883931481505\n",
      "train loss:0.12149505454263325\n",
      "train loss:0.14009504827493005\n",
      "train loss:0.22455889996907177\n",
      "train loss:0.097967374617131\n",
      "train loss:0.08098190491744235\n",
      "train loss:0.042990172208394385\n",
      "train loss:0.17215953551173446\n",
      "train loss:0.0689745226792264\n",
      "train loss:0.06649585521265708\n",
      "train loss:0.10049219451549486\n",
      "train loss:0.11562198080312852\n",
      "train loss:0.13463939812836684\n",
      "train loss:0.08019739847999059\n",
      "train loss:0.08801068601639903\n",
      "train loss:0.11301160802996862\n",
      "train loss:0.048546967389752685\n",
      "train loss:0.08140418367383892\n",
      "train loss:0.13052918401719296\n",
      "train loss:0.06054616951780068\n",
      "train loss:0.17834952680845412\n",
      "train loss:0.06322733077055367\n",
      "train loss:0.07802626344063034\n",
      "train loss:0.20550732196272098\n",
      "train loss:0.09905358723837812\n",
      "train loss:0.09937215203861724\n",
      "train loss:0.14903323708598018\n",
      "train loss:0.05255845807840012\n",
      "train loss:0.13291068064587927\n",
      "train loss:0.1075894387894833\n",
      "train loss:0.0935839075158429\n",
      "train loss:0.11484981203386793\n",
      "train loss:0.058576214440241484\n",
      "train loss:0.1406283712803832\n",
      "train loss:0.12249261900762315\n",
      "train loss:0.05944853595896707\n",
      "train loss:0.06355409486412732\n",
      "train loss:0.09175960926010206\n",
      "train loss:0.2205837698333508\n",
      "train loss:0.10750843193832477\n",
      "train loss:0.08508880850244262\n",
      "train loss:0.15587935766679395\n",
      "train loss:0.13258973466987073\n",
      "train loss:0.07032112060815254\n",
      "train loss:0.09973362650521712\n",
      "train loss:0.1341174880571014\n",
      "train loss:0.12950921835093368\n",
      "train loss:0.11561161510019324\n",
      "train loss:0.10435519618472557\n",
      "train loss:0.07452925084393926\n",
      "train loss:0.054208518460814564\n",
      "train loss:0.14598200458397403\n",
      "train loss:0.19443373576990663\n",
      "train loss:0.05568851499387652\n",
      "train loss:0.08348224115118054\n",
      "train loss:0.1689025330570563\n",
      "train loss:0.21697934973624594\n",
      "train loss:0.17009790628976998\n",
      "train loss:0.10165987268844702\n",
      "train loss:0.1365314270709487\n",
      "train loss:0.07431854544910349\n",
      "train loss:0.08141029326657008\n",
      "train loss:0.08010135467198429\n",
      "train loss:0.11123766777092489\n",
      "train loss:0.16259701777930735\n",
      "train loss:0.17161425875857148\n",
      "train loss:0.0699163759313792\n",
      "train loss:0.05778448957807832\n",
      "train loss:0.06289678678699243\n",
      "train loss:0.0477974848309124\n",
      "train loss:0.14983718449250782\n",
      "train loss:0.061956130370165695\n",
      "train loss:0.20892439303997906\n",
      "train loss:0.18344307266957974\n",
      "train loss:0.11646670770900591\n",
      "train loss:0.05004637650882425\n",
      "train loss:0.08070877441002872\n",
      "train loss:0.1406606790952269\n",
      "train loss:0.08262774325076938\n",
      "train loss:0.0645358196871694\n",
      "train loss:0.09588352504810112\n",
      "train loss:0.14862671079413242\n",
      "train loss:0.09602875638577299\n",
      "train loss:0.18571732732590474\n",
      "train loss:0.04788836609437402\n",
      "train loss:0.18297760828834758\n",
      "train loss:0.12419237821123177\n",
      "train loss:0.09918024284197506\n",
      "train loss:0.16703403357132707\n",
      "train loss:0.12160437081014114\n",
      "train loss:0.17379550005423688\n",
      "train loss:0.12685508687575175\n",
      "train loss:0.10811075865852864\n",
      "train loss:0.12992071918106168\n",
      "train loss:0.059199089913139685\n",
      "train loss:0.08446643048358203\n",
      "train loss:0.15780971293769652\n",
      "train loss:0.08277178652695195\n",
      "train loss:0.0376760318877808\n",
      "train loss:0.11979372978315786\n",
      "train loss:0.11864454657663433\n",
      "train loss:0.05167702388151404\n",
      "train loss:0.08994291456471401\n",
      "train loss:0.07945973651325965\n",
      "train loss:0.06719912447827117\n",
      "train loss:0.09725208890986713\n",
      "train loss:0.12362258401399188\n",
      "train loss:0.046291037674613754\n",
      "train loss:0.10279288217556788\n",
      "train loss:0.10941697544087164\n",
      "train loss:0.08178673952811673\n",
      "train loss:0.025129379648257722\n",
      "train loss:0.10755881192902915\n",
      "train loss:0.11681606256561557\n",
      "train loss:0.1960261834401634\n",
      "train loss:0.2058921916225335\n",
      "train loss:0.05312416908386829\n",
      "train loss:0.11653857202010626\n",
      "train loss:0.1060753227309846\n",
      "train loss:0.09431485521678709\n",
      "train loss:0.06300532037433461\n",
      "train loss:0.23192374740000152\n",
      "train loss:0.07476511587195654\n",
      "train loss:0.19333601064029882\n",
      "train loss:0.07198741184418837\n",
      "train loss:0.09429349838871434\n",
      "train loss:0.0803254617335547\n",
      "train loss:0.06090144111535152\n",
      "train loss:0.06590505978942318\n",
      "train loss:0.05944292417463469\n",
      "train loss:0.07423746614757568\n",
      "train loss:0.14110040687409725\n",
      "train loss:0.06843951848167755\n",
      "train loss:0.050598932295613235\n",
      "train loss:0.13807251558595193\n",
      "train loss:0.023575285634441733\n",
      "train loss:0.07360917964773164\n",
      "train loss:0.048645689828090664\n",
      "train loss:0.08659884849882\n",
      "train loss:0.04224444553431981\n",
      "train loss:0.06806592650153752\n",
      "train loss:0.06448179232721399\n",
      "train loss:0.12225917532227382\n",
      "train loss:0.14801882430529806\n",
      "train loss:0.04410726595102031\n",
      "train loss:0.1539244178523979\n",
      "train loss:0.05060924768210377\n",
      "train loss:0.1067254233891387\n",
      "train loss:0.05977574699129011\n",
      "train loss:0.06418642494547926\n",
      "train loss:0.037814822761305183\n",
      "train loss:0.10188682696011563\n",
      "train loss:0.05733884772070599\n",
      "train loss:0.08111677945336349\n",
      "train loss:0.06410476862265317\n",
      "train loss:0.10118518652689441\n",
      "train loss:0.08895589537758958\n",
      "train loss:0.072973811063699\n",
      "train loss:0.13546252553704652\n",
      "train loss:0.06510564558781363\n",
      "train loss:0.09138302034298912\n",
      "train loss:0.06388589145658609\n",
      "train loss:0.1200494570172659\n",
      "train loss:0.03594553390329836\n",
      "train loss:0.1634284241326732\n",
      "train loss:0.1053784803608805\n",
      "train loss:0.0752689326896056\n",
      "train loss:0.08487923263253838\n",
      "train loss:0.03167726257714895\n",
      "train loss:0.14840408154105383\n",
      "train loss:0.11127604417267918\n",
      "train loss:0.05031434920295326\n",
      "train loss:0.09316340279647792\n",
      "train loss:0.03646833937397421\n",
      "train loss:0.10276131194543439\n",
      "train loss:0.06466148840091417\n",
      "train loss:0.22532512361490054\n",
      "train loss:0.10196311492582145\n",
      "train loss:0.06074788274552378\n",
      "train loss:0.17165935349434683\n",
      "train loss:0.1446768903726109\n",
      "train loss:0.16803158669261986\n",
      "train loss:0.035827214889309296\n",
      "train loss:0.12456060039407296\n",
      "train loss:0.0802427720122605\n",
      "train loss:0.0925057551208156\n",
      "train loss:0.05594190172211652\n",
      "train loss:0.18349371126371739\n",
      "train loss:0.1843370939384543\n",
      "train loss:0.09032399344574511\n",
      "train loss:0.028148722845788822\n",
      "train loss:0.09671243859451131\n",
      "train loss:0.09856878367316747\n",
      "train loss:0.052570849078568946\n",
      "train loss:0.10060548025679351\n",
      "train loss:0.05424737057811174\n",
      "train loss:0.08480353497075678\n",
      "train loss:0.09541224463020848\n",
      "train loss:0.13999511316296703\n",
      "train loss:0.15650958007468294\n",
      "train loss:0.1247337741619407\n",
      "train loss:0.07090730671933416\n",
      "train loss:0.044470760480146844\n",
      "train loss:0.07095255276477701\n",
      "train loss:0.08257736852363413\n",
      "train loss:0.02977975087204708\n",
      "train loss:0.0591681909129193\n",
      "train loss:0.0742167295332856\n",
      "train loss:0.07598637107814488\n",
      "train loss:0.13998645445739152\n",
      "train loss:0.03653113956183636\n",
      "train loss:0.17413965970742504\n",
      "train loss:0.07341062905037375\n",
      "train loss:0.15966838208249798\n",
      "train loss:0.1230548628292736\n",
      "train loss:0.12299772214634927\n",
      "train loss:0.13531893297614497\n",
      "train loss:0.029000899285450587\n",
      "train loss:0.10289370777610544\n",
      "train loss:0.020031678037014573\n",
      "train loss:0.08956315197480404\n",
      "train loss:0.09294923481595765\n",
      "train loss:0.09818515064622017\n",
      "train loss:0.13209666398215697\n",
      "train loss:0.05507899676056331\n",
      "train loss:0.10095050119941043\n",
      "train loss:0.0914854532315708\n",
      "train loss:0.1035763810513021\n",
      "train loss:0.1361679000504074\n",
      "train loss:0.07007714645847476\n",
      "train loss:0.06483056116684784\n",
      "train loss:0.0652575647450258\n",
      "train loss:0.0622043466264225\n",
      "train loss:0.06186157516599373\n",
      "train loss:0.10943759416850801\n",
      "train loss:0.07142210137988707\n",
      "train loss:0.09011468095168576\n",
      "train loss:0.048391653862768606\n",
      "train loss:0.08953484493741173\n",
      "train loss:0.06305680062450722\n",
      "train loss:0.017879469408424654\n",
      "train loss:0.05995055389512341\n",
      "train loss:0.044825150720689384\n",
      "train loss:0.07876917163110835\n",
      "train loss:0.06953195643449482\n",
      "train loss:0.08242869501191162\n",
      "train loss:0.10889250471416557\n",
      "train loss:0.07908880473514462\n",
      "train loss:0.02778535118271303\n",
      "train loss:0.0974925783977551\n",
      "train loss:0.19894822179695662\n",
      "train loss:0.03465310961077061\n",
      "train loss:0.09290185059759455\n",
      "train loss:0.0867802670999663\n",
      "train loss:0.060350613567524\n",
      "train loss:0.062308262973503045\n",
      "train loss:0.04697176690243149\n",
      "train loss:0.10541576389830774\n",
      "train loss:0.052290088409036636\n",
      "train loss:0.07899920350660995\n",
      "train loss:0.14914898550444872\n",
      "train loss:0.07340896901476268\n",
      "train loss:0.035967499670880615\n",
      "train loss:0.16973672875323473\n",
      "train loss:0.07586947365503494\n",
      "train loss:0.06168863295839066\n",
      "train loss:0.07750891366734555\n",
      "train loss:0.031849880542843\n",
      "train loss:0.06816667194619369\n",
      "train loss:0.07344929519838146\n",
      "train loss:0.05968227969604685\n",
      "train loss:0.050052505498637975\n",
      "train loss:0.07008386139756237\n",
      "train loss:0.044535373923994426\n",
      "train loss:0.06385452396053903\n",
      "train loss:0.10581110552032298\n",
      "train loss:0.09842571507484722\n",
      "train loss:0.05660915421978821\n",
      "train loss:0.0313947393962055\n",
      "train loss:0.04998844233004406\n",
      "train loss:0.037890135006931405\n",
      "train loss:0.1274824370337738\n",
      "train loss:0.07886415385401266\n",
      "train loss:0.052784432330841534\n",
      "train loss:0.18458775669715335\n",
      "train loss:0.07719796042206026\n",
      "train loss:0.09056654842167346\n",
      "train loss:0.08614689897624835\n",
      "train loss:0.0890965832934732\n",
      "train loss:0.07131509951715388\n",
      "train loss:0.06558096438214495\n",
      "train loss:0.10220932279812761\n",
      "train loss:0.035232958212519684\n",
      "train loss:0.02756657129857609\n",
      "train loss:0.06310819723618562\n",
      "train loss:0.03385847742657071\n",
      "train loss:0.016647042466400887\n",
      "train loss:0.09632050468839351\n",
      "train loss:0.07981737510957103\n",
      "train loss:0.06724505871228184\n",
      "train loss:0.20770090577237443\n",
      "train loss:0.03577457835779121\n",
      "train loss:0.04361927285810869\n",
      "train loss:0.07215840249213576\n",
      "train loss:0.037742472915467354\n",
      "train loss:0.06195517658301536\n",
      "train loss:0.07654221743608541\n",
      "train loss:0.11729810161600368\n",
      "train loss:0.08855296617823466\n",
      "train loss:0.05835092069613678\n",
      "train loss:0.15342734821112197\n",
      "train loss:0.05756150002400591\n",
      "train loss:0.115409939043904\n",
      "train loss:0.05914064894866009\n",
      "train loss:0.06766851219887053\n",
      "train loss:0.10500237834787375\n",
      "train loss:0.11461524475165503\n",
      "train loss:0.14774643852073333\n",
      "train loss:0.06582363178087797\n",
      "train loss:0.045910978523205914\n",
      "train loss:0.1685253357112938\n",
      "train loss:0.11151671077315034\n",
      "train loss:0.14461759115726833\n",
      "train loss:0.08562429216631783\n",
      "train loss:0.08018347680341892\n",
      "train loss:0.07483105875734072\n",
      "train loss:0.0946567732630894\n",
      "train loss:0.034966893038252324\n",
      "train loss:0.07560100811901205\n",
      "train loss:0.1469363259769384\n",
      "train loss:0.1334954599689807\n",
      "train loss:0.04452160111275234\n",
      "train loss:0.06651125508681532\n",
      "train loss:0.06284021685006753\n",
      "train loss:0.06650937927921005\n",
      "train loss:0.15509443607913742\n",
      "train loss:0.07263495453220037\n",
      "train loss:0.05581102783146888\n",
      "train loss:0.020287676116318334\n",
      "train loss:0.028770073891711333\n",
      "train loss:0.09762755206104862\n",
      "train loss:0.06976237223717485\n",
      "train loss:0.07267507645692041\n",
      "train loss:0.056669571604808865\n",
      "train loss:0.04239644026269459\n",
      "train loss:0.07989586056966008\n",
      "train loss:0.05240318312822517\n",
      "train loss:0.1472811842452113\n",
      "train loss:0.06311158563237851\n",
      "train loss:0.03328690189412633\n",
      "train loss:0.04121984265790697\n",
      "train loss:0.0624519349786645\n",
      "train loss:0.1693669741056512\n",
      "train loss:0.040021269637558345\n",
      "train loss:0.03168042299948762\n",
      "train loss:0.10215129356623581\n",
      "train loss:0.04426618075086417\n",
      "train loss:0.0732580008263559\n",
      "train loss:0.014548726877905154\n",
      "train loss:0.027403351353637317\n",
      "train loss:0.01567153423065533\n",
      "train loss:0.061150475275110604\n",
      "train loss:0.08165842742739195\n",
      "train loss:0.07521750527491147\n",
      "train loss:0.08244591451375877\n",
      "train loss:0.03956336488671973\n",
      "train loss:0.15408737711470252\n",
      "train loss:0.021702904184887375\n",
      "train loss:0.07127029788160832\n",
      "train loss:0.07875444707296443\n",
      "train loss:0.08231129271529695\n",
      "train loss:0.04291283161072917\n",
      "train loss:0.0699051094172643\n",
      "train loss:0.03676420135014652\n",
      "train loss:0.014445960289397448\n",
      "train loss:0.056609178957232935\n",
      "train loss:0.13408175061992617\n",
      "train loss:0.028335899596226587\n",
      "train loss:0.13631958765518434\n",
      "train loss:0.0854714388374611\n",
      "train loss:0.11410870146255463\n",
      "train loss:0.05546428028947381\n",
      "train loss:0.044686169153431425\n",
      "train loss:0.1255757142196635\n",
      "train loss:0.17834284364803188\n",
      "train loss:0.04009059450237742\n",
      "train loss:0.10884430034221548\n",
      "train loss:0.043267382357678165\n",
      "train loss:0.12428116048641062\n",
      "train loss:0.07719174874319631\n",
      "train loss:0.05137891359674926\n",
      "train loss:0.06798628073740408\n",
      "train loss:0.059250502300056025\n",
      "train loss:0.08553289612013149\n",
      "train loss:0.040094662129074206\n",
      "train loss:0.11276900307528531\n",
      "train loss:0.05424169925676117\n",
      "train loss:0.046492267985522175\n",
      "train loss:0.07884677308365248\n",
      "train loss:0.07716485814550587\n",
      "train loss:0.07284155477113514\n",
      "train loss:0.12674804899472902\n",
      "train loss:0.06257833280211955\n",
      "train loss:0.02714611209500704\n",
      "train loss:0.04317816860068019\n",
      "train loss:0.0792629747466294\n",
      "train loss:0.12715526349262898\n",
      "train loss:0.056679288333428425\n",
      "train loss:0.11405386760055182\n",
      "train loss:0.026154479617757707\n",
      "train loss:0.0983018131694384\n",
      "train loss:0.026265015369116865\n",
      "train loss:0.12373182661692082\n",
      "train loss:0.04318023944868303\n",
      "train loss:0.12330112461332975\n",
      "train loss:0.056061957518631106\n",
      "train loss:0.08287219472801359\n",
      "train loss:0.07054640713785042\n",
      "train loss:0.11020455844983008\n",
      "train loss:0.061456495795989434\n",
      "train loss:0.10750823739795667\n",
      "train loss:0.07601321065636786\n",
      "train loss:0.03814979764371242\n",
      "train loss:0.033361176926988645\n",
      "train loss:0.10517719775657214\n",
      "train loss:0.02485077462083922\n",
      "train loss:0.03317866132071813\n",
      "train loss:0.12721535022720018\n",
      "train loss:0.07764839539901723\n",
      "train loss:0.09412100118937448\n",
      "train loss:0.07294918777631343\n",
      "train loss:0.059576381309464875\n",
      "train loss:0.08046025875432489\n",
      "train loss:0.11213719126266172\n",
      "train loss:0.08412838271484252\n",
      "train loss:0.05748672140204619\n",
      "train loss:0.04288470669486702\n",
      "train loss:0.11786968243003695\n",
      "train loss:0.06099032933166461\n",
      "train loss:0.07214433670196044\n",
      "train loss:0.04817264660110637\n",
      "train loss:0.04180854671445997\n",
      "train loss:0.14056884350629079\n",
      "train loss:0.05521006408460503\n",
      "train loss:0.06925921875389551\n",
      "train loss:0.05740124864992498\n",
      "train loss:0.08885155978877192\n",
      "train loss:0.03480329860916177\n",
      "train loss:0.08277233511085157\n",
      "train loss:0.09468805043242927\n",
      "train loss:0.1015008726095015\n",
      "train loss:0.045060952304933136\n",
      "train loss:0.05110812648287456\n",
      "train loss:0.03885113623645582\n",
      "train loss:0.09772768259476391\n",
      "train loss:0.0405068441557597\n",
      "train loss:0.0421404641860239\n",
      "train loss:0.04390770892672426\n",
      "train loss:0.16969972448268889\n",
      "train loss:0.0972911934640704\n",
      "train loss:0.064322583747936\n",
      "train loss:0.1077068403372793\n",
      "train loss:0.04824898901291\n",
      "train loss:0.10188646521817347\n",
      "train loss:0.11886981369332973\n",
      "train loss:0.01925491881200132\n",
      "train loss:0.05398090328005961\n",
      "train loss:0.11541825511808414\n",
      "train loss:0.07074406130754608\n",
      "train loss:0.07839060857917558\n",
      "train loss:0.04288534970707425\n",
      "train loss:0.03132212740620969\n",
      "train loss:0.11143549169436379\n",
      "train loss:0.05585233604349547\n",
      "train loss:0.05867890227040723\n",
      "train loss:0.07600793442702664\n",
      "train loss:0.10050547782436828\n",
      "train loss:0.11506672313305899\n",
      "train loss:0.026288416938460603\n",
      "train loss:0.10333615921773145\n",
      "train loss:0.10687997565006503\n",
      "train loss:0.056793875911148976\n",
      "train loss:0.08939911606654587\n",
      "train loss:0.0570049381546591\n",
      "train loss:0.12886124656647804\n",
      "train loss:0.053349284770699154\n",
      "train loss:0.150945372620407\n",
      "train loss:0.054153137177641704\n",
      "train loss:0.055514252598464896\n",
      "train loss:0.07364239266651224\n",
      "train loss:0.08261378871700567\n",
      "train loss:0.042778656507659274\n",
      "train loss:0.1450234812096649\n",
      "train loss:0.14650144577728574\n",
      "train loss:0.09382316074389369\n",
      "train loss:0.09681164772981163\n",
      "train loss:0.022194835021243874\n",
      "train loss:0.04532404708821091\n",
      "train loss:0.0602835252975071\n",
      "train loss:0.11048824699641262\n",
      "train loss:0.04553596411517085\n",
      "train loss:0.11252809066175383\n",
      "train loss:0.07902169346492399\n",
      "train loss:0.03550229889524899\n",
      "train loss:0.08365282840795862\n",
      "train loss:0.10017184391810224\n",
      "train loss:0.034016249218288115\n",
      "train loss:0.0452426511648443\n",
      "train loss:0.0911711503729768\n",
      "train loss:0.07389570897563029\n",
      "train loss:0.025486444497183344\n",
      "train loss:0.062274441822271875\n",
      "train loss:0.05735905040284327\n",
      "train loss:0.08543552259788566\n",
      "train loss:0.05164771618306971\n",
      "train loss:0.051927126232777204\n",
      "train loss:0.11687808825170576\n",
      "train loss:0.06742739854265732\n",
      "train loss:0.037682717773361504\n",
      "train loss:0.02779600934539215\n",
      "train loss:0.04083202346315813\n",
      "train loss:0.13941035766222032\n",
      "train loss:0.05792864738247952\n",
      "train loss:0.14743035376754002\n",
      "train loss:0.08351033429377143\n",
      "train loss:0.04082824898443991\n",
      "train loss:0.010145033350116454\n",
      "train loss:0.07952243693791691\n",
      "train loss:0.06740358038380463\n",
      "train loss:0.11275788279319113\n",
      "train loss:0.03446548895688465\n",
      "train loss:0.03975154287421272\n",
      "train loss:0.028206451954295723\n",
      "train loss:0.0705533488283913\n",
      "train loss:0.055848108773701846\n",
      "train loss:0.04672597692405116\n",
      "train loss:0.03995971489438847\n",
      "train loss:0.08716484097702354\n",
      "train loss:0.04594383251235042\n",
      "train loss:0.06296968334970403\n",
      "train loss:0.0594957122164936\n",
      "train loss:0.06366098880806381\n",
      "train loss:0.1092795715085596\n",
      "train loss:0.12977524761355186\n",
      "train loss:0.01310918697526988\n",
      "train loss:0.04573460512099515\n",
      "train loss:0.05613667878294181\n",
      "train loss:0.07228851240666415\n",
      "train loss:0.027914902291384804\n",
      "train loss:0.060003614150616044\n",
      "train loss:0.08591721675475358\n",
      "train loss:0.051877755476624834\n",
      "train loss:0.03568043409865803\n",
      "train loss:0.05486335465744166\n",
      "train loss:0.05188092673001929\n",
      "train loss:0.209168682031206\n",
      "train loss:0.18264631765444134\n",
      "train loss:0.03237639491892223\n",
      "train loss:0.07840312965602697\n",
      "train loss:0.15089356263965534\n",
      "train loss:0.05982204166970006\n",
      "=== epoch:3, train acc:0.976, test acc:0.976 ===\n",
      "train loss:0.09596389452491376\n",
      "train loss:0.029568323833821006\n",
      "train loss:0.05840366982711384\n",
      "train loss:0.05617999720637647\n",
      "train loss:0.04418995130640927\n",
      "train loss:0.029232140381029545\n",
      "train loss:0.12289016692243031\n",
      "train loss:0.05700139022309511\n",
      "train loss:0.014803111771822546\n",
      "train loss:0.03966888817912542\n",
      "train loss:0.040723130909235766\n",
      "train loss:0.09608959871303496\n",
      "train loss:0.09609647137759122\n",
      "train loss:0.11904185315110678\n",
      "train loss:0.13436731974008043\n",
      "train loss:0.12584446258317974\n",
      "train loss:0.013552432420947558\n",
      "train loss:0.052205205765160204\n",
      "train loss:0.06123162435367889\n",
      "train loss:0.08403999443504949\n",
      "train loss:0.0791234006555212\n",
      "train loss:0.04673760615412306\n",
      "train loss:0.023627125163442134\n",
      "train loss:0.03821053706541614\n",
      "train loss:0.053166812861880255\n",
      "train loss:0.03302787155666007\n",
      "train loss:0.02949536508558794\n",
      "train loss:0.06431932656590496\n",
      "train loss:0.05222694194937182\n",
      "train loss:0.059308923660346825\n",
      "train loss:0.040873134584552336\n",
      "train loss:0.047287589718698175\n",
      "train loss:0.022804580785464913\n",
      "train loss:0.022000140658785562\n",
      "train loss:0.13613689564353115\n",
      "train loss:0.07637822584952436\n",
      "train loss:0.06559608838530127\n",
      "train loss:0.02754704081824768\n",
      "train loss:0.04406140046382631\n",
      "train loss:0.07816867322396708\n",
      "train loss:0.09125461104045286\n",
      "train loss:0.0318596451493566\n",
      "train loss:0.05968178309848413\n",
      "train loss:0.09138644212135066\n",
      "train loss:0.03507332935608494\n",
      "train loss:0.06085605609793317\n",
      "train loss:0.06146357815021642\n",
      "train loss:0.04816873740994556\n",
      "train loss:0.06447979142054748\n",
      "train loss:0.06291504111429515\n",
      "train loss:0.036697181015130004\n",
      "train loss:0.12140307311898421\n",
      "train loss:0.02068688552489398\n",
      "train loss:0.13067015787131175\n",
      "train loss:0.052595377906220445\n",
      "train loss:0.02489288288876038\n",
      "train loss:0.03275766419424429\n",
      "train loss:0.05837013791803107\n",
      "train loss:0.08397285310328267\n",
      "train loss:0.06649634195418655\n",
      "train loss:0.042876041417749686\n",
      "train loss:0.07522287990374853\n",
      "train loss:0.08475661682884689\n",
      "train loss:0.059688571371126466\n",
      "train loss:0.06139431018757571\n",
      "train loss:0.05591136275040026\n",
      "train loss:0.09784868507530664\n",
      "train loss:0.0644614713992258\n",
      "train loss:0.048575856473973195\n",
      "train loss:0.04533747905274106\n",
      "train loss:0.034398135558364504\n",
      "train loss:0.11048748414856596\n",
      "train loss:0.1045680544660202\n",
      "train loss:0.029608284089636006\n",
      "train loss:0.08525068124785237\n",
      "train loss:0.0497820589928412\n",
      "train loss:0.07783076071611278\n",
      "train loss:0.02967506217614944\n",
      "train loss:0.025017934915300535\n",
      "train loss:0.09195306281838954\n",
      "train loss:0.05395119527614209\n",
      "train loss:0.12210730208086643\n",
      "train loss:0.08564394771242612\n",
      "train loss:0.04171992570889885\n",
      "train loss:0.038864882391552925\n",
      "train loss:0.1049470574517607\n",
      "train loss:0.09875194717003057\n",
      "train loss:0.04076595270958292\n",
      "train loss:0.06008811599013068\n",
      "train loss:0.04086115124117598\n",
      "train loss:0.062266888317078314\n",
      "train loss:0.02254176284897678\n",
      "train loss:0.07424823439036948\n",
      "train loss:0.048970170971236016\n",
      "train loss:0.05227960758140429\n",
      "train loss:0.07392088607924731\n",
      "train loss:0.08200086811901604\n",
      "train loss:0.024732371696906848\n",
      "train loss:0.03134297482140277\n",
      "train loss:0.0752382670417994\n",
      "train loss:0.04412420095620618\n",
      "train loss:0.017641747388478048\n",
      "train loss:0.04083062148067545\n",
      "train loss:0.025289429040542166\n",
      "train loss:0.1104200207730099\n",
      "train loss:0.046540584055589196\n",
      "train loss:0.042037918889810365\n",
      "train loss:0.018429751747690713\n",
      "train loss:0.022146044223501925\n",
      "train loss:0.06745940971202459\n",
      "train loss:0.0499835555820599\n",
      "train loss:0.0524837706230277\n",
      "train loss:0.13794725430373445\n",
      "train loss:0.10347523142955525\n",
      "train loss:0.04277463196284906\n",
      "train loss:0.048128665739892974\n",
      "train loss:0.03760911436656973\n",
      "train loss:0.07905407758006999\n",
      "train loss:0.01188536987246574\n",
      "train loss:0.028788635263090754\n",
      "train loss:0.05832346506632913\n",
      "train loss:0.08461884711280276\n",
      "train loss:0.09353868208553255\n",
      "train loss:0.025763639732330333\n",
      "train loss:0.1161452874725189\n",
      "train loss:0.10944308597154391\n",
      "train loss:0.09751814266249059\n",
      "train loss:0.05418865449098051\n",
      "train loss:0.07722223165201443\n",
      "train loss:0.024965961634406307\n",
      "train loss:0.07120254012573506\n",
      "train loss:0.060657818640462144\n",
      "train loss:0.04955047795762431\n",
      "train loss:0.07721171872132504\n",
      "train loss:0.050768068063659993\n",
      "train loss:0.08639252571759562\n",
      "train loss:0.08085858364634133\n",
      "train loss:0.038559080597807494\n",
      "train loss:0.12328048883088125\n",
      "train loss:0.03676037215087338\n",
      "train loss:0.11898970154827616\n",
      "train loss:0.011787158284549378\n",
      "train loss:0.05655305588870517\n",
      "train loss:0.06782199643468804\n",
      "train loss:0.036606303529943866\n",
      "train loss:0.044291536065759446\n",
      "train loss:0.09423339548941695\n",
      "train loss:0.05506269536832842\n",
      "train loss:0.09272386936191719\n",
      "train loss:0.063955349876257\n",
      "train loss:0.027713429762717786\n",
      "train loss:0.09578170936279061\n",
      "train loss:0.03986830644353176\n",
      "train loss:0.06913706497211713\n",
      "train loss:0.021464195353113426\n",
      "train loss:0.06964106785233219\n",
      "train loss:0.08053119138299776\n",
      "train loss:0.08993304135458403\n",
      "train loss:0.06995669247352893\n",
      "train loss:0.08621829797744748\n",
      "train loss:0.02524805749484764\n",
      "train loss:0.05067595972010512\n",
      "train loss:0.03614629324585408\n",
      "train loss:0.04426026117921399\n",
      "train loss:0.014818838658875053\n",
      "train loss:0.05100174367603521\n",
      "train loss:0.04747890766366484\n",
      "train loss:0.08574839177119652\n",
      "train loss:0.04245875674762346\n",
      "train loss:0.015205710595173023\n",
      "train loss:0.09018919351164931\n",
      "train loss:0.028630664058082102\n",
      "train loss:0.05879095133714048\n",
      "train loss:0.05422155072990715\n",
      "train loss:0.044206097329959136\n",
      "train loss:0.04717979039507249\n",
      "train loss:0.03798771716901805\n",
      "train loss:0.028400581009171328\n",
      "train loss:0.1157006165907592\n",
      "train loss:0.018939981941824172\n",
      "train loss:0.0202420327316563\n",
      "train loss:0.0501741086638222\n",
      "train loss:0.03563055714925722\n",
      "train loss:0.06433255349912173\n",
      "train loss:0.038735813529049444\n",
      "train loss:0.0826293161569456\n",
      "train loss:0.07757174928589003\n",
      "train loss:0.10333987143265372\n",
      "train loss:0.01948948183445839\n",
      "train loss:0.05521893602277868\n",
      "train loss:0.11098823461026236\n",
      "train loss:0.04967991478099892\n",
      "train loss:0.05771770588328566\n",
      "train loss:0.05018416471026813\n",
      "train loss:0.03905220348061686\n",
      "train loss:0.03650113150296755\n",
      "train loss:0.12088292887208314\n",
      "train loss:0.03731191104554894\n",
      "train loss:0.03391033494247789\n",
      "train loss:0.10031933013759584\n",
      "train loss:0.08161487112951157\n",
      "train loss:0.04263600707974955\n",
      "train loss:0.05068795779997612\n",
      "train loss:0.04778944479453894\n",
      "train loss:0.09273145365159163\n",
      "train loss:0.0748925174542795\n",
      "train loss:0.0335649474305656\n",
      "train loss:0.012688796420710574\n",
      "train loss:0.08529025069958975\n",
      "train loss:0.03664251686676757\n",
      "train loss:0.005868254005735922\n",
      "train loss:0.04160795443747918\n",
      "train loss:0.08911054706757046\n",
      "train loss:0.06204289489020555\n",
      "train loss:0.060112247237866505\n",
      "train loss:0.09497152162034263\n",
      "train loss:0.07439399460312897\n",
      "train loss:0.02326230839344219\n",
      "train loss:0.0347385285836097\n",
      "train loss:0.048278165388027566\n",
      "train loss:0.015770433294044418\n",
      "train loss:0.055119766148585894\n",
      "train loss:0.046614042874439966\n",
      "train loss:0.07456835625341734\n",
      "train loss:0.06913824092330396\n",
      "train loss:0.026452953151773807\n",
      "train loss:0.04557914706207516\n",
      "train loss:0.03502622961345886\n",
      "train loss:0.0631858018015964\n",
      "train loss:0.047969221428408974\n",
      "train loss:0.052120227823905546\n",
      "train loss:0.14803430181134394\n",
      "train loss:0.10524284701767257\n",
      "train loss:0.06064316220635601\n",
      "train loss:0.0247829241724527\n",
      "train loss:0.025221465738184925\n",
      "train loss:0.008033204433176955\n",
      "train loss:0.03995545510517892\n",
      "train loss:0.022152431787207653\n",
      "train loss:0.09441659671030087\n",
      "train loss:0.055302393969189526\n",
      "train loss:0.027897787325852717\n",
      "train loss:0.04788485836444389\n",
      "train loss:0.048595624615564874\n",
      "train loss:0.04179323495539218\n",
      "train loss:0.06653426932529674\n",
      "train loss:0.05038838766907824\n",
      "train loss:0.03845923809318225\n",
      "train loss:0.06837994652304852\n",
      "train loss:0.03705854913960708\n",
      "train loss:0.07459512586648366\n",
      "train loss:0.02788364391267604\n",
      "train loss:0.0250331421966334\n",
      "train loss:0.03358666393554837\n",
      "train loss:0.020971091592440167\n",
      "train loss:0.04028852865802693\n",
      "train loss:0.01996037168444881\n",
      "train loss:0.07722474926072298\n",
      "train loss:0.1913781778301377\n",
      "train loss:0.08291497456231699\n",
      "train loss:0.03575902852673539\n",
      "train loss:0.17082941675960472\n",
      "train loss:0.06178132474937642\n",
      "train loss:0.02425057113886115\n",
      "train loss:0.022078281932956453\n",
      "train loss:0.08826216656960095\n",
      "train loss:0.04495077403030345\n",
      "train loss:0.03153804069015861\n",
      "train loss:0.11268244554968858\n",
      "train loss:0.07359169891728774\n",
      "train loss:0.021599351425376237\n",
      "train loss:0.041936389133803396\n",
      "train loss:0.13388060951175113\n",
      "train loss:0.08481424951309353\n",
      "train loss:0.025189699349033135\n",
      "train loss:0.07121621274761797\n",
      "train loss:0.020449444605970746\n",
      "train loss:0.01992422114208615\n",
      "train loss:0.01925449807859845\n",
      "train loss:0.0961692698304714\n",
      "train loss:0.045916841072342666\n",
      "train loss:0.05306261783402363\n",
      "train loss:0.034302152081692586\n",
      "train loss:0.037685082256335406\n",
      "train loss:0.04121313086884784\n",
      "train loss:0.028449338480324063\n",
      "train loss:0.040244347169530265\n",
      "train loss:0.04583220541825383\n",
      "train loss:0.09224049131224764\n",
      "train loss:0.0280003598056974\n",
      "train loss:0.08585298101130448\n",
      "train loss:0.0469548470572598\n",
      "train loss:0.01633880909495703\n",
      "train loss:0.06278336694762786\n",
      "train loss:0.06354880028442868\n",
      "train loss:0.08794251605298653\n",
      "train loss:0.06835418138532473\n",
      "train loss:0.041151412259083385\n",
      "train loss:0.04358948442711807\n",
      "train loss:0.014768236787645089\n",
      "train loss:0.05095930607152737\n",
      "train loss:0.1375050397524417\n",
      "train loss:0.10581640945676739\n",
      "train loss:0.058547770272971575\n",
      "train loss:0.009964844447049965\n",
      "train loss:0.0843236980359597\n",
      "train loss:0.025727099960767233\n",
      "train loss:0.04762561396289516\n",
      "train loss:0.07240385529446469\n",
      "train loss:0.019357935006617855\n",
      "train loss:0.01973232992690939\n",
      "train loss:0.023699107207542786\n",
      "train loss:0.08233655414307998\n",
      "train loss:0.04606614066644699\n",
      "train loss:0.06677754704777049\n",
      "train loss:0.007362691417229842\n",
      "train loss:0.04586457020319574\n",
      "train loss:0.11629241270854263\n",
      "train loss:0.026390390630865943\n",
      "train loss:0.06336799670964985\n",
      "train loss:0.05900786384783798\n",
      "train loss:0.04664902311887818\n",
      "train loss:0.08991586207948715\n",
      "train loss:0.08196975674685301\n",
      "train loss:0.027493231083342562\n",
      "train loss:0.009338867738672136\n",
      "train loss:0.027415486341626702\n",
      "train loss:0.10193635573528591\n",
      "train loss:0.023859449351080832\n",
      "train loss:0.024072337298465935\n",
      "train loss:0.03886885269000464\n",
      "train loss:0.032970713458300736\n",
      "train loss:0.07514764248362905\n",
      "train loss:0.041640040872562054\n",
      "train loss:0.03199711325772712\n",
      "train loss:0.026900778739577778\n",
      "train loss:0.06467737477493303\n",
      "train loss:0.03476151820407299\n",
      "train loss:0.03426303634022212\n",
      "train loss:0.11211838633886448\n",
      "train loss:0.061368178448068224\n",
      "train loss:0.07554570152502518\n",
      "train loss:0.04282566417427182\n",
      "train loss:0.023997082316901185\n",
      "train loss:0.03154524518967564\n",
      "train loss:0.04668710342593703\n",
      "train loss:0.07996758615412004\n",
      "train loss:0.05415126150658496\n",
      "train loss:0.010757819311043762\n",
      "train loss:0.10928445351455009\n",
      "train loss:0.01564661176722521\n",
      "train loss:0.13210665038695896\n",
      "train loss:0.14536688218764324\n",
      "train loss:0.09638729378967341\n",
      "train loss:0.06176156519881789\n",
      "train loss:0.024309014376045258\n",
      "train loss:0.09039108241426944\n",
      "train loss:0.035177590823693164\n",
      "train loss:0.014557872615556182\n",
      "train loss:0.03449997051787029\n",
      "train loss:0.062497558495325276\n",
      "train loss:0.04862745389121329\n",
      "train loss:0.043078077882232516\n",
      "train loss:0.08596803747565814\n",
      "train loss:0.044123761026839015\n",
      "train loss:0.0320388041878016\n",
      "train loss:0.05548295832579911\n",
      "train loss:0.09659457538972877\n",
      "train loss:0.04634084998304675\n",
      "train loss:0.08665637770679872\n",
      "train loss:0.06537371533233795\n",
      "train loss:0.09004103322941706\n",
      "train loss:0.05969379828496686\n",
      "train loss:0.0898094705418423\n",
      "train loss:0.029507580696941323\n",
      "train loss:0.0920005693433667\n",
      "train loss:0.0296533087646686\n",
      "train loss:0.04199419135855205\n",
      "train loss:0.06779185525855666\n",
      "train loss:0.06941382845030585\n",
      "train loss:0.05584830096116192\n",
      "train loss:0.024931575136284714\n",
      "train loss:0.023479218486069837\n",
      "train loss:0.02251740214874194\n",
      "train loss:0.09502884497982907\n",
      "train loss:0.054961037469694975\n",
      "train loss:0.04057126429678529\n",
      "train loss:0.05464314495512489\n",
      "train loss:0.01966910979788341\n",
      "train loss:0.08251192425328713\n",
      "train loss:0.05271618116708109\n",
      "train loss:0.011559148881753047\n",
      "train loss:0.051924821418695165\n",
      "train loss:0.029766451511389003\n",
      "train loss:0.04089792244822221\n",
      "train loss:0.006772269975815871\n",
      "train loss:0.015511939553753972\n",
      "train loss:0.025004174226937077\n",
      "train loss:0.018910065715430007\n",
      "train loss:0.02707047822818246\n",
      "train loss:0.01089381668581217\n",
      "train loss:0.10688457142955304\n",
      "train loss:0.012892545448469303\n",
      "train loss:0.019424458124275953\n",
      "train loss:0.1359994690682345\n",
      "train loss:0.022354775536115944\n",
      "train loss:0.04338403037016237\n",
      "train loss:0.11391204812560618\n",
      "train loss:0.06212465854722066\n",
      "train loss:0.036803605678529666\n",
      "train loss:0.04448263267588086\n",
      "train loss:0.025919080226637844\n",
      "train loss:0.06485203188474037\n",
      "train loss:0.01954288150756643\n",
      "train loss:0.018352973048576745\n",
      "train loss:0.11759779250302334\n",
      "train loss:0.025110010013542558\n",
      "train loss:0.028913675975852184\n",
      "train loss:0.06801530572262508\n",
      "train loss:0.03600045041618064\n",
      "train loss:0.11706790614407564\n",
      "train loss:0.16065862586297272\n",
      "train loss:0.02873854297713079\n",
      "train loss:0.04648131521675457\n",
      "train loss:0.05836615512057142\n",
      "train loss:0.017061807843438553\n",
      "train loss:0.051308316016180806\n",
      "train loss:0.02366582801004963\n",
      "train loss:0.05296472053443754\n",
      "train loss:0.013310994838312933\n",
      "train loss:0.0803394553713699\n",
      "train loss:0.09310654970880565\n",
      "train loss:0.07134144322761107\n",
      "train loss:0.07569142927969445\n",
      "train loss:0.02466402887747781\n",
      "train loss:0.010784123082011534\n",
      "train loss:0.0342215545924075\n",
      "train loss:0.052256388021821455\n",
      "train loss:0.058995690903913156\n",
      "train loss:0.024587373260786576\n",
      "train loss:0.043879306546911086\n",
      "train loss:0.013005746742144268\n",
      "train loss:0.026496617255123634\n",
      "train loss:0.01776245648934088\n",
      "train loss:0.07076463155149856\n",
      "train loss:0.01567116611531469\n",
      "train loss:0.05919006084874567\n",
      "train loss:0.034312770383167816\n",
      "train loss:0.051252368143554286\n",
      "train loss:0.016467338458850886\n",
      "train loss:0.046019514951736396\n",
      "train loss:0.03331831200862693\n",
      "train loss:0.01972389870503483\n",
      "train loss:0.04411693376499746\n",
      "train loss:0.07045259373783802\n",
      "train loss:0.022054338051074693\n",
      "train loss:0.029766333197536844\n",
      "train loss:0.04675636746129761\n",
      "train loss:0.019580891105256647\n",
      "train loss:0.021976814702558235\n",
      "train loss:0.10653780742540253\n",
      "train loss:0.03377183413309387\n",
      "train loss:0.0809936512300644\n",
      "train loss:0.13342005848782676\n",
      "train loss:0.09549556942093605\n",
      "train loss:0.011719052599967559\n",
      "train loss:0.025665738698840417\n",
      "train loss:0.04178649220717758\n",
      "train loss:0.04203618488248042\n",
      "train loss:0.05649538627788511\n",
      "train loss:0.08345511700283509\n",
      "train loss:0.026613057965901285\n",
      "train loss:0.06714393632555211\n",
      "train loss:0.02593002183646893\n",
      "train loss:0.03109700436460057\n",
      "train loss:0.03792539481180929\n",
      "train loss:0.04672012277949409\n",
      "train loss:0.1356177876045842\n",
      "train loss:0.0383587809845627\n",
      "train loss:0.06035954835392565\n",
      "train loss:0.11969509529804682\n",
      "train loss:0.05369274233801577\n",
      "train loss:0.13926751691810355\n",
      "train loss:0.04556196705513092\n",
      "train loss:0.022394498852564126\n",
      "train loss:0.028212739299001068\n",
      "train loss:0.03190088041441374\n",
      "train loss:0.07609053295228184\n",
      "train loss:0.07468091730699815\n",
      "train loss:0.054979626969404574\n",
      "train loss:0.15801508674887763\n",
      "train loss:0.02238450711580291\n",
      "train loss:0.015231078142006296\n",
      "train loss:0.12627054463864248\n",
      "train loss:0.12543563550086254\n",
      "train loss:0.016677234314306823\n",
      "train loss:0.01905208605380487\n",
      "train loss:0.061515327054502904\n",
      "train loss:0.04929280442908764\n",
      "train loss:0.020625306876742574\n",
      "train loss:0.031063914423254436\n",
      "train loss:0.044666878152694524\n",
      "train loss:0.04698368431908013\n",
      "train loss:0.08710452797937723\n",
      "train loss:0.032248627228981704\n",
      "train loss:0.07425773256896802\n",
      "train loss:0.04510897764057571\n",
      "train loss:0.03429230048465741\n",
      "train loss:0.04446598112508893\n",
      "train loss:0.03808390906579195\n",
      "train loss:0.01176733245595502\n",
      "train loss:0.06140133317644929\n",
      "train loss:0.012003445315415296\n",
      "train loss:0.018495431594971922\n",
      "train loss:0.01857680054931552\n",
      "train loss:0.09480153814830024\n",
      "train loss:0.03119519922854399\n",
      "train loss:0.0164820453156732\n",
      "train loss:0.010275357443467414\n",
      "train loss:0.04385252806837935\n",
      "train loss:0.034805089896302155\n",
      "train loss:0.04319468547022152\n",
      "train loss:0.015358927373431853\n",
      "train loss:0.06441907239692996\n",
      "train loss:0.019453398417237474\n",
      "train loss:0.04711588312952979\n",
      "train loss:0.040747804095861914\n",
      "train loss:0.01447255487157187\n",
      "train loss:0.012494142814817887\n",
      "train loss:0.0436077155215899\n",
      "train loss:0.05102052325166089\n",
      "train loss:0.07543379957082746\n",
      "train loss:0.12577431955167787\n",
      "train loss:0.058764264422406794\n",
      "train loss:0.059088966758574404\n",
      "train loss:0.038571402643881786\n",
      "train loss:0.0947039785740768\n",
      "train loss:0.06058400553732479\n",
      "train loss:0.03201492689656039\n",
      "train loss:0.0508281034755203\n",
      "train loss:0.04446374414273499\n",
      "train loss:0.03164887854863375\n",
      "train loss:0.013269704710184316\n",
      "train loss:0.08505706064432336\n",
      "train loss:0.014320013162351729\n",
      "train loss:0.06460677255034913\n",
      "train loss:0.04538063338650994\n",
      "train loss:0.03286216505403182\n",
      "train loss:0.08856282947756021\n",
      "train loss:0.02444157856135547\n",
      "train loss:0.08801708759424066\n",
      "train loss:0.03896154176769608\n",
      "train loss:0.030500721004864704\n",
      "train loss:0.02444764874468614\n",
      "train loss:0.04853083184620401\n",
      "train loss:0.038772699645338464\n",
      "train loss:0.046069261833548064\n",
      "train loss:0.038723763646212925\n",
      "train loss:0.08956141078474061\n",
      "train loss:0.06863893494720222\n",
      "train loss:0.05221878518732227\n",
      "train loss:0.05402585434939716\n",
      "train loss:0.035889780240104384\n",
      "train loss:0.010507238689723895\n",
      "train loss:0.04809100504337115\n",
      "train loss:0.08542592483904614\n",
      "train loss:0.018123543461588353\n",
      "train loss:0.026460511215886176\n",
      "train loss:0.04167263768779371\n",
      "train loss:0.02499413877996067\n",
      "train loss:0.024678470981457198\n",
      "train loss:0.06492152545626338\n",
      "train loss:0.020843523179600264\n",
      "train loss:0.018244214416390556\n",
      "train loss:0.025668839889497433\n",
      "train loss:0.06037887996333244\n",
      "train loss:0.026810607477188286\n",
      "train loss:0.08978070220166494\n",
      "train loss:0.05900669580188252\n",
      "train loss:0.043216584882459205\n",
      "train loss:0.04984537816823629\n",
      "train loss:0.01827119156174622\n",
      "train loss:0.021826135147444744\n",
      "train loss:0.02666804705433\n",
      "train loss:0.027903003297024734\n",
      "train loss:0.08177445419783033\n",
      "train loss:0.08881813541730171\n",
      "train loss:0.016028311313866655\n",
      "train loss:0.039024614044810994\n",
      "train loss:0.08762346538342458\n",
      "train loss:0.04310789795913826\n",
      "train loss:0.12925575144611892\n",
      "train loss:0.04993689083805644\n",
      "train loss:0.035138707499548574\n",
      "train loss:0.026991230397156993\n",
      "train loss:0.056971321428043786\n",
      "train loss:0.058103942915407505\n",
      "train loss:0.03802470784231188\n",
      "train loss:0.024351016321631815\n",
      "train loss:0.08219859859237834\n",
      "=== epoch:4, train acc:0.981, test acc:0.98 ===\n",
      "train loss:0.009267111398866814\n",
      "train loss:0.0734556883309534\n",
      "train loss:0.04090882168126238\n",
      "train loss:0.022415662494870068\n",
      "train loss:0.034993099701356604\n",
      "train loss:0.024865121839239442\n",
      "train loss:0.0707990835807786\n",
      "train loss:0.0946930975641646\n",
      "train loss:0.014541194028701708\n",
      "train loss:0.0324371111648402\n",
      "train loss:0.011920308887904065\n",
      "train loss:0.02586174404269701\n",
      "train loss:0.10135098593977528\n",
      "train loss:0.1347865256283168\n",
      "train loss:0.021511736290734537\n",
      "train loss:0.06662548548706715\n",
      "train loss:0.07471487672787722\n",
      "train loss:0.060769957188512504\n",
      "train loss:0.025153607137830075\n",
      "train loss:0.07780864096813743\n",
      "train loss:0.021564771003472566\n",
      "train loss:0.06280534918757612\n",
      "train loss:0.025556959346185407\n",
      "train loss:0.014954091549600009\n",
      "train loss:0.06417689554499523\n",
      "train loss:0.10179537129864498\n",
      "train loss:0.10708464693402651\n",
      "train loss:0.017312488764895502\n",
      "train loss:0.05458942853934524\n",
      "train loss:0.013295002474654536\n",
      "train loss:0.029731839756076538\n",
      "train loss:0.041565668790436924\n",
      "train loss:0.007715649882393385\n",
      "train loss:0.052214238838986245\n",
      "train loss:0.048385424989400086\n",
      "train loss:0.03133168943973314\n",
      "train loss:0.005506236372180553\n",
      "train loss:0.009228592474596507\n",
      "train loss:0.04043936689704768\n",
      "train loss:0.06633309078701612\n",
      "train loss:0.042615105601962716\n",
      "train loss:0.059808324654203646\n",
      "train loss:0.009638200522249184\n",
      "train loss:0.031190677684622675\n",
      "train loss:0.04875235604485366\n",
      "train loss:0.028775466353785743\n",
      "train loss:0.035693692093673415\n",
      "train loss:0.015429295694511467\n",
      "train loss:0.09377387579542817\n",
      "train loss:0.0325748073671125\n",
      "train loss:0.021987080664294934\n",
      "train loss:0.16878318113333848\n",
      "train loss:0.031474184548992375\n",
      "train loss:0.08538432504154878\n",
      "train loss:0.015666398065275908\n",
      "train loss:0.03585409021194894\n",
      "train loss:0.02401849989708788\n",
      "train loss:0.013821552936965647\n",
      "train loss:0.03511312412061817\n",
      "train loss:0.03824463275143806\n",
      "train loss:0.0986217430025096\n",
      "train loss:0.0171332117046995\n",
      "train loss:0.030599059749926022\n",
      "train loss:0.055971812690385016\n",
      "train loss:0.028643998316779582\n",
      "train loss:0.01777092667594799\n",
      "train loss:0.059524020351938045\n",
      "train loss:0.016726663138973377\n",
      "train loss:0.09195727995143795\n",
      "train loss:0.12766351539296597\n",
      "train loss:0.05826419231425201\n",
      "train loss:0.022796926875311128\n",
      "train loss:0.12780335760392458\n",
      "train loss:0.041225205555107794\n",
      "train loss:0.01734126477905307\n",
      "train loss:0.057783882800080344\n",
      "train loss:0.03628303698137432\n",
      "train loss:0.08261883857018944\n",
      "train loss:0.1374651370332543\n",
      "train loss:0.043272712792414465\n",
      "train loss:0.11681096491543898\n",
      "train loss:0.08869833359900682\n",
      "train loss:0.034970653591160306\n",
      "train loss:0.055740669760753585\n",
      "train loss:0.0894816028662815\n",
      "train loss:0.04181053945730756\n",
      "train loss:0.07315810959208992\n",
      "train loss:0.025251877861512145\n",
      "train loss:0.04144258519395081\n",
      "train loss:0.007771955381509428\n",
      "train loss:0.03638784431908417\n",
      "train loss:0.03574823512533512\n",
      "train loss:0.03958705747057596\n",
      "train loss:0.10668094198775423\n",
      "train loss:0.038374772715791\n",
      "train loss:0.020373059161973687\n",
      "train loss:0.019064130895505807\n",
      "train loss:0.02697635954737907\n",
      "train loss:0.04023405424867006\n",
      "train loss:0.01120498080716361\n",
      "train loss:0.03642411719534142\n",
      "train loss:0.08893949096181362\n",
      "train loss:0.04284583929222815\n",
      "train loss:0.020127464227299368\n",
      "train loss:0.1496307185020814\n",
      "train loss:0.006490619798190769\n",
      "train loss:0.02997355449972542\n",
      "train loss:0.015992508773479932\n",
      "train loss:0.07174521020109856\n",
      "train loss:0.03568332633819926\n",
      "train loss:0.01791259255251345\n",
      "train loss:0.012728582395384687\n",
      "train loss:0.06835185034411984\n",
      "train loss:0.03492164864494409\n",
      "train loss:0.018627682850131968\n",
      "train loss:0.021868597880293713\n",
      "train loss:0.018401413662162688\n",
      "train loss:0.032500085344303035\n",
      "train loss:0.015635897620683115\n",
      "train loss:0.06352886154822146\n",
      "train loss:0.03196273963614435\n",
      "train loss:0.03158287420093523\n",
      "train loss:0.04818595793932841\n",
      "train loss:0.011621678664543613\n",
      "train loss:0.055792443215185734\n",
      "train loss:0.1259223854236287\n",
      "train loss:0.022963032522173674\n",
      "train loss:0.0633780359763731\n",
      "train loss:0.0626864345009293\n",
      "train loss:0.0223412996469472\n",
      "train loss:0.048338771167786225\n",
      "train loss:0.02987371215557341\n",
      "train loss:0.0203793833674345\n",
      "train loss:0.0612539031860871\n",
      "train loss:0.08212021754435188\n",
      "train loss:0.018945667743852687\n",
      "train loss:0.016319222958653748\n",
      "train loss:0.07609146746706247\n",
      "train loss:0.12738791703677974\n",
      "train loss:0.029678847472048332\n",
      "train loss:0.20646755144978252\n",
      "train loss:0.0693874383199681\n",
      "train loss:0.03898257423827542\n",
      "train loss:0.018529475531375104\n",
      "train loss:0.0469898612369603\n",
      "train loss:0.03998907398326172\n",
      "train loss:0.02733332275504194\n",
      "train loss:0.0858209757142931\n",
      "train loss:0.04629630449684639\n",
      "train loss:0.026596254822045518\n",
      "train loss:0.03451416833041235\n",
      "train loss:0.051313919102388784\n",
      "train loss:0.06450848548732827\n",
      "train loss:0.0683582872121088\n",
      "train loss:0.043458607303174274\n",
      "train loss:0.019401140724700186\n",
      "train loss:0.020792453422325944\n",
      "train loss:0.12514225428327808\n",
      "train loss:0.03694809682750989\n",
      "train loss:0.01595339616335183\n",
      "train loss:0.014120249287674115\n",
      "train loss:0.09755561151686377\n",
      "train loss:0.1727187326924196\n",
      "train loss:0.03453530290327012\n",
      "train loss:0.02221320534860946\n",
      "train loss:0.020350031329338596\n",
      "train loss:0.026184678678753702\n",
      "train loss:0.006125254553755617\n",
      "train loss:0.03816710323307273\n",
      "train loss:0.038428954187678575\n",
      "train loss:0.036396137395418075\n",
      "train loss:0.014904301730479479\n",
      "train loss:0.07916490365423948\n",
      "train loss:0.027357203142912578\n",
      "train loss:0.03600443908655032\n",
      "train loss:0.07877115109734255\n",
      "train loss:0.014403952168320591\n",
      "train loss:0.025402531844019523\n",
      "train loss:0.01893502787475582\n",
      "train loss:0.05850587939291091\n",
      "train loss:0.011335128514034162\n",
      "train loss:0.010234056063327812\n",
      "train loss:0.09700710718966787\n",
      "train loss:0.11936098668765407\n",
      "train loss:0.037902413734360346\n",
      "train loss:0.047729691556980924\n",
      "train loss:0.03215219473195538\n",
      "train loss:0.05077381731245452\n",
      "train loss:0.028340307192917268\n",
      "train loss:0.013928387188421954\n",
      "train loss:0.025831532957984873\n",
      "train loss:0.07084580987827775\n",
      "train loss:0.0367656465626199\n",
      "train loss:0.04283960368112991\n",
      "train loss:0.03125392565861638\n",
      "train loss:0.048876642232560524\n",
      "train loss:0.041100653190387595\n",
      "train loss:0.018776686988020784\n",
      "train loss:0.02762199434897066\n",
      "train loss:0.019243042546477733\n",
      "train loss:0.007151826718245968\n",
      "train loss:0.012767528328597651\n",
      "train loss:0.09297645945359462\n",
      "train loss:0.07892897432858971\n",
      "train loss:0.03534910632947827\n",
      "train loss:0.009892938507477977\n",
      "train loss:0.03710148202648799\n",
      "train loss:0.020298853186823833\n",
      "train loss:0.05883659305577714\n",
      "train loss:0.0318934053620875\n",
      "train loss:0.04178325754165104\n",
      "train loss:0.038444566241336235\n",
      "train loss:0.010091298104526226\n",
      "train loss:0.02542829158360601\n",
      "train loss:0.010527044990275989\n",
      "train loss:0.028358464246987986\n",
      "train loss:0.02630117114500241\n",
      "train loss:0.02469118077496225\n",
      "train loss:0.02234817212393731\n",
      "train loss:0.041982389308741395\n",
      "train loss:0.04728868147202053\n",
      "train loss:0.05616556456892858\n",
      "train loss:0.04650983350915658\n",
      "train loss:0.04542422362874669\n",
      "train loss:0.008153711524539326\n",
      "train loss:0.03963895922407328\n",
      "train loss:0.005029259112828849\n",
      "train loss:0.07604975208798798\n",
      "train loss:0.0679620818658888\n",
      "train loss:0.01651586518347522\n",
      "train loss:0.03018357276731956\n",
      "train loss:0.06259782305075964\n",
      "train loss:0.08013416677754949\n",
      "train loss:0.023618079695247207\n",
      "train loss:0.09725366034285977\n",
      "train loss:0.03946532886869816\n",
      "train loss:0.03690408955093825\n",
      "train loss:0.07894402688906683\n",
      "train loss:0.03889572186468115\n",
      "train loss:0.04424997860167075\n",
      "train loss:0.027825846480254247\n",
      "train loss:0.03079315105935666\n",
      "train loss:0.049208797189804254\n",
      "train loss:0.01674093679589226\n",
      "train loss:0.04890126500275043\n",
      "train loss:0.08913591648611643\n",
      "train loss:0.0292190077998464\n",
      "train loss:0.08747889018066851\n",
      "train loss:0.020012563615015022\n",
      "train loss:0.07897628274935613\n",
      "train loss:0.03274807144150273\n",
      "train loss:0.009985838434948357\n",
      "train loss:0.15234365162656682\n",
      "train loss:0.01693015813468727\n",
      "train loss:0.024231982876451848\n",
      "train loss:0.03192565896982361\n",
      "train loss:0.0603187426862513\n",
      "train loss:0.029133552123949715\n",
      "train loss:0.029855974426764534\n",
      "train loss:0.048672097864253666\n",
      "train loss:0.035162256197941597\n",
      "train loss:0.09213375760441814\n",
      "train loss:0.04041468416643294\n",
      "train loss:0.013764973161736416\n",
      "train loss:0.07481043883844675\n",
      "train loss:0.056917767235473635\n",
      "train loss:0.07497183303225674\n",
      "train loss:0.030809301650649004\n",
      "train loss:0.017998898797242122\n",
      "train loss:0.08382611962572613\n",
      "train loss:0.01640056712445701\n",
      "train loss:0.07726448786485406\n",
      "train loss:0.03344273685785481\n",
      "train loss:0.05013349215383036\n",
      "train loss:0.04151175356460368\n",
      "train loss:0.03921669811346724\n",
      "train loss:0.01572028915473156\n",
      "train loss:0.015373536845193974\n",
      "train loss:0.06291721430091188\n",
      "train loss:0.009267228424909362\n",
      "train loss:0.032136643660086485\n",
      "train loss:0.05632220303099901\n",
      "train loss:0.013790264877628886\n",
      "train loss:0.042303311049401236\n",
      "train loss:0.029753563007640264\n",
      "train loss:0.0092890569986514\n",
      "train loss:0.08823788221304535\n",
      "train loss:0.055196363048709\n",
      "train loss:0.02518188462671368\n",
      "train loss:0.04893103990721864\n",
      "train loss:0.07092256366285288\n",
      "train loss:0.03998190016777998\n",
      "train loss:0.08487735495246022\n",
      "train loss:0.04402465877698299\n",
      "train loss:0.06841355030600461\n",
      "train loss:0.04037361303495772\n",
      "train loss:0.008585737266315554\n",
      "train loss:0.02512760682669665\n",
      "train loss:0.021252601888270065\n",
      "train loss:0.11002448050414376\n",
      "train loss:0.047499470999719545\n",
      "train loss:0.0930045056742965\n",
      "train loss:0.043043825410540484\n",
      "train loss:0.014552701947165831\n",
      "train loss:0.022398163221704473\n",
      "train loss:0.0156687864677721\n",
      "train loss:0.03215305627079728\n",
      "train loss:0.04294628660268519\n",
      "train loss:0.004761471629239914\n",
      "train loss:0.04048897093381524\n",
      "train loss:0.030931473995105048\n",
      "train loss:0.026915067556981323\n",
      "train loss:0.022600675848077484\n",
      "train loss:0.026771160036461263\n",
      "train loss:0.10585743442276002\n",
      "train loss:0.05564959462003973\n",
      "train loss:0.09612056312041561\n",
      "train loss:0.020441408470283955\n",
      "train loss:0.021419214206077054\n",
      "train loss:0.04793687136091822\n",
      "train loss:0.020295543203857923\n",
      "train loss:0.06318993093710108\n",
      "train loss:0.03328693499834194\n",
      "train loss:0.027506247981388444\n",
      "train loss:0.05663071690398464\n",
      "train loss:0.02593964894794335\n",
      "train loss:0.06672853567401012\n",
      "train loss:0.054328867656060584\n",
      "train loss:0.11746818730549968\n",
      "train loss:0.017497429703139913\n",
      "train loss:0.014272840891180893\n",
      "train loss:0.02098405657852859\n",
      "train loss:0.054207269547093026\n",
      "train loss:0.04728690378600134\n",
      "train loss:0.03524543933164377\n",
      "train loss:0.0147882147205598\n",
      "train loss:0.0241387769945201\n",
      "train loss:0.03539637271884522\n",
      "train loss:0.09026943978938058\n",
      "train loss:0.03366112643307194\n",
      "train loss:0.014069520196089973\n",
      "train loss:0.041141773985144646\n",
      "train loss:0.02131837364692986\n",
      "train loss:0.05129156704169179\n",
      "train loss:0.04738182652968357\n",
      "train loss:0.06688652985298378\n",
      "train loss:0.03184790342068318\n",
      "train loss:0.06506249295125464\n",
      "train loss:0.024390088651856545\n",
      "train loss:0.018657667317042687\n",
      "train loss:0.04522871511561436\n",
      "train loss:0.013318600303039321\n",
      "train loss:0.014021400686671539\n",
      "train loss:0.029524430166028136\n",
      "train loss:0.08001048499242307\n",
      "train loss:0.05824333454315629\n",
      "train loss:0.01853077558865299\n",
      "train loss:0.02085894571854908\n",
      "train loss:0.017682041396255857\n",
      "train loss:0.01725764248922718\n",
      "train loss:0.01232584815223343\n",
      "train loss:0.010214699119613688\n",
      "train loss:0.056938360349630776\n",
      "train loss:0.09338463842611286\n",
      "train loss:0.07035220072752132\n",
      "train loss:0.030475258803108293\n",
      "train loss:0.0597601022106422\n",
      "train loss:0.022495622903711175\n",
      "train loss:0.013829465454974798\n",
      "train loss:0.1392355313716901\n",
      "train loss:0.02792362482063754\n",
      "train loss:0.026683280560331563\n",
      "train loss:0.021193032451019524\n",
      "train loss:0.03010051608022536\n",
      "train loss:0.01949329513025646\n",
      "train loss:0.009733265741439111\n",
      "train loss:0.02898926186955463\n",
      "train loss:0.03501621956379664\n",
      "train loss:0.037273495324256584\n",
      "train loss:0.013444488668790095\n",
      "train loss:0.025417919820844525\n",
      "train loss:0.02513519419460619\n",
      "train loss:0.018043526004291065\n",
      "train loss:0.01187770652880613\n",
      "train loss:0.05787982434434588\n",
      "train loss:0.10548218305424323\n",
      "train loss:0.02094295213100696\n",
      "train loss:0.007195828227431228\n",
      "train loss:0.06410252229258095\n",
      "train loss:0.030657166063783863\n",
      "train loss:0.0499742443263519\n",
      "train loss:0.044315030948843426\n",
      "train loss:0.015386097283303958\n",
      "train loss:0.05920122688004865\n",
      "train loss:0.0654818399416483\n",
      "train loss:0.03481525456563858\n",
      "train loss:0.015463384303668734\n",
      "train loss:0.025156682030442624\n",
      "train loss:0.027650701834285557\n",
      "train loss:0.037031373905631555\n",
      "train loss:0.029424115750958754\n",
      "train loss:0.017018295530222306\n",
      "train loss:0.04664774094318749\n",
      "train loss:0.06256417113995007\n",
      "train loss:0.013075050418751157\n",
      "train loss:0.05119871706185635\n",
      "train loss:0.09634882577777132\n",
      "train loss:0.02411275221281302\n",
      "train loss:0.02089590470243536\n",
      "train loss:0.043560880806417346\n",
      "train loss:0.0458171756510419\n",
      "train loss:0.1258454781028367\n",
      "train loss:0.03796873985977309\n",
      "train loss:0.0688784976015286\n",
      "train loss:0.09245709191013617\n",
      "train loss:0.03001435989049709\n",
      "train loss:0.08388756875910026\n",
      "train loss:0.017760927834873666\n",
      "train loss:0.04502745007683813\n",
      "train loss:0.02617755774991459\n",
      "train loss:0.11033157775819952\n",
      "train loss:0.08573652833376326\n",
      "train loss:0.0722389633818466\n",
      "train loss:0.033119801239505996\n",
      "train loss:0.06458682819435926\n",
      "train loss:0.026911736472107744\n",
      "train loss:0.024353323951680977\n",
      "train loss:0.015017346015573863\n",
      "train loss:0.05584357589786251\n",
      "train loss:0.09347392562407703\n",
      "train loss:0.016722336424462626\n",
      "train loss:0.01705537751960337\n",
      "train loss:0.01799857913361519\n",
      "train loss:0.0037300465123833375\n",
      "train loss:0.10917305320132303\n",
      "train loss:0.01839243866937426\n",
      "train loss:0.017352012315551633\n",
      "train loss:0.021393232531226783\n",
      "train loss:0.04587647630870987\n",
      "train loss:0.07391313611027264\n",
      "train loss:0.032992792380040055\n",
      "train loss:0.06240668408750029\n",
      "train loss:0.03624309457234326\n",
      "train loss:0.01446887936858758\n",
      "train loss:0.07957620062515794\n",
      "train loss:0.1147381211274827\n",
      "train loss:0.012493657115463217\n",
      "train loss:0.07082792578992501\n",
      "train loss:0.10336847987210772\n",
      "train loss:0.021548605107227704\n",
      "train loss:0.006935051753348163\n",
      "train loss:0.028216426760574363\n",
      "train loss:0.015346260695577159\n",
      "train loss:0.03535510751860738\n",
      "train loss:0.09715698883386643\n",
      "train loss:0.03859936665163154\n",
      "train loss:0.032097824589345805\n",
      "train loss:0.05178512570737556\n",
      "train loss:0.05501208821732229\n",
      "train loss:0.031068139777505208\n",
      "train loss:0.051493760645918873\n",
      "train loss:0.0143512190938168\n",
      "train loss:0.03793063547961751\n",
      "train loss:0.06440073984386405\n",
      "train loss:0.014973257134781634\n",
      "train loss:0.030567473962052415\n",
      "train loss:0.05468297740218188\n",
      "train loss:0.016985161704221954\n",
      "train loss:0.010150090362040074\n",
      "train loss:0.010168708354235792\n",
      "train loss:0.03462274665324431\n",
      "train loss:0.06824907723994471\n",
      "train loss:0.0336456512999801\n",
      "train loss:0.032233274567355\n",
      "train loss:0.025691933217034117\n",
      "train loss:0.020023013381696018\n",
      "train loss:0.028548608567930435\n",
      "train loss:0.031637121875192055\n",
      "train loss:0.01808279198503174\n",
      "train loss:0.01183725891771726\n",
      "train loss:0.05216095547751612\n",
      "train loss:0.012991390121234143\n",
      "train loss:0.0610621544259159\n",
      "train loss:0.010452467155756375\n",
      "train loss:0.02627733431871285\n",
      "train loss:0.01820876878974365\n",
      "train loss:0.014124821903727398\n",
      "train loss:0.026181073496781407\n",
      "train loss:0.03161608861337836\n",
      "train loss:0.0836617567825756\n",
      "train loss:0.004429301694378291\n",
      "train loss:0.05038028548334303\n",
      "train loss:0.03514297706252289\n",
      "train loss:0.07765843957908494\n",
      "train loss:0.012059293067058163\n",
      "train loss:0.0630105992358728\n",
      "train loss:0.019638739453739076\n",
      "train loss:0.05656604116807997\n",
      "train loss:0.2130525105135834\n",
      "train loss:0.08307990057051709\n",
      "train loss:0.021669372295280237\n",
      "train loss:0.017587573255252446\n",
      "train loss:0.036966776921038516\n",
      "train loss:0.04723161948706079\n",
      "train loss:0.010981400695603763\n",
      "train loss:0.019937773887513995\n",
      "train loss:0.0592203125244495\n",
      "train loss:0.014086074448863879\n",
      "train loss:0.03284997177920205\n",
      "train loss:0.07109457239012283\n",
      "train loss:0.009289359918116077\n",
      "train loss:0.013657603973879351\n",
      "train loss:0.118222563410778\n",
      "train loss:0.01791539237266922\n",
      "train loss:0.017158243591704793\n",
      "train loss:0.03229923350377884\n",
      "train loss:0.04054358588643088\n",
      "train loss:0.03489785766685311\n",
      "train loss:0.033962811091267496\n",
      "train loss:0.039893256896881814\n",
      "train loss:0.01261259072058407\n",
      "train loss:0.019827996117373542\n",
      "train loss:0.03254070307108268\n",
      "train loss:0.03531432409537171\n",
      "train loss:0.014135196144626987\n",
      "train loss:0.036776089637746326\n",
      "train loss:0.025067670178159366\n",
      "train loss:0.008570816607872922\n",
      "train loss:0.022046489737504036\n",
      "train loss:0.013159689725570805\n",
      "train loss:0.01449013916238844\n",
      "train loss:0.029188743016501362\n",
      "train loss:0.02599698793592688\n",
      "train loss:0.023515372663073665\n",
      "train loss:0.031553698882075695\n",
      "train loss:0.04107458163529579\n",
      "train loss:0.012923559582968829\n",
      "train loss:0.03339580509492306\n",
      "train loss:0.05133504099367792\n",
      "train loss:0.13935493669658627\n",
      "train loss:0.010419387659046722\n",
      "train loss:0.08850110625835375\n",
      "train loss:0.03026686273523173\n",
      "train loss:0.047460147932589666\n",
      "train loss:0.036656876723740156\n",
      "train loss:0.04244564470176782\n",
      "train loss:0.03061460174971972\n",
      "train loss:0.030706476341350628\n",
      "train loss:0.012317703241081524\n",
      "train loss:0.027016514457182256\n",
      "train loss:0.04045417595304733\n",
      "train loss:0.023380073334544105\n",
      "train loss:0.0032423991565243625\n",
      "train loss:0.05616929621396693\n",
      "train loss:0.01672332200031309\n",
      "train loss:0.018578735771619635\n",
      "train loss:0.011671404899541372\n",
      "train loss:0.019674009169338626\n",
      "train loss:0.009853406392972412\n",
      "train loss:0.019306608815701277\n",
      "train loss:0.04465546694043272\n",
      "train loss:0.023553716469809468\n",
      "train loss:0.06496945188047247\n",
      "train loss:0.014922663807192031\n",
      "train loss:0.03468635584754056\n",
      "train loss:0.013671118971536505\n",
      "train loss:0.02170807884164967\n",
      "train loss:0.022991587252634556\n",
      "train loss:0.020453579918366143\n",
      "train loss:0.07146878241016756\n",
      "train loss:0.021454978879144382\n",
      "train loss:0.026492160319286553\n",
      "train loss:0.014332578532320638\n",
      "train loss:0.04889578800664937\n",
      "train loss:0.004638651987086684\n",
      "train loss:0.04243780273755309\n",
      "train loss:0.005275846712656448\n",
      "train loss:0.019466999582230842\n",
      "train loss:0.011276490493191588\n",
      "train loss:0.020799341565361798\n",
      "train loss:0.015819572615350986\n",
      "train loss:0.02052442544940757\n",
      "train loss:0.028882833800710585\n",
      "train loss:0.05480013427534318\n",
      "train loss:0.025032117309891282\n",
      "train loss:0.03209343519279377\n",
      "train loss:0.021601958883675394\n",
      "train loss:0.046270172003674316\n",
      "train loss:0.016508031156965256\n",
      "train loss:0.02127872264614371\n",
      "train loss:0.005721934290807973\n",
      "train loss:0.0048435469580056815\n",
      "train loss:0.03382006304351393\n",
      "train loss:0.012328337047983355\n",
      "train loss:0.049072561212205254\n",
      "train loss:0.00972854896055069\n",
      "train loss:0.013982533369506452\n",
      "train loss:0.09578691903376418\n",
      "train loss:0.11929779036627958\n",
      "train loss:0.009108781589952191\n",
      "=== epoch:5, train acc:0.987, test acc:0.986 ===\n",
      "train loss:0.009264005087205911\n",
      "train loss:0.013717391552548681\n",
      "train loss:0.016037104936129736\n",
      "train loss:0.00921226754396662\n",
      "train loss:0.01401946969994162\n",
      "train loss:0.02273975928489275\n",
      "train loss:0.009365897317416242\n",
      "train loss:0.06713452808851972\n",
      "train loss:0.03543438696373087\n",
      "train loss:0.012522094544311822\n",
      "train loss:0.0481338374454243\n",
      "train loss:0.022499858179847246\n",
      "train loss:0.07868611765047366\n",
      "train loss:0.035553417420819355\n",
      "train loss:0.07887193067480071\n",
      "train loss:0.023593578876289514\n",
      "train loss:0.019606128128191083\n",
      "train loss:0.015947582220585066\n",
      "train loss:0.01785642624454982\n",
      "train loss:0.0385563662103656\n",
      "train loss:0.009435830106228072\n",
      "train loss:0.15490457133632518\n",
      "train loss:0.04949791935610333\n",
      "train loss:0.03265482553161818\n",
      "train loss:0.020906330857045963\n",
      "train loss:0.07389236504572379\n",
      "train loss:0.055359041337668814\n",
      "train loss:0.018179884978034973\n",
      "train loss:0.051513955176737784\n",
      "train loss:0.024539919082476214\n",
      "train loss:0.03523766176393807\n",
      "train loss:0.01725327967465087\n",
      "train loss:0.04817363663391441\n",
      "train loss:0.009573852772404417\n",
      "train loss:0.02598541965783885\n",
      "train loss:0.0561654027445131\n",
      "train loss:0.04917101935613743\n",
      "train loss:0.012917452513050511\n",
      "train loss:0.0620580747151843\n",
      "train loss:0.12202580162601062\n",
      "train loss:0.02154664307192229\n",
      "train loss:0.008110026404238154\n",
      "train loss:0.007236518198261032\n",
      "train loss:0.016387094187958126\n",
      "train loss:0.0288973454650846\n",
      "train loss:0.036780458661263234\n",
      "train loss:0.04283450939196935\n",
      "train loss:0.0016560932004058924\n",
      "train loss:0.002443828586551809\n",
      "train loss:0.018439575315332742\n",
      "train loss:0.012679981304325134\n",
      "train loss:0.026418736236259364\n",
      "train loss:0.011517034678717238\n",
      "train loss:0.011546356582934325\n",
      "train loss:0.04015353616733641\n",
      "train loss:0.020552341658986818\n",
      "train loss:0.007073888924881497\n",
      "train loss:0.00977034070097405\n",
      "train loss:0.05437510699064635\n",
      "train loss:0.009812357841146093\n",
      "train loss:0.019274316795111456\n",
      "train loss:0.011830779402416303\n",
      "train loss:0.0066993251521207785\n",
      "train loss:0.013952648140253012\n",
      "train loss:0.02042042110629276\n",
      "train loss:0.09044257904463607\n",
      "train loss:0.01883326866364143\n",
      "train loss:0.03613075719793022\n",
      "train loss:0.0342810204378607\n",
      "train loss:0.011067235491330351\n",
      "train loss:0.05583456679856033\n",
      "train loss:0.01742564588234721\n",
      "train loss:0.02909400229019524\n",
      "train loss:0.005403571945970068\n",
      "train loss:0.030077299506443732\n",
      "train loss:0.007885469481607328\n",
      "train loss:0.01458440911645661\n",
      "train loss:0.01719641035155511\n",
      "train loss:0.0861127250795256\n",
      "train loss:0.027482237012146693\n",
      "train loss:0.012908641281631121\n",
      "train loss:0.027590937847835218\n",
      "train loss:0.020530480504494422\n",
      "train loss:0.02054165756955957\n",
      "train loss:0.011193412622431452\n",
      "train loss:0.008760538440158104\n",
      "train loss:0.048054931996704034\n",
      "train loss:0.014670754004209132\n",
      "train loss:0.01723093527450216\n",
      "train loss:0.010051460501790064\n",
      "train loss:0.02050318124210905\n",
      "train loss:0.07550405703622692\n",
      "train loss:0.0059570669799918685\n",
      "train loss:0.007463438529166871\n",
      "train loss:0.03582879780006975\n",
      "train loss:0.009767161577918362\n",
      "train loss:0.05209260073493453\n",
      "train loss:0.05970253801783972\n",
      "train loss:0.014654342649183734\n",
      "train loss:0.007604070038873943\n",
      "train loss:0.021444345354416146\n",
      "train loss:0.033991970465721694\n",
      "train loss:0.004464701635151384\n",
      "train loss:0.0476660591120315\n",
      "train loss:0.056397572777273634\n",
      "train loss:0.019579724848427592\n",
      "train loss:0.06460393689572402\n",
      "train loss:0.02227419675428499\n",
      "train loss:0.03939163015710516\n",
      "train loss:0.026168154997966157\n",
      "train loss:0.006788591797059178\n",
      "train loss:0.01205234659478286\n",
      "train loss:0.016130089689975168\n",
      "train loss:0.044886586205497174\n",
      "train loss:0.019990199746957024\n",
      "train loss:0.0495835559552058\n",
      "train loss:0.011633787363209951\n",
      "train loss:0.009294387856295539\n",
      "train loss:0.07687869734664182\n",
      "train loss:0.016967968068636027\n",
      "train loss:0.034714643174812196\n",
      "train loss:0.016567977246396647\n",
      "train loss:0.005607443503011333\n",
      "train loss:0.01803010850851724\n",
      "train loss:0.011282555315692363\n",
      "train loss:0.05574101032852215\n",
      "train loss:0.01081701797713892\n",
      "train loss:0.020679661987323548\n",
      "train loss:0.03509085841321065\n",
      "train loss:0.017511958069528558\n",
      "train loss:0.019398669411369404\n",
      "train loss:0.0325012099570944\n",
      "train loss:0.022898019798142887\n",
      "train loss:0.030385890268320604\n",
      "train loss:0.02089538187883747\n",
      "train loss:0.04417731370400772\n",
      "train loss:0.013848130317406435\n",
      "train loss:0.044687347226678005\n",
      "train loss:0.04594990801465754\n",
      "train loss:0.023505668967301857\n",
      "train loss:0.02709139777220283\n",
      "train loss:0.03254957032901077\n",
      "train loss:0.0036521489876410883\n",
      "train loss:0.012436121840135303\n",
      "train loss:0.0344772105635747\n",
      "train loss:0.019536546165482817\n",
      "train loss:0.014086522740285967\n",
      "train loss:0.014421633228510372\n",
      "train loss:0.038619865093547416\n",
      "train loss:0.03473507231292896\n",
      "train loss:0.0049854162101440535\n",
      "train loss:0.017358354776149408\n",
      "train loss:0.01698972202199379\n",
      "train loss:0.03063908953086046\n",
      "train loss:0.022123070770956526\n",
      "train loss:0.0631641396367038\n",
      "train loss:0.013348677776926547\n",
      "train loss:0.08063550444428756\n",
      "train loss:0.04385235603071949\n",
      "train loss:0.041105755725262136\n",
      "train loss:0.017420919171771\n",
      "train loss:0.015122825842872159\n",
      "train loss:0.0065925479796459865\n",
      "train loss:0.0215984744397116\n",
      "train loss:0.02198754277829009\n",
      "train loss:0.04080017945456446\n",
      "train loss:0.019217997339579844\n",
      "train loss:0.014628106698291616\n",
      "train loss:0.05325965586659326\n",
      "train loss:0.044436008246796124\n",
      "train loss:0.022466811148196673\n",
      "train loss:0.01964538007734901\n",
      "train loss:0.04001767036662605\n",
      "train loss:0.027819590359470573\n",
      "train loss:0.08711199391709941\n",
      "train loss:0.0041270330800267955\n",
      "train loss:0.029038368796921524\n",
      "train loss:0.008205972055797981\n",
      "train loss:0.02521930725355029\n",
      "train loss:0.008577363889734537\n",
      "train loss:0.036772840963224376\n",
      "train loss:0.046635431825045856\n",
      "train loss:0.0235706145974235\n",
      "train loss:0.010183794263560832\n",
      "train loss:0.024523202894313742\n",
      "train loss:0.04017044236576205\n",
      "train loss:0.010950533319148158\n",
      "train loss:0.022099287921686157\n",
      "train loss:0.06158033998485264\n",
      "train loss:0.02902586162817744\n",
      "train loss:0.014110423740960833\n",
      "train loss:0.03840660229790968\n",
      "train loss:0.00970301746338504\n",
      "train loss:0.014633661887037613\n",
      "train loss:0.02085573521616137\n",
      "train loss:0.017925154161721094\n",
      "train loss:0.002359785833466176\n",
      "train loss:0.03416766699070045\n",
      "train loss:0.014577992386879603\n",
      "train loss:0.10192922203184791\n",
      "train loss:0.01624417607284327\n",
      "train loss:0.006359824964761867\n",
      "train loss:0.02887883176941024\n",
      "train loss:0.020382022872795513\n",
      "train loss:0.11352637186010256\n",
      "train loss:0.03749848731906922\n",
      "train loss:0.029147649360440214\n",
      "train loss:0.029209411940031384\n",
      "train loss:0.030302186766008442\n",
      "train loss:0.01590215142550902\n",
      "train loss:0.04351020283308526\n",
      "train loss:0.03692162026343988\n",
      "train loss:0.03387753548520085\n",
      "train loss:0.002024868217035416\n",
      "train loss:0.010466217567706429\n",
      "train loss:0.00718403752362773\n",
      "train loss:0.0022949698053222364\n",
      "train loss:0.016170264593431415\n",
      "train loss:0.04895182541416709\n",
      "train loss:0.0101964881933215\n",
      "train loss:0.031058720013081943\n",
      "train loss:0.007962839676019533\n",
      "train loss:0.025511527802272425\n",
      "train loss:0.08115443267128841\n",
      "train loss:0.005758061291654667\n",
      "train loss:0.015669698153093613\n",
      "train loss:0.048953072305185884\n",
      "train loss:0.020997522516972667\n",
      "train loss:0.029936410410756783\n",
      "train loss:0.035836923513214214\n",
      "train loss:0.016433079551720547\n",
      "train loss:0.07267953391229946\n",
      "train loss:0.03301423507446951\n",
      "train loss:0.03271398274695943\n",
      "train loss:0.050936870386856244\n",
      "train loss:0.05871369683662416\n",
      "train loss:0.01333684097690562\n",
      "train loss:0.008132071191308815\n",
      "train loss:0.0024237591951963545\n",
      "train loss:0.01716516143968546\n",
      "train loss:0.013080733206778572\n",
      "train loss:0.11029328213334541\n",
      "train loss:0.10243775222130783\n",
      "train loss:0.03071213583257447\n",
      "train loss:0.022681981281302898\n",
      "train loss:0.013339295436157153\n",
      "train loss:0.03469217495564043\n",
      "train loss:0.03134669157404456\n",
      "train loss:0.1082572772228746\n",
      "train loss:0.004116826145328024\n",
      "train loss:0.05692979483490536\n",
      "train loss:0.0597984166364772\n",
      "train loss:0.010831061123802466\n",
      "train loss:0.03800102933351177\n",
      "train loss:0.021916240461276323\n",
      "train loss:0.06732096582820599\n",
      "train loss:0.017393289277141354\n",
      "train loss:0.0285985571415284\n",
      "train loss:0.03793289769378259\n",
      "train loss:0.04251183760157101\n",
      "train loss:0.034996573969613975\n",
      "train loss:0.015354626509350386\n",
      "train loss:0.11038639845578432\n",
      "train loss:0.008956645557290704\n",
      "train loss:0.015343676092030722\n",
      "train loss:0.019725532386958643\n",
      "train loss:0.01630565049872287\n",
      "train loss:0.020418327460058883\n",
      "train loss:0.08076313042261793\n",
      "train loss:0.026063174163270485\n",
      "train loss:0.015923180783270804\n",
      "train loss:0.029010369328351916\n",
      "train loss:0.03284274863659318\n",
      "train loss:0.04754809750693687\n",
      "train loss:0.01271797624285007\n",
      "train loss:0.017928757144739266\n",
      "train loss:0.02085570930170042\n",
      "train loss:0.004228757361865321\n",
      "train loss:0.04911177384281255\n",
      "train loss:0.02292902084266184\n",
      "train loss:0.048851214420702005\n",
      "train loss:0.05036962000656565\n",
      "train loss:0.027712008736465753\n",
      "train loss:0.012785310895081996\n",
      "train loss:0.029480993319492686\n",
      "train loss:0.022432102227535503\n",
      "train loss:0.053154200924909606\n",
      "train loss:0.014038474916132184\n",
      "train loss:0.016379708465366797\n",
      "train loss:0.011262865548654822\n",
      "train loss:0.007411143368961086\n",
      "train loss:0.02501624686914479\n",
      "train loss:0.006743499993320336\n",
      "train loss:0.007767398597702726\n",
      "train loss:0.011944425049239388\n",
      "train loss:0.017628692966335492\n",
      "train loss:0.01183849967081252\n",
      "train loss:0.01861422834554288\n",
      "train loss:0.007021680304988197\n",
      "train loss:0.019677723876983976\n",
      "train loss:0.06032641873035703\n",
      "train loss:0.034942391502327495\n",
      "train loss:0.008289504160546735\n",
      "train loss:0.03156481874164346\n",
      "train loss:0.018957306524081344\n",
      "train loss:0.01137075799262621\n",
      "train loss:0.01235921140844633\n",
      "train loss:0.01878653946746258\n",
      "train loss:0.025471177604137342\n",
      "train loss:0.045735903252935826\n",
      "train loss:0.040699638423761904\n",
      "train loss:0.03864046829409265\n",
      "train loss:0.036863975556474825\n",
      "train loss:0.022070799099240482\n",
      "train loss:0.009201299346231427\n",
      "train loss:0.04749622308678053\n",
      "train loss:0.006755015838361349\n",
      "train loss:0.008760323906403833\n",
      "train loss:0.05975892718023604\n",
      "train loss:0.020633445875319657\n",
      "train loss:0.009381895253327282\n",
      "train loss:0.03714641386473318\n",
      "train loss:0.029746975411881236\n",
      "train loss:0.05184071701604396\n",
      "train loss:0.04112335210296699\n",
      "train loss:0.028218781426535385\n",
      "train loss:0.020921232875204178\n",
      "train loss:0.007203386454668885\n",
      "train loss:0.0227352438259244\n",
      "train loss:0.03299841204833232\n",
      "train loss:0.03097064156282943\n",
      "train loss:0.029119463229181575\n",
      "train loss:0.013822071857225194\n",
      "train loss:0.017076822088460557\n",
      "train loss:0.0072678896782565105\n",
      "train loss:0.009012012658136866\n",
      "train loss:0.00721456361517857\n",
      "train loss:0.052199558954923765\n",
      "train loss:0.02163059858602815\n",
      "train loss:0.009696074393163903\n",
      "train loss:0.007951240640944936\n",
      "train loss:0.033051615933400044\n",
      "train loss:0.05252924252092521\n",
      "train loss:0.13503077259082563\n",
      "train loss:0.01637457258213501\n",
      "train loss:0.024938602760516616\n",
      "train loss:0.04819783779353934\n",
      "train loss:0.015171329220630492\n",
      "train loss:0.021962375480813433\n",
      "train loss:0.01027601379870172\n",
      "train loss:0.02966478769036497\n",
      "train loss:0.03214063770254471\n",
      "train loss:0.015691238500378485\n",
      "train loss:0.007037062542404013\n",
      "train loss:0.06207288503046779\n",
      "train loss:0.0042183829636143364\n",
      "train loss:0.04302356539226732\n",
      "train loss:0.006330101034593035\n",
      "train loss:0.012468463920744072\n",
      "train loss:0.03423390923006719\n",
      "train loss:0.010780515882450836\n",
      "train loss:0.003508465292512558\n",
      "train loss:0.008300275612125872\n",
      "train loss:0.1224734426045721\n",
      "train loss:0.007010977518671916\n",
      "train loss:0.006925479838603071\n",
      "train loss:0.01342067886963238\n",
      "train loss:0.019202820487185305\n",
      "train loss:0.0024025776660783447\n",
      "train loss:0.021870936134281118\n",
      "train loss:0.040719038001824547\n",
      "train loss:0.02636339037790411\n",
      "train loss:0.008285744615922875\n",
      "train loss:0.011026446829249342\n",
      "train loss:0.09502164711520918\n",
      "train loss:0.0046710611134097154\n",
      "train loss:0.008823266355734353\n",
      "train loss:0.012653402587717657\n",
      "train loss:0.05053680120370482\n",
      "train loss:0.02427155545543869\n",
      "train loss:0.028012277003945617\n",
      "train loss:0.04942361951175501\n",
      "train loss:0.015806049236938357\n",
      "train loss:0.03394248775284193\n",
      "train loss:0.08403512306424504\n",
      "train loss:0.054292114777476586\n",
      "train loss:0.06400188871237396\n",
      "train loss:0.006601116683205875\n",
      "train loss:0.06932998653213512\n",
      "train loss:0.03210932945589596\n",
      "train loss:0.012657742899689444\n",
      "train loss:0.02613760707074252\n",
      "train loss:0.0039522482975366405\n",
      "train loss:0.019414647584336368\n",
      "train loss:0.02471328417548747\n",
      "train loss:0.0736519705363823\n",
      "train loss:0.015561889755226046\n",
      "train loss:0.041431827884926144\n",
      "train loss:0.013763396240288555\n",
      "train loss:0.028363364196918598\n",
      "train loss:0.027309506879026467\n",
      "train loss:0.006724120007386096\n",
      "train loss:0.03563907829056104\n",
      "train loss:0.02719434869316777\n",
      "train loss:0.02480880443902322\n",
      "train loss:0.036790722867377265\n",
      "train loss:0.006470999274962679\n",
      "train loss:0.004337734645927336\n",
      "train loss:0.04133887022545204\n",
      "train loss:0.01747472348956477\n",
      "train loss:0.06233457611194912\n",
      "train loss:0.06110131418179187\n",
      "train loss:0.03713932395047978\n",
      "train loss:0.08561875180790944\n",
      "train loss:0.025877264013806616\n",
      "train loss:0.024370296088872655\n",
      "train loss:0.021989027180953084\n",
      "train loss:0.007788654488468529\n",
      "train loss:0.015183269715549938\n",
      "train loss:0.019239540817263964\n",
      "train loss:0.06469467314508628\n",
      "train loss:0.008526594688016345\n",
      "train loss:0.017677171216309604\n",
      "train loss:0.0178739600720206\n",
      "train loss:0.06825512390355916\n",
      "train loss:0.07503719040350172\n",
      "train loss:0.030984290900212662\n",
      "train loss:0.01914462281657116\n",
      "train loss:0.03806274974802929\n",
      "train loss:0.005070905782879399\n",
      "train loss:0.06447482739766441\n",
      "train loss:0.009618311664951218\n",
      "train loss:0.006967712332379894\n",
      "train loss:0.01730751360699722\n",
      "train loss:0.04540637151461967\n",
      "train loss:0.01609749431786293\n",
      "train loss:0.03578223220416558\n",
      "train loss:0.008966754640980052\n",
      "train loss:0.029732593527834278\n",
      "train loss:0.008053997236081677\n",
      "train loss:0.016662463336297624\n",
      "train loss:0.013286565084709627\n",
      "train loss:0.0033346736392339676\n",
      "train loss:0.007457551603980564\n",
      "train loss:0.032230423057828084\n",
      "train loss:0.007678320388308623\n",
      "train loss:0.06418115592368535\n",
      "train loss:0.03275217997648405\n",
      "train loss:0.007643234066517969\n",
      "train loss:0.07539863547064024\n",
      "train loss:0.02813835716856888\n",
      "train loss:0.014400903041591645\n",
      "train loss:0.022828532628326122\n",
      "train loss:0.027274253991274634\n",
      "train loss:0.00942366404499332\n",
      "train loss:0.05664936665235851\n",
      "train loss:0.046619258905917886\n",
      "train loss:0.1199876264726927\n",
      "train loss:0.0060895017471528255\n",
      "train loss:0.007301788862142543\n",
      "train loss:0.0018069983427334627\n",
      "train loss:0.056757251928830604\n",
      "train loss:0.014474938381476683\n",
      "train loss:0.027530727483947234\n",
      "train loss:0.0374088607404994\n",
      "train loss:0.022228685768109265\n",
      "train loss:0.0134364724535413\n",
      "train loss:0.031588541123291694\n",
      "train loss:0.017471996579195986\n",
      "train loss:0.008597554155373749\n",
      "train loss:0.03430629829134929\n",
      "train loss:0.01083155554504379\n",
      "train loss:0.008741144278690844\n",
      "train loss:0.02835274469422082\n",
      "train loss:0.015819020894986525\n",
      "train loss:0.033117593385513754\n",
      "train loss:0.03237908490520248\n",
      "train loss:0.04741009805568011\n",
      "train loss:0.012236562314398966\n",
      "train loss:0.05533420898446875\n",
      "train loss:0.011005655630678655\n",
      "train loss:0.015882259890650438\n",
      "train loss:0.013144372399273716\n",
      "train loss:0.011930193143975486\n",
      "train loss:0.017736952363625175\n",
      "train loss:0.0062211096149251376\n",
      "train loss:0.01573223890817619\n",
      "train loss:0.010117006877001986\n",
      "train loss:0.010928158043910324\n",
      "train loss:0.03718130932790275\n",
      "train loss:0.06319922471092977\n",
      "train loss:0.01663196653803554\n",
      "train loss:0.014656608265981625\n",
      "train loss:0.028221476265565978\n",
      "train loss:0.03201896296063238\n",
      "train loss:0.03569974529830516\n",
      "train loss:0.030026661147451117\n",
      "train loss:0.05973041005445587\n",
      "train loss:0.01614040175194119\n",
      "train loss:0.004743692755166386\n",
      "train loss:0.03140301508189223\n",
      "train loss:0.040874904563955365\n",
      "train loss:0.014356999774044907\n",
      "train loss:0.08817344289534029\n",
      "train loss:0.006667262767528425\n",
      "train loss:0.013386334992617592\n",
      "train loss:0.008758931467823456\n",
      "train loss:0.012530219529568655\n",
      "train loss:0.019156659975820625\n",
      "train loss:0.033376715303639476\n",
      "train loss:0.010032980414446422\n",
      "train loss:0.012093806386381712\n",
      "train loss:0.00919878546281749\n",
      "train loss:0.04073145496333758\n",
      "train loss:0.048879499156495435\n",
      "train loss:0.03328771195426567\n",
      "train loss:0.057429705643608074\n",
      "train loss:0.03877892057996057\n",
      "train loss:0.02506475711023512\n",
      "train loss:0.03360323389893279\n",
      "train loss:0.013259632507364226\n",
      "train loss:0.028973031810428748\n",
      "train loss:0.06306817194639405\n",
      "train loss:0.04876322577213351\n",
      "train loss:0.022951393497088916\n",
      "train loss:0.04960113124013523\n",
      "train loss:0.01870595402156791\n",
      "train loss:0.08719279149727191\n",
      "train loss:0.006596948838197404\n",
      "train loss:0.04623383006791862\n",
      "train loss:0.0537128539485485\n",
      "train loss:0.03299365237821394\n",
      "train loss:0.008208975591396\n",
      "train loss:0.00705021499163182\n",
      "train loss:0.03993444609753706\n",
      "train loss:0.024559725589816273\n",
      "train loss:0.015287299329073181\n",
      "train loss:0.03399388588621326\n",
      "train loss:0.009521445306421133\n",
      "train loss:0.011041577522752012\n",
      "train loss:0.016429955750042434\n",
      "train loss:0.030238213141998035\n",
      "train loss:0.03207695851858105\n",
      "train loss:0.07290803946407783\n",
      "train loss:0.04158188254962852\n",
      "train loss:0.002493034133758482\n",
      "train loss:0.03514734946762214\n",
      "train loss:0.017866492389313152\n",
      "train loss:0.060495116478951144\n",
      "train loss:0.027090032222242096\n",
      "train loss:0.012100226543951216\n",
      "train loss:0.016564761281235577\n",
      "train loss:0.03418792745509518\n",
      "train loss:0.014476585734266537\n",
      "train loss:0.05060438988354821\n",
      "train loss:0.01753845309466444\n",
      "train loss:0.020129745479453735\n",
      "train loss:0.008300151050849792\n",
      "train loss:0.014654815068426842\n",
      "train loss:0.008194968141109139\n",
      "train loss:0.01133525265597571\n",
      "train loss:0.08542953049349718\n",
      "train loss:0.011777910081637059\n",
      "train loss:0.004477690195277174\n",
      "train loss:0.0963133820881919\n",
      "train loss:0.012259716326788324\n",
      "train loss:0.016228348584951983\n",
      "train loss:0.01666123611022878\n",
      "train loss:0.012497096407802357\n",
      "train loss:0.032276467097299484\n",
      "train loss:0.05082047273158865\n",
      "train loss:0.003485132602395209\n",
      "train loss:0.014591100711566263\n",
      "train loss:0.023703687438961202\n",
      "train loss:0.00516823177842837\n",
      "train loss:0.012888013181883146\n",
      "train loss:0.011115702391418352\n",
      "train loss:0.011108498261021997\n",
      "train loss:0.019654055541490627\n",
      "train loss:0.00722285239732034\n",
      "train loss:0.012182939558320146\n",
      "train loss:0.015220546850057503\n",
      "train loss:0.028646337695051257\n",
      "train loss:0.0263573220358823\n",
      "train loss:0.005934212172384995\n",
      "train loss:0.006988798621343051\n",
      "train loss:0.03226764580895812\n",
      "train loss:0.11893403719986365\n",
      "train loss:0.007592033236092599\n",
      "train loss:0.08564473902528423\n",
      "train loss:0.05857418268689532\n",
      "train loss:0.042708082310912075\n",
      "train loss:0.025871181224520062\n",
      "train loss:0.02040974089005555\n",
      "train loss:0.002618196691994301\n",
      "train loss:0.02635874340379799\n",
      "train loss:0.008184634830988021\n",
      "train loss:0.029051159153800618\n",
      "train loss:0.01824264138348659\n",
      "train loss:0.08119685160867526\n",
      "=== epoch:6, train acc:0.991, test acc:0.979 ===\n",
      "train loss:0.004395531521804777\n",
      "train loss:0.01610739580660482\n",
      "train loss:0.023825303887296614\n",
      "train loss:0.012146120852106763\n",
      "train loss:0.04632545717726238\n",
      "train loss:0.008046112840542446\n",
      "train loss:0.00954814371725826\n",
      "train loss:0.016831507423086988\n",
      "train loss:0.017533401862088956\n",
      "train loss:0.048288290719819355\n",
      "train loss:0.009508192548155258\n",
      "train loss:0.017596817076684108\n",
      "train loss:0.01986658789541277\n",
      "train loss:0.007853619590369539\n",
      "train loss:0.011152637290441752\n",
      "train loss:0.020970153438706406\n",
      "train loss:0.018015335178827772\n",
      "train loss:0.008946344519930332\n",
      "train loss:0.025918618169914465\n",
      "train loss:0.0774718246569379\n",
      "train loss:0.02750580868328333\n",
      "train loss:0.013807487133741768\n",
      "train loss:0.011631766677345444\n",
      "train loss:0.010540624961097833\n",
      "train loss:0.021177224198563795\n",
      "train loss:0.06504560573811098\n",
      "train loss:0.02211782477722131\n",
      "train loss:0.023743560308207165\n",
      "train loss:0.10599904976766415\n",
      "train loss:0.017083112424891543\n",
      "train loss:0.060768639701160525\n",
      "train loss:0.007875195886444263\n",
      "train loss:0.010154957130471225\n",
      "train loss:0.047561908937139824\n",
      "train loss:0.055741363029561085\n",
      "train loss:0.008905467108816316\n",
      "train loss:0.025617819167720084\n",
      "train loss:0.04052518674960127\n",
      "train loss:0.03863156933807893\n",
      "train loss:0.011886104454111859\n",
      "train loss:0.021834404539308436\n",
      "train loss:0.01654938221568413\n",
      "train loss:0.08424331473424342\n",
      "train loss:0.020079957875223667\n",
      "train loss:0.006487825505325172\n",
      "train loss:0.06898983329888356\n",
      "train loss:0.029566913263751965\n",
      "train loss:0.03561876431503327\n",
      "train loss:0.010640737476786512\n",
      "train loss:0.02030579160283576\n",
      "train loss:0.02379892346666261\n",
      "train loss:0.0532004451790745\n",
      "train loss:0.08164452837987471\n",
      "train loss:0.040942948617895984\n",
      "train loss:0.08904684085048528\n",
      "train loss:0.025992109109236185\n",
      "train loss:0.023939699689989706\n",
      "train loss:0.03643435368043177\n",
      "train loss:0.045520065586546\n",
      "train loss:0.023486077445789674\n",
      "train loss:0.016586051237229827\n",
      "train loss:0.04399866845228586\n",
      "train loss:0.0674617537680373\n",
      "train loss:0.025564599522715413\n",
      "train loss:0.007889711146610712\n",
      "train loss:0.06013568262729811\n",
      "train loss:0.06912258951465707\n",
      "train loss:0.011845185116812963\n",
      "train loss:0.011944721668951946\n",
      "train loss:0.012970004065842235\n",
      "train loss:0.05249582427180553\n",
      "train loss:0.005444836714233769\n",
      "train loss:0.025992077136646495\n",
      "train loss:0.005813168813313867\n",
      "train loss:0.02652618881747979\n",
      "train loss:0.07024605257160792\n",
      "train loss:0.038967765184504996\n",
      "train loss:0.04327962585087138\n",
      "train loss:0.053917174482046096\n",
      "train loss:0.04108362977412903\n",
      "train loss:0.0169222323391178\n",
      "train loss:0.020369658111604273\n",
      "train loss:0.008813783194682302\n",
      "train loss:0.02008285259297964\n",
      "train loss:0.01360377727745851\n",
      "train loss:0.036180335732101894\n",
      "train loss:0.011593931737115822\n",
      "train loss:0.02789345006538995\n",
      "train loss:0.0063336559387653655\n",
      "train loss:0.034919733508423166\n",
      "train loss:0.025828223818361095\n",
      "train loss:0.005933389178754095\n",
      "train loss:0.019763577004659052\n",
      "train loss:0.07683738378464476\n",
      "train loss:0.0073182495751478605\n",
      "train loss:0.03185467323976162\n",
      "train loss:0.010490706933471552\n",
      "train loss:0.05055729044259328\n",
      "train loss:0.017058821109360862\n",
      "train loss:0.010200405714075059\n",
      "train loss:0.0037922770510400573\n",
      "train loss:0.03343571778128579\n",
      "train loss:0.008651921098966574\n",
      "train loss:0.01865980436032599\n",
      "train loss:0.030272018085494654\n",
      "train loss:0.08498412664654245\n",
      "train loss:0.01875673238278621\n",
      "train loss:0.01475922836254504\n",
      "train loss:0.024818245166541478\n",
      "train loss:0.014102454640287271\n",
      "train loss:0.015620993546371859\n",
      "train loss:0.038859776488220685\n",
      "train loss:0.01577433335352897\n",
      "train loss:0.08563834456422828\n",
      "train loss:0.03477797816208035\n",
      "train loss:0.013800043019462084\n",
      "train loss:0.03709947596832883\n",
      "train loss:0.035512947717056015\n",
      "train loss:0.013056587813284205\n",
      "train loss:0.021715719102599635\n",
      "train loss:0.026254469712389778\n",
      "train loss:0.011389141083798272\n",
      "train loss:0.007733745545712517\n",
      "train loss:0.008113207558117445\n",
      "train loss:0.014981776348832394\n",
      "train loss:0.011219581033920725\n",
      "train loss:0.00695401226257138\n",
      "train loss:0.0288386492412926\n",
      "train loss:0.010366093889225785\n",
      "train loss:0.003427274205908344\n",
      "train loss:0.03653175277736131\n",
      "train loss:0.014361141956783154\n",
      "train loss:0.02649709973370675\n",
      "train loss:0.005781616515766053\n",
      "train loss:0.06257460701231475\n",
      "train loss:0.010558084119997573\n",
      "train loss:0.029774147678239535\n",
      "train loss:0.016796657237834318\n",
      "train loss:0.013572760468951941\n",
      "train loss:0.0355129932920763\n",
      "train loss:0.04897093340905221\n",
      "train loss:0.02577095301488088\n",
      "train loss:0.010024029090715949\n",
      "train loss:0.05697904833472054\n",
      "train loss:0.007727902670190186\n",
      "train loss:0.004189565231403071\n",
      "train loss:0.023125844056884177\n",
      "train loss:0.027351135590035974\n",
      "train loss:0.009511032349404317\n",
      "train loss:0.03666423170504172\n",
      "train loss:0.052567771237814066\n",
      "train loss:0.025964658716413857\n",
      "train loss:0.01454236373179968\n",
      "train loss:0.03503215896689594\n",
      "train loss:0.00731706990360863\n",
      "train loss:0.013324628949588677\n",
      "train loss:0.022236139786214998\n",
      "train loss:0.006551981535492136\n",
      "train loss:0.017819727720516796\n",
      "train loss:0.052766780788715856\n",
      "train loss:0.03836453596437561\n",
      "train loss:0.08501086550160313\n",
      "train loss:0.04455756886417559\n",
      "train loss:0.009714993843063409\n",
      "train loss:0.014199177411292629\n",
      "train loss:0.01539003806522363\n",
      "train loss:0.02799739114067251\n",
      "train loss:0.04656035238728258\n",
      "train loss:0.032003721231628854\n",
      "train loss:0.03532903208844168\n",
      "train loss:0.011709067535373627\n",
      "train loss:0.010846610013819034\n",
      "train loss:0.032252388800387964\n",
      "train loss:0.01203462074789686\n",
      "train loss:0.04933590063441797\n",
      "train loss:0.05643272016095728\n",
      "train loss:0.013638780017491213\n",
      "train loss:0.02405935659053476\n",
      "train loss:0.0076597008911308715\n",
      "train loss:0.008204880649079434\n",
      "train loss:0.01876703764078006\n",
      "train loss:0.015801916732082713\n",
      "train loss:0.009809833491043833\n",
      "train loss:0.004510344777586222\n",
      "train loss:0.009437376906629977\n",
      "train loss:0.07463347031956893\n",
      "train loss:0.04961500147783733\n",
      "train loss:0.026847531246802708\n",
      "train loss:0.02574046298310603\n",
      "train loss:0.07258932143849552\n",
      "train loss:0.01895326578552148\n",
      "train loss:0.009803437258764069\n",
      "train loss:0.004568340494580957\n",
      "train loss:0.002010402079485895\n",
      "train loss:0.011304326712426538\n",
      "train loss:0.04149856434840018\n",
      "train loss:0.059229670499251874\n",
      "train loss:0.024693696645147277\n",
      "train loss:0.129059487494864\n",
      "train loss:0.0471730144018148\n",
      "train loss:0.01272932577093334\n",
      "train loss:0.003779523293746867\n",
      "train loss:0.00701235977640757\n",
      "train loss:0.008147124093298432\n",
      "train loss:0.011350563960185299\n",
      "train loss:0.019475506434064715\n",
      "train loss:0.03618532692951338\n",
      "train loss:0.02615040252845502\n",
      "train loss:0.04847471709506058\n",
      "train loss:0.014065317447140055\n",
      "train loss:0.046485117056660814\n",
      "train loss:0.005791921989621771\n",
      "train loss:0.008504585805684781\n",
      "train loss:0.005795614202626309\n",
      "train loss:0.009588005251992177\n",
      "train loss:0.02198937979151967\n",
      "train loss:0.02662140338139169\n",
      "train loss:0.06006915875628817\n",
      "train loss:0.034140635137590566\n",
      "train loss:0.018346877681935617\n",
      "train loss:0.01557077166243807\n",
      "train loss:0.007881996692477576\n",
      "train loss:0.023354689379871535\n",
      "train loss:0.007594674630985413\n",
      "train loss:0.037976387724202365\n",
      "train loss:0.005701022256205497\n",
      "train loss:0.009858643561451235\n",
      "train loss:0.017291732099976233\n",
      "train loss:0.007466633722928844\n",
      "train loss:0.018595985199251593\n",
      "train loss:0.033282144053021726\n",
      "train loss:0.01349921571625836\n",
      "train loss:0.02917904573601644\n",
      "train loss:0.014010397068471918\n",
      "train loss:0.027640270423308695\n",
      "train loss:0.04480413778008274\n",
      "train loss:0.01137303480878073\n",
      "train loss:0.02062087199120674\n",
      "train loss:0.004426525689417477\n",
      "train loss:0.007270197984169763\n",
      "train loss:0.01338108091951926\n",
      "train loss:0.008758427336804044\n",
      "train loss:0.05922740541273273\n",
      "train loss:0.006451048111862635\n",
      "train loss:0.03336125209715765\n",
      "train loss:0.023298587309334734\n",
      "train loss:0.05342874818014713\n",
      "train loss:0.010382902439334716\n",
      "train loss:0.07789693280056881\n",
      "train loss:0.027222623447836734\n",
      "train loss:0.10611276849153145\n",
      "train loss:0.010221776496039644\n",
      "train loss:0.019459827838885783\n",
      "train loss:0.0055268606664172016\n",
      "train loss:0.037660355223880455\n",
      "train loss:0.015548973116083848\n",
      "train loss:0.020906818190923982\n",
      "train loss:0.03823375694130325\n",
      "train loss:0.014772165305107127\n",
      "train loss:0.0041816602097577\n",
      "train loss:0.009586314275732418\n",
      "train loss:0.015676765032588233\n",
      "train loss:0.014269170707755172\n",
      "train loss:0.012003190288205406\n",
      "train loss:0.013309538243893465\n",
      "train loss:0.01589907154343766\n",
      "train loss:0.011387157381971814\n",
      "train loss:0.019632915984130556\n",
      "train loss:0.00816886880182393\n",
      "train loss:0.014819999522283904\n",
      "train loss:0.033401227637563834\n",
      "train loss:0.10418338846205097\n",
      "train loss:0.007365753126160227\n",
      "train loss:0.01249173432467352\n",
      "train loss:0.035274836557298316\n",
      "train loss:0.03543307902209875\n",
      "train loss:0.007819436223258395\n",
      "train loss:0.019146492700586073\n",
      "train loss:0.024979195656792373\n",
      "train loss:0.0021976596683090676\n",
      "train loss:0.042086678075163156\n",
      "train loss:0.04207561025311853\n",
      "train loss:0.007217960496038659\n",
      "train loss:0.003019069231431621\n",
      "train loss:0.05402023168149971\n",
      "train loss:0.02446735215158925\n",
      "train loss:0.004052309279052088\n",
      "train loss:0.005576292343269661\n",
      "train loss:0.023576999235562727\n",
      "train loss:0.05006342840421994\n",
      "train loss:0.014172188940686893\n",
      "train loss:0.010505585687631491\n",
      "train loss:0.06292215765610865\n",
      "train loss:0.01492554125794309\n",
      "train loss:0.010107781055608887\n",
      "train loss:0.039647136987589475\n",
      "train loss:0.08170270641904079\n",
      "train loss:0.02239409358052168\n",
      "train loss:0.00697410939996674\n",
      "train loss:0.03395492294584415\n",
      "train loss:0.008699777010900843\n",
      "train loss:0.023959243916128675\n",
      "train loss:0.016513727011751043\n",
      "train loss:0.009707387914659242\n",
      "train loss:0.005945908871097689\n",
      "train loss:0.008763147230560733\n",
      "train loss:0.0062166792752978775\n",
      "train loss:0.036638369135507894\n",
      "train loss:0.06983647692950258\n",
      "train loss:0.009362981076611418\n",
      "train loss:0.014537377202851427\n",
      "train loss:0.006658656621732594\n",
      "train loss:0.020135312585987605\n",
      "train loss:0.0160534843658791\n",
      "train loss:0.009307232498643027\n",
      "train loss:0.005659559984367255\n",
      "train loss:0.0603585233608645\n",
      "train loss:0.012643167163273829\n",
      "train loss:0.02560508899220064\n",
      "train loss:0.007440913416759061\n",
      "train loss:0.01975388744572302\n",
      "train loss:0.034498450423095385\n",
      "train loss:0.05329912850967323\n",
      "train loss:0.021985754676294943\n",
      "train loss:0.0027283338482444824\n",
      "train loss:0.010492300933153507\n",
      "train loss:0.004958045828910953\n",
      "train loss:0.01547090012775201\n",
      "train loss:0.005142511225302193\n",
      "train loss:0.007672942828231721\n",
      "train loss:0.026494140404370127\n",
      "train loss:0.059529611345640984\n",
      "train loss:0.019248512923837217\n",
      "train loss:0.051679328582041766\n",
      "train loss:0.03460702009713698\n",
      "train loss:0.06561880540876824\n",
      "train loss:0.0186791410646501\n",
      "train loss:0.00538241790134391\n",
      "train loss:0.019503330451714106\n",
      "train loss:0.025050735601555844\n",
      "train loss:0.005514622745207655\n",
      "train loss:0.02915333549695948\n",
      "train loss:0.04027643378969656\n",
      "train loss:0.013818766375809673\n",
      "train loss:0.03241260954906644\n",
      "train loss:0.013559885037000709\n",
      "train loss:0.01587004615197346\n",
      "train loss:0.0272012540096308\n",
      "train loss:0.028820858521020946\n",
      "train loss:0.0238248242354166\n",
      "train loss:0.018385953367642163\n",
      "train loss:0.016643267029075887\n",
      "train loss:0.0053779658528520236\n",
      "train loss:0.007507354371603477\n",
      "train loss:0.05808500673074893\n",
      "train loss:0.026947925061866367\n",
      "train loss:0.024678790874439985\n",
      "train loss:0.018547374666506314\n",
      "train loss:0.0036635211959915063\n",
      "train loss:0.004997630015935545\n",
      "train loss:0.03292787928329098\n",
      "train loss:0.01694643040550228\n",
      "train loss:0.032493592443847724\n",
      "train loss:0.011288134809534238\n",
      "train loss:0.015966790693543908\n",
      "train loss:0.0018964497134040595\n",
      "train loss:0.00916570849773868\n",
      "train loss:0.016803582977729613\n",
      "train loss:0.026627751806386424\n",
      "train loss:0.013155552261195764\n",
      "train loss:0.029556665945940357\n",
      "train loss:0.017941706862447202\n",
      "train loss:0.035067294851212745\n",
      "train loss:0.013540354655216908\n",
      "train loss:0.0489479512299458\n",
      "train loss:0.0176354891921177\n",
      "train loss:0.022691017331906798\n",
      "train loss:0.03830774586308817\n",
      "train loss:0.005488304766510389\n",
      "train loss:0.007860333536957604\n",
      "train loss:0.01428456794635338\n",
      "train loss:0.08275116532753214\n",
      "train loss:0.04656446794062953\n",
      "train loss:0.0031110119476100463\n",
      "train loss:0.03808367506171345\n",
      "train loss:0.07577112757584734\n",
      "train loss:0.03540413680258774\n",
      "train loss:0.014403919489128987\n",
      "train loss:0.019773601321552314\n",
      "train loss:0.04324295226597001\n",
      "train loss:0.05903657774674887\n",
      "train loss:0.025990237741859015\n",
      "train loss:0.00776545324945476\n",
      "train loss:0.007486437306144242\n",
      "train loss:0.01204426599943291\n",
      "train loss:0.011074663387375661\n",
      "train loss:0.007898679238673462\n",
      "train loss:0.016423974994723513\n",
      "train loss:0.023207917431088097\n",
      "train loss:0.007060819145500655\n",
      "train loss:0.022906393557480528\n",
      "train loss:0.02781532648958193\n",
      "train loss:0.004808505248766639\n",
      "train loss:0.022049482707091558\n",
      "train loss:0.01013433893956302\n",
      "train loss:0.005536007474070586\n",
      "train loss:0.014682011792606397\n",
      "train loss:0.022138356990819008\n",
      "train loss:0.010062590730583763\n",
      "train loss:0.018485685434410384\n",
      "train loss:0.015258556498894467\n",
      "train loss:0.0593623550797532\n",
      "train loss:0.010810762972513521\n",
      "train loss:0.02685435535911875\n",
      "train loss:0.062003846981551286\n",
      "train loss:0.011773753580651601\n",
      "train loss:0.01423479158254544\n",
      "train loss:0.018262959533139136\n",
      "train loss:0.01784661317369961\n",
      "train loss:0.011570121316392228\n",
      "train loss:0.03583301687192027\n",
      "train loss:0.017422406161489414\n",
      "train loss:0.004115689188833699\n",
      "train loss:0.014245031430500585\n",
      "train loss:0.00283982130386762\n",
      "train loss:0.004926574056056154\n",
      "train loss:0.03655388873825124\n",
      "train loss:0.005356596989799015\n",
      "train loss:0.024742270428159147\n",
      "train loss:0.007160214830290532\n",
      "train loss:0.02220941034726839\n",
      "train loss:0.008149381347038683\n",
      "train loss:0.008161559403658206\n",
      "train loss:0.019542695476109394\n",
      "train loss:0.017726708963185615\n",
      "train loss:0.022878234754516828\n",
      "train loss:0.00264336894536653\n",
      "train loss:0.01965849607789592\n",
      "train loss:0.013793795996616478\n",
      "train loss:0.011842300675565475\n",
      "train loss:0.018987441632523246\n",
      "train loss:0.010467912038728346\n",
      "train loss:0.034157336672322416\n",
      "train loss:0.009364226023389089\n",
      "train loss:0.013602981791684558\n",
      "train loss:0.08004396833664851\n",
      "train loss:0.00439778305104034\n",
      "train loss:0.01549568406948891\n",
      "train loss:0.01521665595821673\n",
      "train loss:0.028263849495107317\n",
      "train loss:0.025541065031077833\n",
      "train loss:0.022932707326581894\n",
      "train loss:0.027829303238638893\n",
      "train loss:0.04703064082291765\n",
      "train loss:0.023923615759991416\n",
      "train loss:0.035764672626738395\n",
      "train loss:0.027886754687543886\n",
      "train loss:0.024380155049956453\n",
      "train loss:0.01496159062869094\n",
      "train loss:0.037953872656597616\n",
      "train loss:0.07396717159535265\n",
      "train loss:0.010717507839306748\n",
      "train loss:0.013756175959421161\n",
      "train loss:0.018605696872279285\n",
      "train loss:0.03996239419271039\n",
      "train loss:0.036757601361781994\n",
      "train loss:0.010517108155888542\n",
      "train loss:0.017997590369478712\n",
      "train loss:0.0058289906793134345\n",
      "train loss:0.0025946503632419894\n",
      "train loss:0.02657610476242751\n",
      "train loss:0.00788466064042297\n",
      "train loss:0.015360418574883938\n",
      "train loss:0.029828451031927026\n",
      "train loss:0.022253947975934284\n",
      "train loss:0.016514651723889058\n",
      "train loss:0.05127515221428511\n",
      "train loss:0.019854124356079344\n",
      "train loss:0.0201169821158857\n",
      "train loss:0.012218961718950367\n",
      "train loss:0.017498792808407385\n",
      "train loss:0.018435618671640175\n",
      "train loss:0.004800631312334266\n",
      "train loss:0.010439209000814041\n",
      "train loss:0.0031086991950374527\n",
      "train loss:0.02701831355021843\n",
      "train loss:0.030054353683431186\n",
      "train loss:0.010527526412185724\n",
      "train loss:0.04463360245603882\n",
      "train loss:0.019399822903915967\n",
      "train loss:0.04984466770856171\n",
      "train loss:0.006622329741650259\n",
      "train loss:0.02529284837985655\n",
      "train loss:0.020227282053072705\n",
      "train loss:0.0047897299526189815\n",
      "train loss:0.04478962218363534\n",
      "train loss:0.014670960831341152\n",
      "train loss:0.0669038692919806\n",
      "train loss:0.017622300671410296\n",
      "train loss:0.002522893681720028\n",
      "train loss:0.026226030306720362\n",
      "train loss:0.010460899414211085\n",
      "train loss:0.045761572357914865\n",
      "train loss:0.009500001076201126\n",
      "train loss:0.01153652427680943\n",
      "train loss:0.02417991677041542\n",
      "train loss:0.007826393004522047\n",
      "train loss:0.02915564819083516\n",
      "train loss:0.013080229797413006\n",
      "train loss:0.0074440582152492385\n",
      "train loss:0.0058921978649061554\n",
      "train loss:0.04941508778591434\n",
      "train loss:0.01019290012646952\n",
      "train loss:0.01691680766462288\n",
      "train loss:0.027031485047355717\n",
      "train loss:0.020392218604913886\n",
      "train loss:0.006654003751619361\n",
      "train loss:0.0035689818566651916\n",
      "train loss:0.005455375617708453\n",
      "train loss:0.015753417038620657\n",
      "train loss:0.006703920626486603\n",
      "train loss:0.010300110977617384\n",
      "train loss:0.005076568624162333\n",
      "train loss:0.0452411080029676\n",
      "train loss:0.05551483937195121\n",
      "train loss:0.007105927895093489\n",
      "train loss:0.01621178740460888\n",
      "train loss:0.012398012582837433\n",
      "train loss:0.0069397703857847085\n",
      "train loss:0.02000940425819902\n",
      "train loss:0.010515210501524841\n",
      "train loss:0.01696572522885972\n",
      "train loss:0.018143075120319704\n",
      "train loss:0.011559066021863414\n",
      "train loss:0.03043459482789274\n",
      "train loss:0.029546051390284895\n",
      "train loss:0.024073260972890077\n",
      "train loss:0.009619819070528321\n",
      "train loss:0.00984944907784428\n",
      "train loss:0.024887986056882506\n",
      "train loss:0.031540835160555794\n",
      "train loss:0.005759822977504215\n",
      "train loss:0.01334052158311935\n",
      "train loss:0.04801792465783586\n",
      "train loss:0.007246791832258196\n",
      "train loss:0.021001392473409073\n",
      "train loss:0.00455294267363315\n",
      "train loss:0.014273810756667228\n",
      "train loss:0.0020324243293023585\n",
      "train loss:0.014776658364839339\n",
      "train loss:0.013110781674401297\n",
      "train loss:0.004908086855415537\n",
      "train loss:0.00793025988604689\n",
      "train loss:0.0035530247899428435\n",
      "train loss:0.04346704479302763\n",
      "train loss:0.024387193883131566\n",
      "train loss:0.024354007617153846\n",
      "train loss:0.015731605449511785\n",
      "train loss:0.006775424976940948\n",
      "train loss:0.011554236207288949\n",
      "train loss:0.029578917214328256\n",
      "train loss:0.0048959980902160615\n",
      "train loss:0.04150970166742774\n",
      "train loss:0.01452302325113025\n",
      "train loss:0.025708760530718168\n",
      "train loss:0.04673883188506128\n",
      "train loss:0.00658444315241872\n",
      "train loss:0.026155635148879277\n",
      "train loss:0.0009350412559820363\n",
      "train loss:0.015806146806350355\n",
      "train loss:0.0010363353081372035\n",
      "train loss:0.0043676830339741905\n",
      "train loss:0.01215844019050773\n",
      "train loss:0.042916176103042505\n",
      "train loss:0.004469877114225445\n",
      "train loss:0.009010225324672147\n",
      "train loss:0.009993481539872175\n",
      "train loss:0.009290724388821775\n",
      "train loss:0.022783899620772048\n",
      "train loss:0.016063304373958226\n",
      "train loss:0.014959307009070466\n",
      "train loss:0.007710083283605007\n",
      "train loss:0.03584905244959427\n",
      "train loss:0.004894196052353458\n",
      "train loss:0.022101930619391585\n",
      "train loss:0.022227989662901654\n",
      "train loss:0.012717352420249559\n",
      "train loss:0.009488845350005042\n",
      "train loss:0.010569735054245624\n",
      "train loss:0.004976995900714388\n",
      "train loss:0.007508861184876271\n",
      "train loss:0.011120839868526156\n",
      "train loss:0.012886731270834634\n",
      "train loss:0.017369994227470254\n",
      "train loss:0.0038882786906353534\n",
      "train loss:0.023657229622290478\n",
      "train loss:0.006881714663482268\n",
      "train loss:0.06589299481152791\n",
      "train loss:0.00919697293226011\n",
      "train loss:0.021188468180081722\n",
      "=== epoch:7, train acc:0.992, test acc:0.989 ===\n",
      "train loss:0.09850578217111339\n",
      "train loss:0.013944982117160509\n",
      "train loss:0.018235938113807058\n",
      "train loss:0.019844939523252995\n",
      "train loss:0.01611139991630074\n",
      "train loss:0.0017757766175477217\n",
      "train loss:0.030703928835837387\n",
      "train loss:0.006855879887505926\n",
      "train loss:0.003620543018455214\n",
      "train loss:0.008538901715614897\n",
      "train loss:0.008334111079309404\n",
      "train loss:0.03578522523206427\n",
      "train loss:0.018576915204017764\n",
      "train loss:0.008773351377089764\n",
      "train loss:0.00303313354461463\n",
      "train loss:0.044937607208671876\n",
      "train loss:0.009677759446097736\n",
      "train loss:0.005947013748889083\n",
      "train loss:0.008517492053158316\n",
      "train loss:0.010666392194652749\n",
      "train loss:0.024297634786603507\n",
      "train loss:0.006691410999525107\n",
      "train loss:0.007719383138647062\n",
      "train loss:0.01269599015684621\n",
      "train loss:0.016183817891752283\n",
      "train loss:0.015407754522117725\n",
      "train loss:0.05870756068284952\n",
      "train loss:0.030028551574674232\n",
      "train loss:0.007371189640625255\n",
      "train loss:0.00866200696672521\n",
      "train loss:0.0027214163514107438\n",
      "train loss:0.01200675653841472\n",
      "train loss:0.005877380051301879\n",
      "train loss:0.010781093778732057\n",
      "train loss:0.029545317516992853\n",
      "train loss:0.02148490884105438\n",
      "train loss:0.0093062408149064\n",
      "train loss:0.010286812787320323\n",
      "train loss:0.009240092159524678\n",
      "train loss:0.0674607384769001\n",
      "train loss:0.035436963976389776\n",
      "train loss:0.001929973777501428\n",
      "train loss:0.0993951850443777\n",
      "train loss:0.00485579921165841\n",
      "train loss:0.0038045477138518246\n",
      "train loss:0.013589109264233498\n",
      "train loss:0.008304789421867539\n",
      "train loss:0.03037876403789677\n",
      "train loss:0.010178795432458622\n",
      "train loss:0.015730178316408026\n",
      "train loss:0.009952311956226754\n",
      "train loss:0.035416437183738644\n",
      "train loss:0.018108350394022997\n",
      "train loss:0.06708468420002106\n",
      "train loss:0.015200868761560076\n",
      "train loss:0.031004475954806602\n",
      "train loss:0.014409062785950538\n",
      "train loss:0.04106832917216642\n",
      "train loss:0.016372391852716175\n",
      "train loss:0.04341436656683708\n",
      "train loss:0.016809694322770297\n",
      "train loss:0.01184079533071059\n",
      "train loss:0.009933493528320977\n",
      "train loss:0.035106583579707196\n",
      "train loss:0.00689319046899517\n",
      "train loss:0.023842395566251225\n",
      "train loss:0.010891034679517315\n",
      "train loss:0.006130232326986879\n",
      "train loss:0.015261975791447047\n",
      "train loss:0.006723283025481606\n",
      "train loss:0.03716615330053672\n",
      "train loss:0.017715168770847542\n",
      "train loss:0.005023633873141118\n",
      "train loss:0.009642856770187313\n",
      "train loss:0.015386576251108244\n",
      "train loss:0.004891917973335431\n",
      "train loss:0.01667489501596582\n",
      "train loss:0.02413002478846155\n",
      "train loss:0.021059104274836704\n",
      "train loss:0.013932206933170196\n",
      "train loss:0.005934248319761856\n",
      "train loss:0.06543007174676257\n",
      "train loss:0.024930031707784215\n",
      "train loss:0.009443375766227858\n",
      "train loss:0.006382233825861854\n",
      "train loss:0.034052574266868446\n",
      "train loss:0.002512943422886922\n",
      "train loss:0.024682280473223354\n",
      "train loss:0.004175069052253396\n",
      "train loss:0.009079127995861446\n",
      "train loss:0.018537265459156432\n",
      "train loss:0.011166832208843952\n",
      "train loss:0.011642448088306829\n",
      "train loss:0.015428539234393796\n",
      "train loss:0.008711872644842009\n",
      "train loss:0.017889259506753862\n",
      "train loss:0.01098481112605388\n",
      "train loss:0.012478746193573398\n",
      "train loss:0.008626063794032482\n",
      "train loss:0.01032660002758646\n",
      "train loss:0.11495474680547259\n",
      "train loss:0.10647237243006799\n",
      "train loss:0.02262017162911226\n",
      "train loss:0.030740850808328404\n",
      "train loss:0.010023945336919223\n",
      "train loss:0.025656315709004743\n",
      "train loss:0.004240645002272084\n",
      "train loss:0.006972536400815671\n",
      "train loss:0.10564248377797361\n",
      "train loss:0.0011277485826230685\n",
      "train loss:0.003885033811920922\n",
      "train loss:0.023826250706435473\n",
      "train loss:0.034342059535450255\n",
      "train loss:0.06505474890211974\n",
      "train loss:0.012842143719077135\n",
      "train loss:0.013913449702307495\n",
      "train loss:0.036773782389683166\n",
      "train loss:0.018555357054309555\n",
      "train loss:0.026094441899116996\n",
      "train loss:0.002986129985472115\n",
      "train loss:0.019404428311221526\n",
      "train loss:0.006899894999941504\n",
      "train loss:0.0074910639672872755\n",
      "train loss:0.04400093587914017\n",
      "train loss:0.010101270027693397\n",
      "train loss:0.018178447266170978\n",
      "train loss:0.004921610728389014\n",
      "train loss:0.08260753595493453\n",
      "train loss:0.04285368433910748\n",
      "train loss:0.021231664645819194\n",
      "train loss:0.013235320633599971\n",
      "train loss:0.005637000820549084\n",
      "train loss:0.025570893646315086\n",
      "train loss:0.02629654737697039\n",
      "train loss:0.011546978368204598\n",
      "train loss:0.03478336009983646\n",
      "train loss:0.014025508021672028\n",
      "train loss:0.03173781697733468\n",
      "train loss:0.005559970706228823\n",
      "train loss:0.015629251027877992\n",
      "train loss:0.010367828561905936\n",
      "train loss:0.005399187455827393\n",
      "train loss:0.006670525735589509\n",
      "train loss:0.023457731717161923\n",
      "train loss:0.0031890038769713815\n",
      "train loss:0.01014883751990129\n",
      "train loss:0.001085620862466903\n",
      "train loss:0.03630635779526986\n",
      "train loss:0.0019698846904388\n",
      "train loss:0.009021432697640648\n",
      "train loss:0.010612840572745077\n",
      "train loss:0.007039251442085663\n",
      "train loss:0.0042247434886915815\n",
      "train loss:0.01617936273078856\n",
      "train loss:0.00408622878865649\n",
      "train loss:0.013307874435110587\n",
      "train loss:0.03790797183679146\n",
      "train loss:0.007506320129063385\n",
      "train loss:0.002647259873646808\n",
      "train loss:0.004626642958256063\n",
      "train loss:0.03775194804510015\n",
      "train loss:0.013965990795159195\n",
      "train loss:0.005570766970171492\n",
      "train loss:0.023637328508021405\n",
      "train loss:0.02042899670936588\n",
      "train loss:0.008361445972909137\n",
      "train loss:0.018334437986945958\n",
      "train loss:0.050833925238705134\n",
      "train loss:0.05211089739982724\n",
      "train loss:0.03649443072669309\n",
      "train loss:0.012496485325585348\n",
      "train loss:0.004671163558363966\n",
      "train loss:0.002236206049483807\n",
      "train loss:0.006329114840417541\n",
      "train loss:0.008780651690989514\n",
      "train loss:0.008937022468768884\n",
      "train loss:0.003995073035012383\n",
      "train loss:0.012815974788053288\n",
      "train loss:0.00772870602922466\n",
      "train loss:0.014189262242584296\n",
      "train loss:0.005706031383474109\n",
      "train loss:0.007121225341665\n",
      "train loss:0.0016891435549770584\n",
      "train loss:0.013303611613460302\n",
      "train loss:0.0025497175202560084\n",
      "train loss:0.07218759160825622\n",
      "train loss:0.025231062492315996\n",
      "train loss:0.005858075569028826\n",
      "train loss:0.002851132986506738\n",
      "train loss:0.015379543978224252\n",
      "train loss:0.00453647443726565\n",
      "train loss:0.02579379296683331\n",
      "train loss:0.007536252104976096\n",
      "train loss:0.01337746704061208\n",
      "train loss:0.006780048317907132\n",
      "train loss:0.0074852886249009145\n",
      "train loss:0.01482097851929111\n",
      "train loss:0.007728624198555265\n",
      "train loss:0.0405059121851951\n",
      "train loss:0.006647678902728613\n",
      "train loss:0.04520069264144021\n",
      "train loss:0.003091635335173553\n",
      "train loss:0.0063634980663660815\n",
      "train loss:0.003950432441144895\n",
      "train loss:0.02546614190696958\n",
      "train loss:0.0024471094370002336\n",
      "train loss:0.0046937121594728855\n",
      "train loss:0.07811607907658508\n",
      "train loss:0.00498140213098348\n",
      "train loss:0.039572354626082766\n",
      "train loss:0.008111682883690841\n",
      "train loss:0.0024802733510956698\n",
      "train loss:0.06824867674108642\n",
      "train loss:0.02270935266889078\n",
      "train loss:0.03096825667336856\n",
      "train loss:0.06849447265612438\n",
      "train loss:0.022036007991435395\n",
      "train loss:0.01937391648149274\n",
      "train loss:0.06301745700892135\n",
      "train loss:0.024762656259049685\n",
      "train loss:0.03721833635717932\n",
      "train loss:0.011329779785855476\n",
      "train loss:0.011441157191014305\n",
      "train loss:0.004313897887047615\n",
      "train loss:0.005377655850522577\n",
      "train loss:0.07319516119676281\n",
      "train loss:0.06691664459228945\n",
      "train loss:0.014395095724177617\n",
      "train loss:0.009101324130639424\n",
      "train loss:0.05089856843477405\n",
      "train loss:0.0032920423154875027\n",
      "train loss:0.012777748576980276\n",
      "train loss:0.010074646522230706\n",
      "train loss:0.025475879364075707\n",
      "train loss:0.008250410794203836\n",
      "train loss:0.0032809006005432495\n",
      "train loss:0.07574457904115735\n",
      "train loss:0.021191085678158507\n",
      "train loss:0.018188920843307345\n",
      "train loss:0.005362740594698673\n",
      "train loss:0.00940748233602112\n",
      "train loss:0.005772708358064122\n",
      "train loss:0.01458069230017217\n",
      "train loss:0.008508759924900753\n",
      "train loss:0.007543435367173263\n",
      "train loss:0.04045647836664232\n",
      "train loss:0.01364516094757677\n",
      "train loss:0.03498242363368121\n",
      "train loss:0.007206187781430918\n",
      "train loss:0.009869812591996631\n",
      "train loss:0.033501387404699795\n",
      "train loss:0.024839041667716034\n",
      "train loss:0.01076433592001416\n",
      "train loss:0.02323589816188897\n",
      "train loss:0.03933355398499871\n",
      "train loss:0.004077396125543049\n",
      "train loss:0.056854788003982776\n",
      "train loss:0.16644917508865759\n",
      "train loss:0.009981960200675884\n",
      "train loss:0.03258465731849654\n",
      "train loss:0.008998437052531547\n",
      "train loss:0.050718276948932516\n",
      "train loss:0.007058471168984948\n",
      "train loss:0.005938752199467275\n",
      "train loss:0.009755197192894091\n",
      "train loss:0.005988470863970671\n",
      "train loss:0.01258872949251213\n",
      "train loss:0.014181518620758761\n",
      "train loss:0.042755889469610675\n",
      "train loss:0.017448788787711073\n",
      "train loss:0.020158969061935057\n",
      "train loss:0.02430430448768203\n",
      "train loss:0.020021202596419888\n",
      "train loss:0.007464187492470559\n",
      "train loss:0.05667625440347326\n",
      "train loss:0.005818371116550799\n",
      "train loss:0.017141400324320398\n",
      "train loss:0.036363947319179635\n",
      "train loss:0.012695003209316864\n",
      "train loss:0.0306706593322292\n",
      "train loss:0.010659637699776648\n",
      "train loss:0.038211729045572296\n",
      "train loss:0.007512224564765597\n",
      "train loss:0.009855145160226522\n",
      "train loss:0.005118252104883289\n",
      "train loss:0.007138330211926206\n",
      "train loss:0.011669101068818036\n",
      "train loss:0.012217419584487719\n",
      "train loss:0.01983235945042633\n",
      "train loss:0.021017437634182366\n",
      "train loss:0.04906475424573229\n",
      "train loss:0.005365650754971287\n",
      "train loss:0.005825106917409993\n",
      "train loss:0.01849718570234519\n",
      "train loss:0.005081021925477617\n",
      "train loss:0.003138324629463122\n",
      "train loss:0.008447456640645166\n",
      "train loss:0.004414828912591921\n",
      "train loss:0.025363317058959386\n",
      "train loss:0.0072100653263404875\n",
      "train loss:0.0036916958921384886\n",
      "train loss:0.010018581606618839\n",
      "train loss:0.041613554485553\n",
      "train loss:0.003815478052688351\n",
      "train loss:0.0074336545949773005\n",
      "train loss:0.0301368665753723\n",
      "train loss:0.013211613167859968\n",
      "train loss:0.015893026912340916\n",
      "train loss:0.03253095246674627\n",
      "train loss:0.0022132060706649935\n",
      "train loss:0.02585202065617615\n",
      "train loss:0.04870637674138648\n",
      "train loss:0.02100904842721043\n",
      "train loss:0.032472335389771685\n",
      "train loss:0.04633137342042185\n",
      "train loss:0.01954823949318682\n",
      "train loss:0.023410623671410762\n",
      "train loss:0.01629097367220617\n",
      "train loss:0.014332473883063739\n",
      "train loss:0.024027441221654903\n",
      "train loss:0.039944215558825\n",
      "train loss:0.011137599494584727\n",
      "train loss:0.036251956578001295\n",
      "train loss:0.005649030951759079\n",
      "train loss:0.006405372322415686\n",
      "train loss:0.06437174605066397\n",
      "train loss:0.017858338071222994\n",
      "train loss:0.029870169927460107\n",
      "train loss:0.020278915735597928\n",
      "train loss:0.010664606821407139\n",
      "train loss:0.02624101230969092\n",
      "train loss:0.01101492822590518\n",
      "train loss:0.030325499440567682\n",
      "train loss:0.004021140017499452\n",
      "train loss:0.004905550933452884\n",
      "train loss:0.013157508014485501\n",
      "train loss:0.004225507972682979\n",
      "train loss:0.008977905208351022\n",
      "train loss:0.011706815486330674\n",
      "train loss:0.03926468797267422\n",
      "train loss:0.06237646238790598\n",
      "train loss:0.024809250362085113\n",
      "train loss:0.011532048527224085\n",
      "train loss:0.004588555090903442\n",
      "train loss:0.02376492222108946\n",
      "train loss:0.014840675191466526\n",
      "train loss:0.0016382947103191817\n",
      "train loss:0.013257878521424037\n",
      "train loss:0.009134451896829368\n",
      "train loss:0.01732759923540447\n",
      "train loss:0.03076825840526742\n",
      "train loss:0.04263967930524746\n",
      "train loss:0.005049904615662028\n",
      "train loss:0.019680424178577457\n",
      "train loss:0.02371447414177964\n",
      "train loss:0.04906673124781036\n",
      "train loss:0.13326539778124377\n",
      "train loss:0.0031991243095529404\n",
      "train loss:0.011489894226525457\n",
      "train loss:0.0077123078240113\n",
      "train loss:0.04623811836525554\n",
      "train loss:0.009046939490783895\n",
      "train loss:0.03089374236138216\n",
      "train loss:0.0067519986576516635\n",
      "train loss:0.049474326819996986\n",
      "train loss:0.02650341705268526\n",
      "train loss:0.023176394084545183\n",
      "train loss:0.010636043053274279\n",
      "train loss:0.002919221993548943\n",
      "train loss:0.04090200334592946\n",
      "train loss:0.0067935213854544915\n",
      "train loss:0.012350802334012166\n",
      "train loss:0.005930928514703085\n",
      "train loss:0.007263925340231394\n",
      "train loss:0.01145422702727346\n",
      "train loss:0.029786131203855294\n",
      "train loss:0.045921881625471575\n",
      "train loss:0.004265424563624487\n",
      "train loss:0.004255611428904971\n",
      "train loss:0.01888293208594819\n",
      "train loss:0.119784198615616\n",
      "train loss:0.02589483561727214\n",
      "train loss:0.013036669463864104\n",
      "train loss:0.014513787157773728\n",
      "train loss:0.02412385359807967\n",
      "train loss:0.00909308273728607\n",
      "train loss:0.04373589329185652\n",
      "train loss:0.005544217092576468\n",
      "train loss:0.005289705648933925\n",
      "train loss:0.03541207525766005\n",
      "train loss:0.0073323441359416375\n",
      "train loss:0.07066599980338094\n",
      "train loss:0.035935154850095345\n",
      "train loss:0.01523790953396243\n",
      "train loss:0.009452608777032016\n",
      "train loss:0.014067425890165543\n",
      "train loss:0.011321746327404321\n",
      "train loss:0.0060981221071208834\n",
      "train loss:0.008482028874444148\n",
      "train loss:0.010578812988093657\n",
      "train loss:0.0336611201276812\n",
      "train loss:0.010959323338511746\n",
      "train loss:0.0073234987989777645\n",
      "train loss:0.04580960471168757\n",
      "train loss:0.009752292078465438\n",
      "train loss:0.026376281263283296\n",
      "train loss:0.009371535798114707\n",
      "train loss:0.005173807882677505\n",
      "train loss:0.05334432130106319\n",
      "train loss:0.011303457623268232\n",
      "train loss:0.006039189282176564\n",
      "train loss:0.01146780443888565\n",
      "train loss:0.002419512089267658\n",
      "train loss:0.014381792632472181\n",
      "train loss:0.005152635321325163\n",
      "train loss:0.010022376097437176\n",
      "train loss:0.005834878648313312\n",
      "train loss:0.03391725994172301\n",
      "train loss:0.03058759410375453\n",
      "train loss:0.035808394783110715\n",
      "train loss:0.004045614783056211\n",
      "train loss:0.018191288912363428\n",
      "train loss:0.003560252753271929\n",
      "train loss:0.010173278931947933\n",
      "train loss:0.00624088804933134\n",
      "train loss:0.002867952819768193\n",
      "train loss:0.011141808239544344\n",
      "train loss:0.04415697257017629\n",
      "train loss:0.004834404148388412\n",
      "train loss:0.002184875039726891\n",
      "train loss:0.016483174741439496\n",
      "train loss:0.012090950765648196\n",
      "train loss:0.008047084321668747\n",
      "train loss:0.009498180949512906\n",
      "train loss:0.012816173239944561\n",
      "train loss:0.005666939233386642\n",
      "train loss:0.02163624311006325\n",
      "train loss:0.014247056385324136\n",
      "train loss:0.05985993838334285\n",
      "train loss:0.01187437736786201\n",
      "train loss:0.006178780191544366\n",
      "train loss:0.007347287879319953\n",
      "train loss:0.006825665928037482\n",
      "train loss:0.0062361626748934\n",
      "train loss:0.005636083804732353\n",
      "train loss:0.020909029587348315\n",
      "train loss:0.008794563101905954\n",
      "train loss:0.023789370080169913\n",
      "train loss:0.037010112939250946\n",
      "train loss:0.015460688671321485\n",
      "train loss:0.0402922585729538\n",
      "train loss:0.0065788122094001065\n",
      "train loss:0.014593359317328804\n",
      "train loss:0.004449538146871235\n",
      "train loss:0.009252859688486506\n",
      "train loss:0.004420042477914058\n",
      "train loss:0.003710538915334982\n",
      "train loss:0.0026990031947058605\n",
      "train loss:0.05185845154357314\n",
      "train loss:0.020890966592917396\n",
      "train loss:0.027450149729695495\n",
      "train loss:0.012179169995866466\n",
      "train loss:0.0014978420249016525\n",
      "train loss:0.013428125021768487\n",
      "train loss:0.006789666400516181\n",
      "train loss:0.02629305751466772\n",
      "train loss:0.007314277327996912\n",
      "train loss:0.01288910892504087\n",
      "train loss:0.007826120755617923\n",
      "train loss:0.006982031132877562\n",
      "train loss:0.0026700378399696777\n",
      "train loss:0.01497847211383289\n",
      "train loss:0.005044967382618683\n",
      "train loss:0.007572236242300393\n",
      "train loss:0.007594258485989216\n",
      "train loss:0.014202870040418649\n",
      "train loss:0.0072188970493723345\n",
      "train loss:0.0016557738038910194\n",
      "train loss:0.037359371342121785\n",
      "train loss:0.010542736530780718\n",
      "train loss:0.006591304565867492\n",
      "train loss:0.001179512361734397\n",
      "train loss:0.020259740203531443\n",
      "train loss:0.019523683524360447\n",
      "train loss:0.009500781134449287\n",
      "train loss:0.0035740764554380235\n",
      "train loss:0.008921147870432796\n",
      "train loss:0.016896903303999306\n",
      "train loss:0.0012562475014740073\n",
      "train loss:0.007429762761206685\n",
      "train loss:0.02107814400089645\n",
      "train loss:0.0011357404221850928\n",
      "train loss:0.029262712749987024\n",
      "train loss:0.02689789273547637\n",
      "train loss:0.011593544094587328\n",
      "train loss:0.007515722196330344\n",
      "train loss:0.008016775757411663\n",
      "train loss:0.0034303251818981876\n",
      "train loss:0.006321595369286897\n",
      "train loss:0.00605280309727475\n",
      "train loss:0.0019966341989843756\n",
      "train loss:0.023996604851360664\n",
      "train loss:0.020568327905336733\n",
      "train loss:0.002360225630000065\n",
      "train loss:0.022564643196592925\n",
      "train loss:0.008330457307272181\n",
      "train loss:0.01663102294069238\n",
      "train loss:0.018828416404237328\n",
      "train loss:0.007945169686724811\n",
      "train loss:0.0052038284452786045\n",
      "train loss:0.04219384528524675\n",
      "train loss:0.023615204290477906\n",
      "train loss:0.0021076198206809475\n",
      "train loss:0.002606379525057365\n",
      "train loss:0.008575431412130047\n",
      "train loss:0.020880256040863097\n",
      "train loss:0.009626907053097999\n",
      "train loss:0.008095768128341846\n",
      "train loss:0.014639550975858015\n",
      "train loss:0.009107797079532371\n",
      "train loss:0.008915303811660013\n",
      "train loss:0.025765885323006103\n",
      "train loss:0.011387280423602701\n",
      "train loss:0.001566746679708732\n",
      "train loss:0.01659374608245753\n",
      "train loss:0.001325594204611583\n",
      "train loss:0.0038289924029113413\n",
      "train loss:0.006102015841893305\n",
      "train loss:0.06558717987405104\n",
      "train loss:0.013699701570476428\n",
      "train loss:0.00939215479705742\n",
      "train loss:0.007377506402311698\n",
      "train loss:0.007226410995651964\n",
      "train loss:0.031241825062092565\n",
      "train loss:0.024853353831948973\n",
      "train loss:0.0029358667249326257\n",
      "train loss:0.0018334386214628445\n",
      "train loss:0.012023132958045088\n",
      "train loss:0.037970945110024905\n",
      "train loss:0.011505703277284294\n",
      "train loss:0.028596855293575887\n",
      "train loss:0.006280451062439735\n",
      "train loss:0.0186228602078761\n",
      "train loss:0.005537067002933544\n",
      "train loss:0.010277352224145928\n",
      "train loss:0.012505919471693763\n",
      "train loss:0.005421307226344036\n",
      "train loss:0.010545667418849942\n",
      "train loss:0.035778963008366464\n",
      "train loss:0.008691433045175707\n",
      "train loss:0.005529210907360655\n",
      "train loss:0.025172978905511813\n",
      "train loss:0.005848150170996528\n",
      "train loss:0.010404841605144044\n",
      "train loss:0.0017330256142704798\n",
      "train loss:0.008079254505836374\n",
      "train loss:0.004047635782139448\n",
      "train loss:0.0021326103256524647\n",
      "train loss:0.0034088021567283493\n",
      "train loss:0.0038796234238864125\n",
      "train loss:0.02125828862005005\n",
      "train loss:0.006227821727898697\n",
      "train loss:0.013828101199476008\n",
      "train loss:0.0015159709568999\n",
      "train loss:0.005029305045727522\n",
      "train loss:0.042263035313543876\n",
      "train loss:0.008176356761403675\n",
      "train loss:0.036549150363694154\n",
      "train loss:0.03812297929811662\n",
      "train loss:0.0072144192279643485\n",
      "train loss:0.01791092147245682\n",
      "train loss:0.08961968316589841\n",
      "train loss:0.006367034979862807\n",
      "train loss:0.006144061431295941\n",
      "train loss:0.013844508519631156\n",
      "train loss:0.010816405524817803\n",
      "train loss:0.018253847779992602\n",
      "train loss:0.005439161911010115\n",
      "train loss:0.02051868972210416\n",
      "train loss:0.012870041496803417\n",
      "train loss:0.04867214814892855\n",
      "train loss:0.015330879381899387\n",
      "train loss:0.18554071518972923\n",
      "train loss:0.008941534589945371\n",
      "train loss:0.01340732710868479\n",
      "train loss:0.007352049564651138\n",
      "train loss:0.01389105755521236\n",
      "train loss:0.0022124516801131826\n",
      "train loss:0.008981240042202313\n",
      "train loss:0.008753445648843696\n",
      "train loss:0.006093955643224524\n",
      "train loss:0.006285793673120594\n",
      "train loss:0.003758404653489273\n",
      "train loss:0.0063531895974284006\n",
      "train loss:0.01011974465761484\n",
      "train loss:0.028590584591873983\n",
      "train loss:0.007149678982233453\n",
      "train loss:0.015499301943727813\n",
      "train loss:0.015819393938749742\n",
      "train loss:0.010274602055609977\n",
      "=== epoch:8, train acc:0.992, test acc:0.982 ===\n",
      "train loss:0.01119818451435415\n",
      "train loss:0.022208445701004113\n",
      "train loss:0.011833510967276542\n",
      "train loss:0.010557933486326686\n",
      "train loss:0.0296421381896356\n",
      "train loss:0.003634192646478489\n",
      "train loss:0.04255008936125772\n",
      "train loss:0.004025727608010233\n",
      "train loss:0.014552386012367786\n",
      "train loss:0.08135218357624734\n",
      "train loss:0.07466267212591285\n",
      "train loss:0.004781557506657763\n",
      "train loss:0.0044397586705179045\n",
      "train loss:0.017525493199696016\n",
      "train loss:0.007977400298927052\n",
      "train loss:0.007156639007960135\n",
      "train loss:0.009472142260551618\n",
      "train loss:0.009812690077496076\n",
      "train loss:0.007173115700712417\n",
      "train loss:0.03213821270291564\n",
      "train loss:0.005730381346204536\n",
      "train loss:0.013488185527325242\n",
      "train loss:0.012161786185640885\n",
      "train loss:0.014065407782184547\n",
      "train loss:0.0049906465537043814\n",
      "train loss:0.04842973248735895\n",
      "train loss:0.004404636625698941\n",
      "train loss:0.010191520657495363\n",
      "train loss:0.006288012151672041\n",
      "train loss:0.0038420490864254867\n",
      "train loss:0.013423529984284041\n",
      "train loss:0.012119008715325112\n",
      "train loss:0.03515832309316734\n",
      "train loss:0.03167567349735458\n",
      "train loss:0.002342873863941939\n",
      "train loss:0.003603394192397611\n",
      "train loss:0.007621208360119917\n",
      "train loss:0.002868304103139581\n",
      "train loss:0.004604858574475874\n",
      "train loss:0.0008730773886601973\n",
      "train loss:0.037063282714346016\n",
      "train loss:0.007355816487640788\n",
      "train loss:0.0029349079456123567\n",
      "train loss:0.021738431663598807\n",
      "train loss:0.0739546490750893\n",
      "train loss:0.031066818402642662\n",
      "train loss:0.0032999496834319486\n",
      "train loss:0.00161595565396989\n",
      "train loss:0.005764676033505825\n",
      "train loss:0.027368901556373754\n",
      "train loss:0.003960939834679138\n",
      "train loss:0.0016003571207177777\n",
      "train loss:0.0035411200864942254\n",
      "train loss:0.0014092484375170687\n",
      "train loss:0.028161420713464773\n",
      "train loss:0.01104153000211291\n",
      "train loss:0.020838294308920452\n",
      "train loss:0.0222381159989027\n",
      "train loss:0.007611768493950255\n",
      "train loss:0.011138066764649962\n",
      "train loss:0.009867228422339776\n",
      "train loss:0.015554810860582979\n",
      "train loss:0.005620110644457279\n",
      "train loss:0.0076845499872404575\n",
      "train loss:0.004571568531651832\n",
      "train loss:0.004298966681725031\n",
      "train loss:0.026855950595875155\n",
      "train loss:0.011856186625388782\n",
      "train loss:0.022534779031239\n",
      "train loss:0.005395635123981598\n",
      "train loss:0.00844298239752482\n",
      "train loss:0.03518186924882616\n",
      "train loss:0.010555006858404303\n",
      "train loss:0.004857716967907623\n",
      "train loss:0.030157828413038105\n",
      "train loss:0.007518019459960268\n",
      "train loss:0.0043628513641028235\n",
      "train loss:0.010233304107549593\n",
      "train loss:0.007729236454645731\n",
      "train loss:0.006100482973454106\n",
      "train loss:0.003980107462763186\n",
      "train loss:0.0260454703315422\n",
      "train loss:0.028382753856083753\n",
      "train loss:0.02615841634205351\n",
      "train loss:0.0020215043825025287\n",
      "train loss:0.0010588266966437595\n",
      "train loss:0.0042168928723145226\n",
      "train loss:0.007316278230290999\n",
      "train loss:0.004183236694953855\n",
      "train loss:0.00239721990912406\n",
      "train loss:0.0161376286152579\n",
      "train loss:0.011058881897539986\n",
      "train loss:0.007728629429221894\n",
      "train loss:0.020524572661195256\n",
      "train loss:0.015537290411152547\n",
      "train loss:0.005985655950039932\n",
      "train loss:0.014509087363743596\n",
      "train loss:0.010964937171092579\n",
      "train loss:0.02685451940407441\n",
      "train loss:0.02323944235094928\n",
      "train loss:0.010094553859828317\n",
      "train loss:0.0050694848861518\n",
      "train loss:0.0047849044270007295\n",
      "train loss:0.026378401903729053\n",
      "train loss:0.0010683161881931023\n",
      "train loss:0.009984084023980613\n",
      "train loss:0.016079480867271765\n",
      "train loss:0.026351448319530214\n",
      "train loss:0.009401026921965578\n",
      "train loss:0.019965200644681796\n",
      "train loss:0.0021096986038557602\n",
      "train loss:0.03687544913722918\n",
      "train loss:0.004136002245036174\n",
      "train loss:0.02304473026571009\n",
      "train loss:0.037303070573780565\n",
      "train loss:0.003052528774462826\n",
      "train loss:0.0024904347306453135\n",
      "train loss:0.03260195632856483\n",
      "train loss:0.011404173618419371\n",
      "train loss:0.0371230390406282\n",
      "train loss:0.002813380924649129\n",
      "train loss:0.09258750658445453\n",
      "train loss:0.01025698942641511\n",
      "train loss:0.005948899620588669\n",
      "train loss:0.026151716927363268\n",
      "train loss:0.00256438712094246\n",
      "train loss:0.009645325693769247\n",
      "train loss:0.01089717612664724\n",
      "train loss:0.004219706625521039\n",
      "train loss:0.010516234382271401\n",
      "train loss:0.011438133678585372\n",
      "train loss:0.0031724992093575943\n",
      "train loss:0.005965656241709445\n",
      "train loss:0.07565438666083726\n",
      "train loss:0.02260441399992198\n",
      "train loss:0.013322892645655005\n",
      "train loss:0.016584613933503915\n",
      "train loss:0.012134597372500198\n",
      "train loss:0.005357668499965893\n",
      "train loss:0.005522683775997318\n",
      "train loss:0.013291863650688235\n",
      "train loss:0.020366254727692445\n",
      "train loss:0.039716902231143394\n",
      "train loss:0.010199148463910233\n",
      "train loss:0.02121926618665772\n",
      "train loss:0.03790281747879018\n",
      "train loss:0.003584556481780233\n",
      "train loss:0.007295342558725389\n",
      "train loss:0.009533950278457796\n",
      "train loss:0.026829951210531545\n",
      "train loss:0.0014617709541896132\n",
      "train loss:0.01972965684970274\n",
      "train loss:0.011796138826386892\n",
      "train loss:0.040478738867548734\n",
      "train loss:0.02191445440731009\n",
      "train loss:0.011587540195194224\n",
      "train loss:0.005918285716177294\n",
      "train loss:0.060939104539772665\n",
      "train loss:0.0038350264204911216\n",
      "train loss:0.007038317132084852\n",
      "train loss:0.019571249137198817\n",
      "train loss:0.008189533057795129\n",
      "train loss:0.009146509091284151\n",
      "train loss:0.0026966112457272712\n",
      "train loss:0.0010603878906358811\n",
      "train loss:0.11405234425275898\n",
      "train loss:0.002464761145554103\n",
      "train loss:0.005530267672576283\n",
      "train loss:0.012655136049669504\n",
      "train loss:0.16430615466364512\n",
      "train loss:0.004812774036309745\n",
      "train loss:0.01629873781163581\n",
      "train loss:0.03440398917080098\n",
      "train loss:0.007730909970175472\n",
      "train loss:0.0022944653265712157\n",
      "train loss:0.0035602124629907322\n",
      "train loss:0.018074424186207673\n",
      "train loss:0.015943545433475745\n",
      "train loss:0.003918002417860553\n",
      "train loss:0.008731440925946144\n",
      "train loss:0.006578427876596697\n",
      "train loss:0.023411173399810126\n",
      "train loss:0.022074683968384884\n",
      "train loss:0.007284406805797843\n",
      "train loss:0.008707427785108744\n",
      "train loss:0.002189670785421455\n",
      "train loss:0.007353662066595341\n",
      "train loss:0.006605667721788705\n",
      "train loss:0.05410201811222239\n",
      "train loss:0.013293876880659153\n",
      "train loss:0.01053332838126306\n",
      "train loss:0.029773981877405733\n",
      "train loss:0.009215204715186377\n",
      "train loss:0.03304898057512142\n",
      "train loss:0.01039286727901419\n",
      "train loss:0.005494027957807177\n",
      "train loss:0.023529340396380282\n",
      "train loss:0.024314105859383338\n",
      "train loss:0.0021905889838504185\n",
      "train loss:0.0046588129676752325\n",
      "train loss:0.004679083027107817\n",
      "train loss:0.012489964533710618\n",
      "train loss:0.051487864502327986\n",
      "train loss:0.013364652173356388\n",
      "train loss:0.04869057101890271\n",
      "train loss:0.003621732169723841\n",
      "train loss:0.021060921750906552\n",
      "train loss:0.0387590930814628\n",
      "train loss:0.06400819552497668\n",
      "train loss:0.013469719049732624\n",
      "train loss:0.02874985730604692\n",
      "train loss:0.006475751354986566\n",
      "train loss:0.023059999916503742\n",
      "train loss:0.011962251329785496\n",
      "train loss:0.024615400371403626\n",
      "train loss:0.01129209893161168\n",
      "train loss:0.0023129337746737454\n",
      "train loss:0.023879355820409908\n",
      "train loss:0.0029713508945662763\n",
      "train loss:0.04094091234272139\n",
      "train loss:0.016253234546191694\n",
      "train loss:0.022096297884720884\n",
      "train loss:0.0017866587717645756\n",
      "train loss:0.03201831069379224\n",
      "train loss:0.009429155567737548\n",
      "train loss:0.006538685244079046\n",
      "train loss:0.004512357553131709\n",
      "train loss:0.01030219896989764\n",
      "train loss:0.003260098089588387\n",
      "train loss:0.0021641097573141805\n",
      "train loss:0.002957797386234703\n",
      "train loss:0.0016682522826761959\n",
      "train loss:0.0030144990021478356\n",
      "train loss:0.02242699161468015\n",
      "train loss:0.006217706290251535\n",
      "train loss:0.0024079162370764704\n",
      "train loss:0.03684587426597435\n",
      "train loss:0.019827642078141864\n",
      "train loss:0.00120104298606142\n",
      "train loss:0.00200910358032421\n",
      "train loss:0.016105125608006657\n",
      "train loss:0.00508999452017196\n",
      "train loss:0.03168969713352875\n",
      "train loss:0.02566700533220044\n",
      "train loss:0.0007068915263973947\n",
      "train loss:0.022074381213131603\n",
      "train loss:0.03639326273750085\n",
      "train loss:0.0101266057848985\n",
      "train loss:0.005077926410048326\n",
      "train loss:0.002964448110419941\n",
      "train loss:0.01050224258142971\n",
      "train loss:0.017962845398837343\n",
      "train loss:0.026666377186475684\n",
      "train loss:0.014284126140979252\n",
      "train loss:0.0037749849996752565\n",
      "train loss:0.01677620062202837\n",
      "train loss:0.045608180513858565\n",
      "train loss:0.0012402010146363481\n",
      "train loss:0.002851120345409233\n",
      "train loss:0.004414383666339743\n",
      "train loss:0.023304853063658256\n",
      "train loss:0.014288394822418908\n",
      "train loss:0.005900220486685933\n",
      "train loss:0.047387763558164264\n",
      "train loss:0.010468483267729738\n",
      "train loss:0.007599920006340109\n",
      "train loss:0.00945069833104273\n",
      "train loss:0.024990786446774136\n",
      "train loss:0.01404858610204261\n",
      "train loss:0.0045146359130317235\n",
      "train loss:0.016622521483951328\n",
      "train loss:0.007592556805287862\n",
      "train loss:0.007649356686256281\n",
      "train loss:0.023658653018052634\n",
      "train loss:0.004496238740164066\n",
      "train loss:0.011639130804287516\n",
      "train loss:0.016512690692178686\n",
      "train loss:0.028192415346971714\n",
      "train loss:0.0008650791016510849\n",
      "train loss:0.0014491481403121078\n",
      "train loss:0.031243922179710625\n",
      "train loss:0.005462810675087497\n",
      "train loss:0.0027484939421521877\n",
      "train loss:0.011813807925383626\n",
      "train loss:0.00792978534496024\n",
      "train loss:0.0075195882121115755\n",
      "train loss:0.005436395712724275\n",
      "train loss:0.004062632827438719\n",
      "train loss:0.001569391640506198\n",
      "train loss:0.004891374368255252\n",
      "train loss:0.041473856290105096\n",
      "train loss:0.004990919796138335\n",
      "train loss:0.005370262915736623\n",
      "train loss:0.007756335042545894\n",
      "train loss:0.09771339194258313\n",
      "train loss:0.016626622428654335\n",
      "train loss:0.00651711409177176\n",
      "train loss:0.012478633807144984\n",
      "train loss:0.05467491992879893\n",
      "train loss:0.017438881365472215\n",
      "train loss:0.03823244836550027\n",
      "train loss:0.004437725186870754\n",
      "train loss:0.037439138548807684\n",
      "train loss:0.002191980358846829\n",
      "train loss:0.03262204638248325\n",
      "train loss:0.02044083909742548\n",
      "train loss:0.046627221731414946\n",
      "train loss:0.00809654937505141\n",
      "train loss:0.017211119681183613\n",
      "train loss:0.00671069416001771\n",
      "train loss:0.01344768781760626\n",
      "train loss:0.021118484191873326\n",
      "train loss:0.039102513099656705\n",
      "train loss:0.0018459881929648247\n",
      "train loss:0.012826651139821274\n",
      "train loss:0.0038442100263043743\n",
      "train loss:0.007306707975878999\n",
      "train loss:0.002225023360865817\n",
      "train loss:0.010723371082840116\n",
      "train loss:0.04539192217992139\n",
      "train loss:0.026943533155924914\n",
      "train loss:0.005542437883198884\n",
      "train loss:0.011821772428653631\n",
      "train loss:0.01879963417091889\n",
      "train loss:0.02132250912886673\n",
      "train loss:0.020962083314284962\n",
      "train loss:0.03296367579447399\n",
      "train loss:0.0030068221182140047\n",
      "train loss:0.009465425257097385\n",
      "train loss:0.018606896771121886\n",
      "train loss:0.006436477018630876\n",
      "train loss:0.013122338608897942\n",
      "train loss:0.0204361003117984\n",
      "train loss:0.018583667992679465\n",
      "train loss:0.004527409055329553\n",
      "train loss:0.010672059046751699\n",
      "train loss:0.022245640112549737\n",
      "train loss:0.008439046017778846\n",
      "train loss:0.009201365816848465\n",
      "train loss:0.005640276024519952\n",
      "train loss:0.004126897174226037\n",
      "train loss:0.013067197775247931\n",
      "train loss:0.0064510115233448895\n",
      "train loss:0.011882633743123787\n",
      "train loss:0.023596825764162083\n",
      "train loss:0.0043191451725335455\n",
      "train loss:0.007684694449724745\n",
      "train loss:0.0027734123023155444\n",
      "train loss:0.03961659505555724\n",
      "train loss:0.008769542843637986\n",
      "train loss:0.016625962317783197\n",
      "train loss:0.007219644840964201\n",
      "train loss:0.008412959947888403\n",
      "train loss:0.004791018528413548\n",
      "train loss:0.0062166774675006045\n",
      "train loss:0.004091925652146936\n",
      "train loss:0.015471318254176634\n",
      "train loss:0.05519358783225184\n",
      "train loss:0.002878009654374783\n",
      "train loss:0.002856148414241004\n",
      "train loss:0.09399061189017312\n",
      "train loss:0.02369350609826195\n",
      "train loss:0.019940287078559774\n",
      "train loss:0.007756113332420818\n",
      "train loss:0.004786120521340142\n",
      "train loss:0.006222900945065954\n",
      "train loss:0.007831085721212766\n",
      "train loss:0.003241698405981022\n",
      "train loss:0.003113795374014471\n",
      "train loss:0.012679097781884635\n",
      "train loss:0.07042726659575481\n",
      "train loss:0.012746955913098023\n",
      "train loss:0.00878785576883759\n",
      "train loss:0.0092162109247047\n",
      "train loss:0.027743836744732877\n",
      "train loss:0.04003390282099626\n",
      "train loss:0.014086830000845586\n",
      "train loss:0.016158992084174345\n",
      "train loss:0.0054720206740134884\n",
      "train loss:0.015802940022788096\n",
      "train loss:0.00884010025522402\n",
      "train loss:0.005738341767138666\n",
      "train loss:0.024624476916845457\n",
      "train loss:0.00841581499490906\n",
      "train loss:0.003042371285211242\n",
      "train loss:0.006367829320524245\n",
      "train loss:0.060473092849523874\n",
      "train loss:0.013348675134991972\n",
      "train loss:0.0027490945064977184\n",
      "train loss:0.0037721941767797727\n",
      "train loss:0.0006155693024545162\n",
      "train loss:0.0048632105745961204\n",
      "train loss:0.0005685857250535009\n",
      "train loss:0.0037898821051131097\n",
      "train loss:0.0208936045448513\n",
      "train loss:0.004316126245731212\n",
      "train loss:0.015455943048584202\n",
      "train loss:0.008299086617292376\n",
      "train loss:0.028614333616894045\n",
      "train loss:0.0019553219844166123\n",
      "train loss:0.013004865732769448\n",
      "train loss:0.004630146768592391\n",
      "train loss:0.0018043727061801817\n",
      "train loss:0.004852062008619936\n",
      "train loss:0.00682955039082356\n",
      "train loss:0.01668936905065037\n",
      "train loss:0.01766890618426995\n",
      "train loss:0.010273780906343228\n",
      "train loss:0.00341039752307736\n",
      "train loss:0.010591588536933467\n",
      "train loss:0.011724283661506553\n",
      "train loss:0.001350691867161075\n",
      "train loss:0.008749161679940531\n",
      "train loss:0.002104848588528613\n",
      "train loss:0.003998022080132672\n",
      "train loss:0.027130176381974788\n",
      "train loss:0.01979511431535793\n",
      "train loss:0.01643302603065067\n",
      "train loss:0.00498124779236\n",
      "train loss:0.014903158821587664\n",
      "train loss:0.04993128314353557\n",
      "train loss:0.03271214331710124\n",
      "train loss:0.003927328488652725\n",
      "train loss:0.01540095987228849\n",
      "train loss:0.03449022018885124\n",
      "train loss:0.0039840801962781735\n",
      "train loss:0.0027307145835386228\n",
      "train loss:0.0014225975644110553\n",
      "train loss:0.006579473930446094\n",
      "train loss:0.00208504537954943\n",
      "train loss:0.0035378588257061515\n",
      "train loss:0.0022712812221578876\n",
      "train loss:0.004618115404829203\n",
      "train loss:0.003032960106914163\n",
      "train loss:0.023625439724570308\n",
      "train loss:0.00442902913386024\n",
      "train loss:0.007786763539917729\n",
      "train loss:0.0028335745458938338\n",
      "train loss:0.01597773937108301\n",
      "train loss:0.03037502926249231\n",
      "train loss:0.006821421721405964\n",
      "train loss:0.003411669293466715\n",
      "train loss:0.023165111985060865\n",
      "train loss:0.014905620177882568\n",
      "train loss:0.003415880723629561\n",
      "train loss:0.004720735314928889\n",
      "train loss:0.001594944972851875\n",
      "train loss:0.003970087970479733\n",
      "train loss:0.0032465334090542256\n",
      "train loss:0.02902082217489968\n",
      "train loss:0.014005703136168002\n",
      "train loss:0.016081393864122596\n",
      "train loss:0.019576213698296218\n",
      "train loss:0.01168828879792781\n",
      "train loss:0.006542298138405264\n",
      "train loss:0.00919094082667523\n",
      "train loss:0.03213937364149825\n",
      "train loss:0.002901825233961963\n",
      "train loss:0.0063943608645865525\n",
      "train loss:0.010238255945234406\n",
      "train loss:0.00461889804658168\n",
      "train loss:0.012504815302247901\n",
      "train loss:0.013757502424558398\n",
      "train loss:0.02225366977365074\n",
      "train loss:0.009498435851194795\n",
      "train loss:0.006378432463086583\n",
      "train loss:0.0015935351536946916\n",
      "train loss:0.006371770842341149\n",
      "train loss:0.0062837612519997975\n",
      "train loss:0.007140483764283774\n",
      "train loss:0.005024099363708404\n",
      "train loss:0.005628471637480429\n",
      "train loss:0.002336028271722017\n",
      "train loss:0.006989238408342261\n",
      "train loss:0.006271296372547327\n",
      "train loss:0.010968594124695785\n",
      "train loss:0.001691503147253473\n",
      "train loss:0.005637352580581574\n",
      "train loss:0.0038902594846964293\n",
      "train loss:0.002766072532154057\n",
      "train loss:0.0015781468306241327\n",
      "train loss:0.030188321836431244\n",
      "train loss:0.01410884344022171\n",
      "train loss:0.0063839180430298155\n",
      "train loss:0.006597478909238358\n",
      "train loss:0.025717466124825346\n",
      "train loss:0.0061561881074226835\n",
      "train loss:0.003426509997773357\n",
      "train loss:0.0034900534774164136\n",
      "train loss:0.01556165843083655\n",
      "train loss:0.014539618650870374\n",
      "train loss:0.005343724368632131\n",
      "train loss:0.0265785033437094\n",
      "train loss:0.036478597567820216\n",
      "train loss:0.005296271720391358\n",
      "train loss:0.03230497898242238\n",
      "train loss:0.011514108939103247\n",
      "train loss:0.011698856894996277\n",
      "train loss:0.0044972138453192225\n",
      "train loss:0.00556982989145425\n",
      "train loss:0.012962761201553326\n",
      "train loss:0.007881169429425993\n",
      "train loss:0.003353663817371961\n",
      "train loss:0.00425017621360248\n",
      "train loss:0.002563608404006107\n",
      "train loss:0.012060260550702981\n",
      "train loss:0.003118684695502799\n",
      "train loss:0.00424533572014406\n",
      "train loss:0.003749887170735411\n",
      "train loss:0.0015578473775609888\n",
      "train loss:0.011129676210443442\n",
      "train loss:0.001442491588677116\n",
      "train loss:0.0019318014562425903\n",
      "train loss:0.01490195476266974\n",
      "train loss:0.00943041898033709\n",
      "train loss:0.006653614531308816\n",
      "train loss:0.0034123715328977034\n",
      "train loss:0.01731434483950397\n",
      "train loss:0.008361648268363246\n",
      "train loss:0.015779217937355584\n",
      "train loss:0.010352697704808287\n",
      "train loss:0.01759185320173208\n",
      "train loss:0.008054223457754126\n",
      "train loss:0.006028907515402832\n",
      "train loss:0.0034667860729876913\n",
      "train loss:0.007250893237762866\n",
      "train loss:0.00915271598155457\n",
      "train loss:0.007403576118277088\n",
      "train loss:0.0013337437791022388\n",
      "train loss:0.036516445347894486\n",
      "train loss:0.00896781223120533\n",
      "train loss:0.0070533381906119595\n",
      "train loss:0.0020484033464547497\n",
      "train loss:0.013734585693570665\n",
      "train loss:0.004109783632632866\n",
      "train loss:0.007731293477373692\n",
      "train loss:0.014018156078387608\n",
      "train loss:0.0025457349644682713\n",
      "train loss:0.007707469535613224\n",
      "train loss:0.006254409875849205\n",
      "train loss:0.010494601712205995\n",
      "train loss:0.002610658426861649\n",
      "train loss:0.00441637376472154\n",
      "train loss:0.03563183998161836\n",
      "train loss:0.01755841736758434\n",
      "train loss:0.0028293593529224725\n",
      "train loss:0.006187770210497257\n",
      "train loss:0.0005351498177374482\n",
      "train loss:0.012272723568859844\n",
      "train loss:0.014966435466887092\n",
      "train loss:0.0020773265526366847\n",
      "train loss:0.008813890521303581\n",
      "train loss:0.021216399335519776\n",
      "train loss:0.005218257311271652\n",
      "train loss:0.002654743722930901\n",
      "train loss:0.006306582399649327\n",
      "train loss:0.011870172452613228\n",
      "train loss:0.0025303727737023447\n",
      "train loss:0.011021525629942882\n",
      "train loss:0.004652022585527705\n",
      "train loss:0.0011815155687010435\n",
      "train loss:0.002083443633334335\n",
      "train loss:0.022204019696200145\n",
      "train loss:0.003045462789085461\n",
      "train loss:0.03815056311723002\n",
      "train loss:0.008391189986698619\n",
      "train loss:0.002347595144721074\n",
      "train loss:0.0033479196362792506\n",
      "train loss:0.02403799398679434\n",
      "train loss:0.008442174286706933\n",
      "train loss:0.003738652321355856\n",
      "train loss:0.01218443041403523\n",
      "train loss:0.004154650623200736\n",
      "train loss:0.0006502434233263887\n",
      "train loss:0.004228895484406964\n",
      "train loss:0.011181446334922846\n",
      "train loss:0.005467326713105857\n",
      "train loss:0.025271505675254685\n",
      "train loss:0.0033887125144596727\n",
      "train loss:0.004207450056210176\n",
      "train loss:0.008075816332470075\n",
      "train loss:0.002299308193709861\n",
      "train loss:0.0031240971003363687\n",
      "train loss:0.005102983019522382\n",
      "train loss:0.021315720512638036\n",
      "train loss:0.015493562529784996\n",
      "train loss:0.03086934022245853\n",
      "train loss:0.012534931726845267\n",
      "train loss:0.003370713389488401\n",
      "train loss:0.0027044966686278586\n",
      "train loss:0.006988792593011594\n",
      "train loss:0.010523874502703373\n",
      "train loss:0.002727445208085449\n",
      "train loss:0.013861803869453343\n",
      "train loss:0.11659612596198814\n",
      "train loss:0.0056460535947064\n",
      "train loss:0.004877531023277815\n",
      "train loss:0.003397304108155039\n",
      "train loss:0.005601953156356905\n",
      "train loss:0.0191634165178696\n",
      "=== epoch:9, train acc:0.992, test acc:0.988 ===\n",
      "train loss:0.008975330464633852\n",
      "train loss:0.009758149279826578\n",
      "train loss:0.0024070490990250252\n",
      "train loss:0.006559642425600574\n",
      "train loss:0.0035946937405155494\n",
      "train loss:0.004542508018106781\n",
      "train loss:0.003124940864486463\n",
      "train loss:0.015736213858222232\n",
      "train loss:0.03133982552924077\n",
      "train loss:0.002610572992467701\n",
      "train loss:0.005609107110860381\n",
      "train loss:0.03168072925458772\n",
      "train loss:0.014460874911548903\n",
      "train loss:0.014098356912770322\n",
      "train loss:0.00735930711144221\n",
      "train loss:0.025864387211715578\n",
      "train loss:0.00928498234791762\n",
      "train loss:0.023128852390951696\n",
      "train loss:0.025026249443480326\n",
      "train loss:0.01751614728516763\n",
      "train loss:0.0008329890850920624\n",
      "train loss:0.013217739002589688\n",
      "train loss:0.026985544515813076\n",
      "train loss:0.004032452916384521\n",
      "train loss:0.02188181911885251\n",
      "train loss:0.03196466972258447\n",
      "train loss:0.010857853548578758\n",
      "train loss:0.022595571456840494\n",
      "train loss:0.004726745405251659\n",
      "train loss:0.0032518919495136646\n",
      "train loss:0.004142188890960228\n",
      "train loss:0.0038987085254711916\n",
      "train loss:0.0041393432381969745\n",
      "train loss:0.003962058853163529\n",
      "train loss:0.0023240269514881377\n",
      "train loss:0.01209471052393602\n",
      "train loss:0.002390242190495305\n",
      "train loss:0.001935805256122851\n",
      "train loss:0.00107449812556765\n",
      "train loss:0.006541049234524893\n",
      "train loss:0.0013894576720903668\n",
      "train loss:0.0103934969682051\n",
      "train loss:0.0066558343845102685\n",
      "train loss:0.03439992117252124\n",
      "train loss:0.013445415705035128\n",
      "train loss:0.0023415352585824563\n",
      "train loss:0.010399100668677805\n",
      "train loss:0.004205086435869773\n",
      "train loss:0.0339623278702689\n",
      "train loss:0.005689584674835304\n",
      "train loss:0.0030064113875444337\n",
      "train loss:0.0072457080341985225\n",
      "train loss:0.018803023242634012\n",
      "train loss:0.010310042633427436\n",
      "train loss:0.036915934747995774\n",
      "train loss:0.017217130487734034\n",
      "train loss:0.0014291991876778376\n",
      "train loss:0.004285272735585902\n",
      "train loss:0.013227027345575078\n",
      "train loss:0.011300213531120708\n",
      "train loss:0.008952482846371072\n",
      "train loss:0.04775058193456256\n",
      "train loss:0.0051140839194665\n",
      "train loss:0.023724521836219484\n",
      "train loss:0.05671664107891941\n",
      "train loss:0.004531386382419688\n",
      "train loss:0.01769366576227186\n",
      "train loss:0.005787379196376974\n",
      "train loss:0.004529295593393969\n",
      "train loss:0.04443095203556124\n",
      "train loss:0.0013594208251323577\n",
      "train loss:0.015052765888875315\n",
      "train loss:0.0013370192608964917\n",
      "train loss:0.008905567684791805\n",
      "train loss:0.05949546413690718\n",
      "train loss:0.04037851341731469\n",
      "train loss:0.08344727771080594\n",
      "train loss:0.029349917244404996\n",
      "train loss:0.0063101176632786435\n",
      "train loss:0.016224369025045156\n",
      "train loss:0.0010960397611442792\n",
      "train loss:0.004900308991764996\n",
      "train loss:0.0023888207210759744\n",
      "train loss:0.003815715953904811\n",
      "train loss:0.005060415906053804\n",
      "train loss:0.007210831174022784\n",
      "train loss:0.0082046614649307\n",
      "train loss:0.08077392410129419\n",
      "train loss:0.03215178988147352\n",
      "train loss:0.006492993243650803\n",
      "train loss:0.004158870942048203\n",
      "train loss:0.06660535903817302\n",
      "train loss:0.009949670255021534\n",
      "train loss:0.009359828666842089\n",
      "train loss:0.01700080883606934\n",
      "train loss:0.0037302340859938377\n",
      "train loss:0.011883959716414053\n",
      "train loss:0.0028767831770292558\n",
      "train loss:0.004805634814585348\n",
      "train loss:0.0012293210596181089\n",
      "train loss:0.005299682723478142\n",
      "train loss:0.012257765520719579\n",
      "train loss:0.0019606888087738473\n",
      "train loss:0.00400173831372184\n",
      "train loss:0.030844321708228827\n",
      "train loss:0.0018711964168443246\n",
      "train loss:0.002180521923487473\n",
      "train loss:0.007266415553780229\n",
      "train loss:0.01703020278315474\n",
      "train loss:0.003920978799065345\n",
      "train loss:0.007471719066465875\n",
      "train loss:0.005035009520199582\n",
      "train loss:0.021952469642516185\n",
      "train loss:0.013675865122756242\n",
      "train loss:0.02198376757538807\n",
      "train loss:0.01444852600636754\n",
      "train loss:0.01719399214075813\n",
      "train loss:0.007435147317809594\n",
      "train loss:0.0025042833860252885\n",
      "train loss:0.000973820718929973\n",
      "train loss:0.0013552442195738058\n",
      "train loss:0.012014482120255252\n",
      "train loss:0.005307043647673188\n",
      "train loss:0.003785209833678781\n",
      "train loss:0.011261314471637345\n",
      "train loss:0.008842690070912535\n",
      "train loss:0.001425074653560044\n",
      "train loss:0.004079061124173315\n",
      "train loss:0.0157855880993647\n",
      "train loss:0.002239496035093419\n",
      "train loss:0.0018151152869213887\n",
      "train loss:0.014233065516173133\n",
      "train loss:0.0160597614210064\n",
      "train loss:0.0032508116129850956\n",
      "train loss:0.0013232687113470876\n",
      "train loss:0.0047928110001181625\n",
      "train loss:0.005551687861128938\n",
      "train loss:0.013928507464859513\n",
      "train loss:0.015969550683634398\n",
      "train loss:0.004254130377957105\n",
      "train loss:0.021352510542789128\n",
      "train loss:0.005960823767109712\n",
      "train loss:0.002217231045538959\n",
      "train loss:0.005889301248130132\n",
      "train loss:0.022912428586286296\n",
      "train loss:0.0028454782438504106\n",
      "train loss:0.048356448477900556\n",
      "train loss:0.07719860515442824\n",
      "train loss:0.0028613586355416163\n",
      "train loss:0.009433576352635873\n",
      "train loss:0.009059654356646952\n",
      "train loss:0.005828060474070857\n",
      "train loss:0.004033158364706814\n",
      "train loss:0.03813334510094115\n",
      "train loss:0.013468722198402016\n",
      "train loss:0.004019483471410259\n",
      "train loss:0.014737455141623904\n",
      "train loss:0.04935135771584721\n",
      "train loss:0.07092185841399676\n",
      "train loss:0.02164155843807761\n",
      "train loss:0.002770670489683578\n",
      "train loss:0.0015609983225158394\n",
      "train loss:0.0016905127188157224\n",
      "train loss:0.0040476863037544435\n",
      "train loss:0.006021706982957791\n",
      "train loss:0.01614645209265197\n",
      "train loss:0.005414227307488834\n",
      "train loss:0.009357559271588245\n",
      "train loss:0.016038136661993083\n",
      "train loss:0.00649262392613974\n",
      "train loss:0.010892639075908078\n",
      "train loss:0.0038313451261306935\n",
      "train loss:0.008356950506808316\n",
      "train loss:0.00922746237760791\n",
      "train loss:0.02809716129081994\n",
      "train loss:0.00868353064150257\n",
      "train loss:0.00039500339400585333\n",
      "train loss:0.012979851644095484\n",
      "train loss:0.005804030536808609\n",
      "train loss:0.009731588015238128\n",
      "train loss:0.01061789568698017\n",
      "train loss:0.009971607410092937\n",
      "train loss:0.01097825124981629\n",
      "train loss:0.009239667554849224\n",
      "train loss:0.0022500368552483027\n",
      "train loss:0.004335892197328312\n",
      "train loss:0.003315802968660644\n",
      "train loss:0.015350222811790428\n",
      "train loss:0.019443994930977523\n",
      "train loss:0.0031698262133008494\n",
      "train loss:0.004833704463253492\n",
      "train loss:0.0041326250240474125\n",
      "train loss:0.005241562765848034\n",
      "train loss:0.003966558197386583\n",
      "train loss:0.014134247362902941\n",
      "train loss:0.018525321674716603\n",
      "train loss:0.008226260977730673\n",
      "train loss:0.008543023773952347\n",
      "train loss:0.011865452544987195\n",
      "train loss:0.07643527331535194\n",
      "train loss:0.05252155846365916\n",
      "train loss:0.020798609859121627\n",
      "train loss:0.0034358917730842385\n",
      "train loss:0.002220624916287621\n",
      "train loss:0.009892346313355161\n",
      "train loss:0.009666562069571756\n",
      "train loss:0.017076767210865304\n",
      "train loss:0.002917634055791122\n",
      "train loss:0.009710250780598855\n",
      "train loss:0.004544258277763487\n",
      "train loss:0.0024278215930930223\n",
      "train loss:0.02630851880550281\n",
      "train loss:0.005990495690020583\n",
      "train loss:0.00621540552568798\n",
      "train loss:0.03629121504057084\n",
      "train loss:0.010587278171850605\n",
      "train loss:0.004222320998629005\n",
      "train loss:0.008596873249933442\n",
      "train loss:0.0008033008186595117\n",
      "train loss:0.005609757114848964\n",
      "train loss:0.01948815592055217\n",
      "train loss:0.032252614374220355\n",
      "train loss:0.008385348784904578\n",
      "train loss:0.008928301820340136\n",
      "train loss:0.009352428323917054\n",
      "train loss:0.006030565478388574\n",
      "train loss:0.005280782950600622\n",
      "train loss:0.01885537660509997\n",
      "train loss:0.006470226109184418\n",
      "train loss:0.0051375325038302555\n",
      "train loss:0.003929580293787898\n",
      "train loss:0.02255244469321003\n",
      "train loss:0.006940943813623249\n",
      "train loss:0.00339811882193066\n",
      "train loss:0.001852812805851062\n",
      "train loss:0.007868751511179687\n",
      "train loss:0.007844821526234365\n",
      "train loss:0.007891552096901731\n",
      "train loss:0.04919122587526543\n",
      "train loss:0.006709123072092708\n",
      "train loss:0.01816130190760439\n",
      "train loss:0.05475011640847022\n",
      "train loss:0.005701938129168733\n",
      "train loss:0.024769108096201285\n",
      "train loss:0.011034469678769001\n",
      "train loss:0.0019469231849538818\n",
      "train loss:0.004943502316493707\n",
      "train loss:0.009577756383728282\n",
      "train loss:0.01122324717026951\n",
      "train loss:0.003982740706767941\n",
      "train loss:0.006000100666540278\n",
      "train loss:0.008303826841909218\n",
      "train loss:0.004271771089117213\n",
      "train loss:0.006721511424972586\n",
      "train loss:0.003115892978519088\n",
      "train loss:0.004376164997796701\n",
      "train loss:0.00948686079282428\n",
      "train loss:0.008603735617454046\n",
      "train loss:0.05364379955840044\n",
      "train loss:0.026109142144017737\n",
      "train loss:0.030098528567448674\n",
      "train loss:0.006300238316170813\n",
      "train loss:0.00843227273156894\n",
      "train loss:0.003401975788064607\n",
      "train loss:0.09569258904946955\n",
      "train loss:0.0042664762017817085\n",
      "train loss:0.006398311844066389\n",
      "train loss:0.0022356053595833907\n",
      "train loss:0.00959555166664886\n",
      "train loss:0.009977240730732986\n",
      "train loss:0.0193300830555673\n",
      "train loss:0.03128797231803209\n",
      "train loss:0.0014229236444679972\n",
      "train loss:0.004529778884974418\n",
      "train loss:0.043513510495770034\n",
      "train loss:0.03614912286508859\n",
      "train loss:0.0022920248819921435\n",
      "train loss:0.040291768559335625\n",
      "train loss:0.0032753945069074154\n",
      "train loss:0.02335272809928206\n",
      "train loss:0.0029773081130894967\n",
      "train loss:0.00329727923502777\n",
      "train loss:0.0019508000808788797\n",
      "train loss:0.008581492356622046\n",
      "train loss:0.016502677861960562\n",
      "train loss:0.031091883372764192\n",
      "train loss:0.0069410868455099485\n",
      "train loss:0.012993334263886663\n",
      "train loss:0.00609055499900973\n",
      "train loss:0.010467398194190354\n",
      "train loss:0.0030823684742704517\n",
      "train loss:0.004842092092811571\n",
      "train loss:0.03225245202519177\n",
      "train loss:0.003718897103216089\n",
      "train loss:0.006227898916002744\n",
      "train loss:0.010069922782620404\n",
      "train loss:0.002238158094393324\n",
      "train loss:0.0045024080400536464\n",
      "train loss:0.006995594807912036\n",
      "train loss:0.008842996487120565\n",
      "train loss:0.016134138910838355\n",
      "train loss:0.005053847273447457\n",
      "train loss:0.00986028602597801\n",
      "train loss:0.0011565158241185936\n",
      "train loss:0.0048989899943885015\n",
      "train loss:0.012332934834246904\n",
      "train loss:0.004218169890789065\n",
      "train loss:0.0006807606599393919\n",
      "train loss:0.00932704953946852\n",
      "train loss:0.010755739160139763\n",
      "train loss:0.003594517501849629\n",
      "train loss:0.003754044067390815\n",
      "train loss:0.007110665840625038\n",
      "train loss:0.0029325877475675955\n",
      "train loss:0.012907807393286987\n",
      "train loss:0.002361139736469431\n",
      "train loss:0.0050465460680745285\n",
      "train loss:0.012735032286557368\n",
      "train loss:0.013955737248991362\n",
      "train loss:0.001002277539879565\n",
      "train loss:0.0012889027780380459\n",
      "train loss:0.0031062934323418615\n",
      "train loss:0.01100963262330765\n",
      "train loss:0.004537700351675309\n",
      "train loss:0.003814837523573586\n",
      "train loss:0.002094500945713427\n",
      "train loss:0.0022564306662073144\n",
      "train loss:0.002364023427789295\n",
      "train loss:0.004654247068425147\n",
      "train loss:0.016988319303375625\n",
      "train loss:0.010620968594269142\n",
      "train loss:0.013163558791368064\n",
      "train loss:0.0013671929705552457\n",
      "train loss:0.003796603070605791\n",
      "train loss:0.005209507825646061\n",
      "train loss:0.008100310097319074\n",
      "train loss:0.0007761561430229235\n",
      "train loss:0.01343776614380079\n",
      "train loss:0.01875614810151882\n",
      "train loss:0.005237520050831676\n",
      "train loss:0.0029851410961189606\n",
      "train loss:0.0057949691088295665\n",
      "train loss:0.006561402255965652\n",
      "train loss:0.008289891628602624\n",
      "train loss:0.004879757097598946\n",
      "train loss:0.01036781065449293\n",
      "train loss:0.0009209264354610298\n",
      "train loss:0.008845871297918523\n",
      "train loss:0.004325846641992209\n",
      "train loss:0.0028377517675124858\n",
      "train loss:0.00355076327138298\n",
      "train loss:0.005231503523894152\n",
      "train loss:0.0030724661267585336\n",
      "train loss:0.02786336770054931\n",
      "train loss:0.0027127681054658504\n",
      "train loss:0.002339091121122675\n",
      "train loss:0.06383976373670162\n",
      "train loss:0.001851450458350978\n",
      "train loss:0.003125772775182028\n",
      "train loss:0.0016538584475277313\n",
      "train loss:0.00257282282534141\n",
      "train loss:0.008469346230689859\n",
      "train loss:0.005292147354894795\n",
      "train loss:0.01529042202707052\n",
      "train loss:0.0005710737454627879\n",
      "train loss:0.006264839391582508\n",
      "train loss:0.01019372276054668\n",
      "train loss:0.0014213593340501724\n",
      "train loss:0.00428481996375392\n",
      "train loss:0.0008157667256429024\n",
      "train loss:0.004474973599925297\n",
      "train loss:0.0019994313132433448\n",
      "train loss:0.007171556535456057\n",
      "train loss:0.016804918809075246\n",
      "train loss:0.040377125140325903\n",
      "train loss:0.014286824524160737\n",
      "train loss:0.007391467109656577\n",
      "train loss:0.00571851451431773\n",
      "train loss:0.006573979769780474\n",
      "train loss:0.006795893696362066\n",
      "train loss:0.009878892755216773\n",
      "train loss:0.004384616857958314\n",
      "train loss:0.005907787411458466\n",
      "train loss:0.07224752043638163\n",
      "train loss:0.0035665959255468793\n",
      "train loss:0.003599806690306371\n",
      "train loss:0.01615670987878608\n",
      "train loss:0.012054629147275073\n",
      "train loss:0.0046859054204817915\n",
      "train loss:0.00554410731684383\n",
      "train loss:0.006323258343336016\n",
      "train loss:0.0029601718988709036\n",
      "train loss:0.008043970170314451\n",
      "train loss:0.0020639302421098173\n",
      "train loss:0.04999960652038411\n",
      "train loss:0.008227694891310281\n",
      "train loss:0.006334526182919658\n",
      "train loss:0.02198678341511813\n",
      "train loss:0.0016084201985824005\n",
      "train loss:0.0019971931347765983\n",
      "train loss:0.0035294044054013703\n",
      "train loss:0.003802718126347505\n",
      "train loss:0.0060204339473849625\n",
      "train loss:0.0032915577153843243\n",
      "train loss:0.0526523870385222\n",
      "train loss:0.0067935385456398024\n",
      "train loss:0.01610364817740726\n",
      "train loss:0.00840801846849284\n",
      "train loss:0.018582859644466664\n",
      "train loss:0.00845656065337058\n",
      "train loss:0.001822508528560688\n",
      "train loss:0.01600612330728523\n",
      "train loss:0.012716334312724254\n",
      "train loss:0.0022331201988173887\n",
      "train loss:0.003045505587757633\n",
      "train loss:0.017255250419270177\n",
      "train loss:0.008476987726494715\n",
      "train loss:0.002606753276493042\n",
      "train loss:0.0025033621472737365\n",
      "train loss:0.012568350372442609\n",
      "train loss:0.0026139048842133933\n",
      "train loss:0.0018107805612387776\n",
      "train loss:0.015939323335007246\n",
      "train loss:0.0012275243846748708\n",
      "train loss:0.009593114176356521\n",
      "train loss:0.010550394071702393\n",
      "train loss:0.003419754611450887\n",
      "train loss:0.04276331014998343\n",
      "train loss:0.01808833716518903\n",
      "train loss:0.004385975185549561\n",
      "train loss:0.009393245273125246\n",
      "train loss:0.019089641468932786\n",
      "train loss:0.018114119922116586\n",
      "train loss:0.0011931718758805723\n",
      "train loss:0.001647804929925935\n",
      "train loss:0.015244133337834358\n",
      "train loss:0.010976914893311082\n",
      "train loss:0.002753314022131395\n",
      "train loss:0.004367013572298165\n",
      "train loss:0.01170629191264578\n",
      "train loss:0.007972910675002599\n",
      "train loss:0.0009375382624315712\n",
      "train loss:0.0030614990396945242\n",
      "train loss:0.0083436696495671\n",
      "train loss:0.0023373102896520797\n",
      "train loss:0.005303632489312699\n",
      "train loss:0.002284862947574154\n",
      "train loss:0.009713351194878098\n",
      "train loss:0.0014756603010599553\n",
      "train loss:0.004588906429178368\n",
      "train loss:0.0039571867182528085\n",
      "train loss:0.004551183727368952\n",
      "train loss:0.0012385918798081862\n",
      "train loss:0.008439183004954866\n",
      "train loss:0.008373580212224025\n",
      "train loss:0.0032093411369380925\n",
      "train loss:0.009700787537677717\n",
      "train loss:0.0062880842277608495\n",
      "train loss:0.003042293883182319\n",
      "train loss:0.008246822418957914\n",
      "train loss:0.0010515178193752407\n",
      "train loss:0.006579127250658576\n",
      "train loss:0.005426277108707644\n",
      "train loss:0.01032172633544089\n",
      "train loss:0.010022206538115898\n",
      "train loss:0.0014382212242614492\n",
      "train loss:0.002771799439815988\n",
      "train loss:0.009149223327149349\n",
      "train loss:0.01845016171865551\n",
      "train loss:0.0014766076702544511\n",
      "train loss:0.0076685110156531\n",
      "train loss:0.006123657584877119\n",
      "train loss:0.007523300847773512\n",
      "train loss:0.01753179716682213\n",
      "train loss:0.019163755261862477\n",
      "train loss:0.0035972324859309996\n",
      "train loss:0.01260389423737369\n",
      "train loss:0.0013770026175962894\n",
      "train loss:0.03281522837849927\n",
      "train loss:0.0028716358590039153\n",
      "train loss:0.03036642680933026\n",
      "train loss:0.0072282238436565515\n",
      "train loss:0.0007084518862922852\n",
      "train loss:0.012735063618757939\n",
      "train loss:0.0070838601920814506\n",
      "train loss:0.005282333273366281\n",
      "train loss:0.002678776891798825\n",
      "train loss:0.013106548688960133\n",
      "train loss:0.024508091871535586\n",
      "train loss:0.004495829773599557\n",
      "train loss:0.012342178021831235\n",
      "train loss:0.02575352144381158\n",
      "train loss:0.013992764797071185\n",
      "train loss:0.0046130203697384966\n",
      "train loss:0.01179389554866598\n",
      "train loss:0.002525450125590846\n",
      "train loss:0.005437971059652479\n",
      "train loss:0.015801969327420257\n",
      "train loss:0.002064299217965698\n",
      "train loss:0.01541440989087483\n",
      "train loss:0.007457787451317965\n",
      "train loss:0.021984639104537868\n",
      "train loss:0.009263385181742337\n",
      "train loss:0.0075040660449794796\n",
      "train loss:0.004316337681284793\n",
      "train loss:0.007650540300929337\n",
      "train loss:0.015376654886040548\n",
      "train loss:0.006593972675390533\n",
      "train loss:0.00980205437637853\n",
      "train loss:0.08610181763510896\n",
      "train loss:0.005124158785263583\n",
      "train loss:0.004883790762017015\n",
      "train loss:0.022279118818787892\n",
      "train loss:0.014721690493895757\n",
      "train loss:0.00830747599665959\n",
      "train loss:0.023288406435468353\n",
      "train loss:0.0011965113818404127\n",
      "train loss:0.00981789791352638\n",
      "train loss:0.00837207333149962\n",
      "train loss:0.0511596210253424\n",
      "train loss:0.005405954519151513\n",
      "train loss:0.00339747556272211\n",
      "train loss:0.004846463236332349\n",
      "train loss:0.0005887601470035688\n",
      "train loss:0.0038914757424954173\n",
      "train loss:0.02105825296592546\n",
      "train loss:0.0024283429664422834\n",
      "train loss:0.005850277933270432\n",
      "train loss:0.007081101905727165\n",
      "train loss:0.027440394777057363\n",
      "train loss:0.01473775365261797\n",
      "train loss:0.012319708862358423\n",
      "train loss:0.023958932032986976\n",
      "train loss:0.0021071061804759854\n",
      "train loss:0.003994015888870558\n",
      "train loss:0.011920128317539126\n",
      "train loss:0.0038948536246945897\n",
      "train loss:0.006120868163611018\n",
      "train loss:0.004031922049770521\n",
      "train loss:0.017693073868318164\n",
      "train loss:0.0021606179403052617\n",
      "train loss:0.0017527956479290855\n",
      "train loss:0.002359368943201066\n",
      "train loss:0.011276200571555843\n",
      "train loss:0.014734294868937901\n",
      "train loss:0.005191618897842354\n",
      "train loss:0.003641577181881126\n",
      "train loss:0.0009020995208328218\n",
      "train loss:0.003178016956699006\n",
      "train loss:0.018603839344316926\n",
      "train loss:0.008771797468938872\n",
      "train loss:0.0019638760935864643\n",
      "train loss:0.0019167355786744084\n",
      "train loss:0.04646343445119956\n",
      "train loss:0.004438550248051695\n",
      "train loss:0.03401910095102555\n",
      "train loss:0.020967660677204146\n",
      "train loss:0.07045288749515313\n",
      "train loss:0.002357174294021522\n",
      "train loss:0.004187492213956021\n",
      "train loss:0.00754531038898549\n",
      "train loss:0.007599272969368942\n",
      "train loss:0.006297330616413478\n",
      "train loss:0.023103912014686454\n",
      "train loss:0.01579882338250455\n",
      "train loss:0.005330028496692111\n",
      "train loss:0.025554314225673856\n",
      "train loss:0.006950285896427698\n",
      "train loss:0.0007231827224481597\n",
      "train loss:0.013430051264542688\n",
      "train loss:0.008449267724196923\n",
      "train loss:0.007928524524566884\n",
      "train loss:0.017307894846506876\n",
      "train loss:0.008397817963007611\n",
      "train loss:0.004027892150288833\n",
      "train loss:0.012627594659451156\n",
      "train loss:0.011884412762360108\n",
      "train loss:0.002955770788307181\n",
      "train loss:0.001020047069403631\n",
      "train loss:0.003604370838026463\n",
      "train loss:0.007398940636570102\n",
      "train loss:0.00859958934861714\n",
      "train loss:0.060006703549536286\n",
      "train loss:0.006121022715850804\n",
      "train loss:0.02031038705972502\n",
      "train loss:0.009144893198974215\n",
      "train loss:0.012925481473485552\n",
      "train loss:0.01457908962981414\n",
      "train loss:0.0009880835507663042\n",
      "train loss:0.016810790319439935\n",
      "train loss:0.0012604846023778293\n",
      "train loss:0.016352485607068392\n",
      "train loss:0.0014699811312056743\n",
      "train loss:0.004307426001288373\n",
      "train loss:0.016477649849747283\n",
      "train loss:0.05698913668020899\n",
      "train loss:0.005327499722995196\n",
      "train loss:0.004197606959300603\n",
      "train loss:0.16192253595299635\n",
      "train loss:0.016166278590023425\n",
      "=== epoch:10, train acc:0.993, test acc:0.987 ===\n",
      "train loss:0.014415726939040292\n",
      "train loss:0.00574636971858635\n",
      "train loss:0.004444829507310084\n",
      "train loss:0.013485223472052171\n",
      "train loss:0.002764463915444191\n",
      "train loss:0.004820438517081849\n",
      "train loss:0.0013584693098802878\n",
      "train loss:0.035981428870515336\n",
      "train loss:0.0037134347211367894\n",
      "train loss:0.005261808024177178\n",
      "train loss:0.011278631196720743\n",
      "train loss:0.005714241680394167\n",
      "train loss:0.005669868800103449\n",
      "train loss:0.016228718607688698\n",
      "train loss:0.005970010314867692\n",
      "train loss:0.014529400254127592\n",
      "train loss:0.00562949576481242\n",
      "train loss:0.010956859725852568\n",
      "train loss:0.002583195017582949\n",
      "train loss:0.009240400705866446\n",
      "train loss:0.00287366829770907\n",
      "train loss:0.0108341597533868\n",
      "train loss:0.02120170760229715\n",
      "train loss:0.005053242198325668\n",
      "train loss:0.02626550620478464\n",
      "train loss:0.0037088152782411847\n",
      "train loss:0.004858532565696309\n",
      "train loss:0.008228254731042873\n",
      "train loss:0.0035122159526717626\n",
      "train loss:0.029717112292032347\n",
      "train loss:0.010978376849051872\n",
      "train loss:0.005172582146643687\n",
      "train loss:0.003914112648246786\n",
      "train loss:0.0022379766190964295\n",
      "train loss:0.008214901722629685\n",
      "train loss:0.0075457436560653745\n",
      "train loss:0.007253684015392467\n",
      "train loss:0.0008693427731280518\n",
      "train loss:0.003157514352467799\n",
      "train loss:0.004910864742539841\n",
      "train loss:0.0034291058376292516\n",
      "train loss:0.003082966354668159\n",
      "train loss:0.0013990721356370247\n",
      "train loss:0.001633504316819321\n",
      "train loss:0.015438005890767666\n",
      "train loss:0.017917474841800438\n",
      "train loss:0.008917274144815546\n",
      "train loss:0.003176787199073241\n",
      "train loss:0.0048274805324915555\n",
      "train loss:0.029920383742708764\n",
      "train loss:0.0008559069765686797\n",
      "train loss:0.009496789653997119\n",
      "train loss:0.0069204567916676925\n",
      "train loss:0.0014740977644285703\n",
      "train loss:0.0004885028961635101\n",
      "train loss:0.0004190386242266669\n",
      "train loss:0.00781627253297692\n",
      "train loss:0.04713296242731044\n",
      "train loss:0.014019587113667686\n",
      "train loss:0.0012031299150946843\n",
      "train loss:0.0019637098436856974\n",
      "train loss:0.023968969703055926\n",
      "train loss:0.0027462033295285616\n",
      "train loss:0.00321036321931644\n",
      "train loss:0.0032968761868752096\n",
      "train loss:0.029934084908579815\n",
      "train loss:0.004016670093822138\n",
      "train loss:0.00690192701862361\n",
      "train loss:0.01074220838837077\n",
      "train loss:0.010060010213960031\n",
      "train loss:0.0268718250921587\n",
      "train loss:0.013048681314622595\n",
      "train loss:0.0044416722399688476\n",
      "train loss:0.0077256091729479845\n",
      "train loss:0.008317925588659215\n",
      "train loss:0.00927633085354863\n",
      "train loss:0.005197549432721425\n",
      "train loss:0.010575315052063952\n",
      "train loss:0.011452725546602937\n",
      "train loss:0.0029670285859776484\n",
      "train loss:0.004575125236701553\n",
      "train loss:0.0012696799543903674\n",
      "train loss:0.0008056404403198844\n",
      "train loss:0.01701122011636248\n",
      "train loss:0.0017380736793539268\n",
      "train loss:0.029496710706708847\n",
      "train loss:0.0024546472318703527\n",
      "train loss:0.015641980068675333\n",
      "train loss:0.015688782052508475\n",
      "train loss:0.01595831146183331\n",
      "train loss:0.00354932400450956\n",
      "train loss:0.0013031438099873316\n",
      "train loss:0.0026535279316822\n",
      "train loss:0.004590189901024149\n",
      "train loss:0.021281507550662576\n",
      "train loss:0.01625646960183668\n",
      "train loss:0.009230078243411441\n",
      "train loss:0.0032031059023600356\n",
      "train loss:0.002139871228158329\n",
      "train loss:0.011197093421599502\n",
      "train loss:0.0009496088037370881\n",
      "train loss:0.007089918550615249\n",
      "train loss:0.007157600116547107\n",
      "train loss:0.025487262916400005\n",
      "train loss:0.002837317839057299\n",
      "train loss:0.005557126704837385\n",
      "train loss:0.008262641628310892\n",
      "train loss:0.02043715857399337\n",
      "train loss:0.008179132280939196\n",
      "train loss:0.012964428988444859\n",
      "train loss:0.00415376042010627\n",
      "train loss:0.05734857491306014\n",
      "train loss:0.003238795121336687\n",
      "train loss:0.01277771313078669\n",
      "train loss:0.010429589698401378\n",
      "train loss:0.0010547114319685797\n",
      "train loss:0.006054406047072195\n",
      "train loss:0.006388608668263307\n",
      "train loss:0.015053180545793597\n",
      "train loss:0.005763219984143208\n",
      "train loss:0.0018556546909962108\n",
      "train loss:0.02716935511109718\n",
      "train loss:0.045063797718846325\n",
      "train loss:0.0014813306606071728\n",
      "train loss:0.004137004390177859\n",
      "train loss:0.013456239981222601\n",
      "train loss:0.005910791750556972\n",
      "train loss:0.010214850520462447\n",
      "train loss:0.017792422608122672\n",
      "train loss:0.012996388456956134\n",
      "train loss:0.0021008677618792825\n",
      "train loss:0.001721506377731219\n",
      "train loss:0.018456237004582498\n",
      "train loss:0.004661000434792862\n",
      "train loss:0.002720627585565391\n",
      "train loss:0.004512182997686744\n",
      "train loss:0.006019997731118253\n",
      "train loss:0.005522398967298627\n",
      "train loss:0.0023842353384515885\n",
      "train loss:0.004202107517218825\n",
      "train loss:0.0033092057425601136\n",
      "train loss:0.01994936073919323\n",
      "train loss:0.0016805012177012744\n",
      "train loss:0.002715136589573407\n",
      "train loss:0.0048315456013841825\n",
      "train loss:0.005568342175575178\n",
      "train loss:0.03929986085011781\n",
      "train loss:0.003928466908172765\n",
      "train loss:0.005062110635398389\n",
      "train loss:0.03663387616214396\n",
      "train loss:0.00048603681392884757\n",
      "train loss:0.0031875644059845467\n",
      "train loss:0.00507348330772944\n",
      "train loss:0.009166738664489076\n",
      "train loss:0.016711987862102044\n",
      "train loss:0.014162233605027668\n",
      "train loss:0.004780873950710902\n",
      "train loss:0.005269025159318763\n",
      "train loss:0.015296700274330537\n",
      "train loss:0.0022456771804550578\n",
      "train loss:0.009968548908086907\n",
      "train loss:0.002406750494044059\n",
      "train loss:0.002167144124002389\n",
      "train loss:0.011905683285931785\n",
      "train loss:0.0010443839254513448\n",
      "train loss:0.00207651839091734\n",
      "train loss:0.010047047567625582\n",
      "train loss:0.008562838449070896\n",
      "train loss:0.006661949081692989\n",
      "train loss:0.004483838395220691\n",
      "train loss:0.010896674248175859\n",
      "train loss:0.0034272098220311533\n",
      "train loss:0.0017560403478883682\n",
      "train loss:0.01066776159331654\n",
      "train loss:0.006490264474254822\n",
      "train loss:0.003793180358829551\n",
      "train loss:0.001100502756897954\n",
      "train loss:0.001215654061158804\n",
      "train loss:0.01480201283958003\n",
      "train loss:0.0007334397571513617\n",
      "train loss:0.01208656211133817\n",
      "train loss:0.0009503139663142046\n",
      "train loss:0.0017530353586249234\n",
      "train loss:0.014745096103851605\n",
      "train loss:0.006925677502991281\n",
      "train loss:0.004767971657804304\n",
      "train loss:0.04413609883658364\n",
      "train loss:0.0041377778669001416\n",
      "train loss:0.020071569400688164\n",
      "train loss:0.008087800682272378\n",
      "train loss:0.001902977446012041\n",
      "train loss:0.01864610676904109\n",
      "train loss:0.0026510431953299662\n",
      "train loss:0.0007911988140381628\n",
      "train loss:0.0046367345677524475\n",
      "train loss:0.0054488912371384425\n",
      "train loss:0.002950806451136129\n",
      "train loss:0.010311126425469259\n",
      "train loss:0.0019204649637342734\n",
      "train loss:0.007172545775898168\n",
      "train loss:0.0039310500705814445\n",
      "train loss:0.021758790109258096\n",
      "train loss:0.0546478637335462\n",
      "train loss:0.005979267084503787\n",
      "train loss:0.002147984971528669\n",
      "train loss:0.0032638222079250673\n",
      "train loss:0.0026601467911461347\n",
      "train loss:0.00689584723891019\n",
      "train loss:0.0055038457472588705\n",
      "train loss:0.00909034911486356\n",
      "train loss:0.048125414302178704\n",
      "train loss:0.0021805614896782664\n",
      "train loss:0.012598182211803182\n",
      "train loss:0.004492641693631448\n",
      "train loss:0.0036266340537081253\n",
      "train loss:0.00630399300044438\n",
      "train loss:0.015879887685184915\n",
      "train loss:0.007740453213591203\n",
      "train loss:0.0023207512011453654\n",
      "train loss:0.007589186781985609\n",
      "train loss:0.001451486117820301\n",
      "train loss:0.0026100360269361213\n",
      "train loss:0.0018156855573419036\n",
      "train loss:0.0020304494344877246\n",
      "train loss:0.0050900908776053445\n",
      "train loss:0.0005666873240774495\n",
      "train loss:0.001901502681285455\n",
      "train loss:0.0018382453662780377\n",
      "train loss:0.002073605145708003\n",
      "train loss:0.03777230292172617\n",
      "train loss:0.012447297212244237\n",
      "train loss:0.001818842865924177\n",
      "train loss:0.0027271302596185744\n",
      "train loss:0.0024803681823627527\n",
      "train loss:0.0014236026916358713\n",
      "train loss:0.0027647991389084057\n",
      "train loss:0.01200226804299796\n",
      "train loss:0.0022302211096273646\n",
      "train loss:0.0011116525553675034\n",
      "train loss:0.008724408783538582\n",
      "train loss:0.010667165750294754\n",
      "train loss:0.018515135775986746\n",
      "train loss:0.001471367759402511\n",
      "train loss:0.023445153714422504\n",
      "train loss:0.007473898452657398\n",
      "train loss:0.000783489768262831\n",
      "train loss:0.0028794543038158484\n",
      "train loss:0.010140282215862917\n",
      "train loss:0.008757295992282907\n",
      "train loss:0.0030361866677844824\n",
      "train loss:0.005025705366989371\n",
      "train loss:0.0020820618748848144\n",
      "train loss:0.004902434714519902\n",
      "train loss:0.00635425160601437\n",
      "train loss:0.011704944447839356\n",
      "train loss:0.012935257901314863\n",
      "train loss:0.002792625687289256\n",
      "train loss:0.003923481765417977\n",
      "train loss:0.002221157136036817\n",
      "train loss:0.017463645277110272\n",
      "train loss:0.005699089123941872\n",
      "train loss:0.0018023008011479364\n",
      "train loss:0.002284231373322342\n",
      "train loss:0.005516457501868913\n",
      "train loss:0.0034006637853442904\n",
      "train loss:0.010899488256744702\n",
      "train loss:0.009366794756332638\n",
      "train loss:0.0059300151446881125\n",
      "train loss:0.022457924187335658\n",
      "train loss:0.0020555751856601696\n",
      "train loss:0.00041039834040132325\n",
      "train loss:0.014804076799291384\n",
      "train loss:0.008535989540157293\n",
      "train loss:0.003509395670768822\n",
      "train loss:0.0014809411944376946\n",
      "train loss:0.00704179447780488\n",
      "train loss:0.008099009275494251\n",
      "train loss:0.0058199377132536765\n",
      "train loss:0.003045213434839106\n",
      "train loss:0.007770300036699453\n",
      "train loss:0.0025811209944813906\n",
      "train loss:0.010661568282352063\n",
      "train loss:0.009302475131767426\n",
      "train loss:0.010083493304473312\n",
      "train loss:0.010574386893174026\n",
      "train loss:0.013821176583112047\n",
      "train loss:0.007887751924966038\n",
      "train loss:0.002087223984898728\n",
      "train loss:0.00376280474537404\n",
      "train loss:0.003367951715558394\n",
      "train loss:0.011420218217083396\n",
      "train loss:0.025650972580064\n",
      "train loss:0.0002664895353597844\n",
      "train loss:0.0018229220829560514\n",
      "train loss:0.0016115201547779387\n",
      "train loss:0.03585932157821769\n",
      "train loss:0.0027537888708693437\n",
      "train loss:0.008899033879018418\n",
      "train loss:0.0026143980791358294\n",
      "train loss:0.0008361849425852928\n",
      "train loss:0.0017038802330245843\n",
      "train loss:0.0014746188420340115\n",
      "train loss:0.007073812606863674\n",
      "train loss:0.0010065032338484707\n",
      "train loss:0.007913993321828197\n",
      "train loss:0.008148102379333952\n",
      "train loss:0.0003682145764375764\n",
      "train loss:0.004216811879470433\n",
      "train loss:0.004077778105037204\n",
      "train loss:0.0009798998322327688\n",
      "train loss:0.003450453714722167\n",
      "train loss:0.02056910061724707\n",
      "train loss:0.011319973555157873\n",
      "train loss:0.0020800724398998037\n",
      "train loss:0.008515422745604337\n",
      "train loss:0.0025134693570281652\n",
      "train loss:0.005148934007810976\n",
      "train loss:0.008240024612548396\n",
      "train loss:0.006241885431118469\n",
      "train loss:0.004769691601554617\n",
      "train loss:0.001811569114572557\n",
      "train loss:0.0031922695840241335\n",
      "train loss:0.0012313585121939094\n",
      "train loss:0.006145037772830392\n",
      "train loss:0.00451625888300603\n",
      "train loss:0.0011672753964847983\n",
      "train loss:0.0024256928536749646\n",
      "train loss:0.004259041244764036\n",
      "train loss:0.005616052454490389\n",
      "train loss:0.016148728187483183\n",
      "train loss:0.001300435397267071\n",
      "train loss:0.005737742115898452\n",
      "train loss:0.0045649159237063745\n",
      "train loss:0.005771073100473105\n",
      "train loss:0.003465993733625318\n",
      "train loss:0.012557572661850296\n",
      "train loss:0.003041907431819482\n",
      "train loss:0.02404830113651421\n",
      "train loss:0.0008814753231805609\n",
      "train loss:0.009081225761826999\n",
      "train loss:0.007688236916469678\n",
      "train loss:0.019171053513837747\n",
      "train loss:0.005612755593407701\n",
      "train loss:0.01121736749351439\n",
      "train loss:0.014752785149317365\n",
      "train loss:0.016949728433495282\n",
      "train loss:0.029406693929714765\n",
      "train loss:0.0029123972143869485\n",
      "train loss:0.014102018589320262\n",
      "train loss:0.015113275874888858\n",
      "train loss:0.007824792402482992\n",
      "train loss:0.0036508886622852897\n",
      "train loss:0.007402928460820372\n",
      "train loss:0.004788261826930296\n",
      "train loss:0.009426862645008548\n",
      "train loss:0.003948237539884993\n",
      "train loss:0.0009109984216717818\n",
      "train loss:0.039792551303774557\n",
      "train loss:0.005225396664097604\n",
      "train loss:0.006175626489892486\n",
      "train loss:0.0009173396027843493\n",
      "train loss:0.003772096989433718\n",
      "train loss:0.0036802880777538805\n",
      "train loss:0.00518071043679917\n",
      "train loss:0.0002193754498472145\n",
      "train loss:0.010973042476536925\n",
      "train loss:0.007979550396145112\n",
      "train loss:0.006077853505924027\n",
      "train loss:0.003077820527375108\n",
      "train loss:0.0026441708623303082\n",
      "train loss:0.008920943033339051\n",
      "train loss:0.003972680883192111\n",
      "train loss:0.0018470575048883276\n",
      "train loss:0.001997962978122476\n",
      "train loss:0.007328321085113768\n",
      "train loss:0.0039023274554876902\n",
      "train loss:0.00046895105317464264\n",
      "train loss:0.02524489266369341\n",
      "train loss:0.002298665525491576\n",
      "train loss:0.03719356915179292\n",
      "train loss:0.016096820219153044\n",
      "train loss:0.007622826459373944\n",
      "train loss:0.020494418124698037\n",
      "train loss:0.004476827654709055\n",
      "train loss:0.002943816124251698\n",
      "train loss:0.009487224643644243\n",
      "train loss:0.0014863168603600474\n",
      "train loss:0.0030746061425824473\n",
      "train loss:0.015401093412495197\n",
      "train loss:0.006544806284656196\n",
      "train loss:0.007055118741853499\n",
      "train loss:0.00845608642843635\n",
      "train loss:0.012464098352699588\n",
      "train loss:0.005356616711256274\n",
      "train loss:0.005242732897865043\n",
      "train loss:0.0022157664276882664\n",
      "train loss:0.0029427932131987065\n",
      "train loss:0.013446348196788176\n",
      "train loss:0.008808041528091186\n",
      "train loss:0.0010178815138320563\n",
      "train loss:0.00415414909553489\n",
      "train loss:0.0020510816585037252\n",
      "train loss:0.1606302978775402\n",
      "train loss:0.0007734539528119412\n",
      "train loss:0.0007275394447321798\n",
      "train loss:0.002117446340734818\n",
      "train loss:0.03316694953949385\n",
      "train loss:0.007050127427111941\n",
      "train loss:0.00743064832202016\n",
      "train loss:0.004352665713166104\n",
      "train loss:0.018764915447414265\n",
      "train loss:0.002679647916042251\n",
      "train loss:0.003313412844067146\n",
      "train loss:0.007758131682001297\n",
      "train loss:0.032497466909691936\n",
      "train loss:0.013226270676565274\n",
      "train loss:0.0006734519368983606\n",
      "train loss:0.00160478800281146\n",
      "train loss:0.0029499031593110555\n",
      "train loss:0.0016659533136929078\n",
      "train loss:0.018635590274415442\n",
      "train loss:0.007656526315278567\n",
      "train loss:0.009087404447918184\n",
      "train loss:0.0077376586381766995\n",
      "train loss:0.004898607032503831\n",
      "train loss:0.0017230968841692534\n",
      "train loss:0.005507931139463291\n",
      "train loss:0.004514206175004234\n",
      "train loss:0.009960795995275593\n",
      "train loss:0.00046513444372129107\n",
      "train loss:0.007494358135381634\n",
      "train loss:0.005711456101976728\n",
      "train loss:0.016078322085528163\n",
      "train loss:0.002796027884058926\n",
      "train loss:0.0015960593027795056\n",
      "train loss:0.0347286009737227\n",
      "train loss:0.003055529256433876\n",
      "train loss:0.0014732882072783387\n",
      "train loss:0.0044655060711963806\n",
      "train loss:0.012121202727449494\n",
      "train loss:0.007204746172945888\n",
      "train loss:0.003113504428622889\n",
      "train loss:0.010965768665448947\n",
      "train loss:0.0018412195756332356\n",
      "train loss:0.004921568794929451\n",
      "train loss:0.008542827938864518\n",
      "train loss:0.0011337235569069345\n",
      "train loss:0.005230183233173278\n",
      "train loss:0.004382492875478689\n",
      "train loss:0.0049441379203085676\n",
      "train loss:0.005127164721849456\n",
      "train loss:0.00425604497138953\n",
      "train loss:0.0009289795804612347\n",
      "train loss:0.017455098542081823\n",
      "train loss:0.026645162207856083\n",
      "train loss:0.016305094549331135\n",
      "train loss:0.003397145997045892\n",
      "train loss:0.007452018510107925\n",
      "train loss:0.003192458483145336\n",
      "train loss:0.00415190633553412\n",
      "train loss:0.003000648056854654\n",
      "train loss:0.01824885167295924\n",
      "train loss:0.011854650773302677\n",
      "train loss:0.0004171057068854802\n",
      "train loss:0.0073302652802657575\n",
      "train loss:0.006480713265124211\n",
      "train loss:0.06594097852529533\n",
      "train loss:0.0014815705397686516\n",
      "train loss:0.0008503975294711727\n",
      "train loss:0.0060039062620857704\n",
      "train loss:0.0016005268188160142\n",
      "train loss:0.00473746797558326\n",
      "train loss:0.013153426775464714\n",
      "train loss:0.003682128872066022\n",
      "train loss:0.032559163826152586\n",
      "train loss:0.017254260416446468\n",
      "train loss:0.0026144316123872676\n",
      "train loss:0.003673146623703131\n",
      "train loss:0.003475231388766131\n",
      "train loss:0.005343935295663083\n",
      "train loss:0.006274210282109319\n",
      "train loss:0.007753590254757304\n",
      "train loss:0.007756085993269011\n",
      "train loss:0.016216458388558064\n",
      "train loss:0.009182033358273837\n",
      "train loss:0.016823930085377126\n",
      "train loss:0.013732552677093382\n",
      "train loss:0.0005001991848645352\n",
      "train loss:0.001938739476815648\n",
      "train loss:0.00543714080919795\n",
      "train loss:0.003073324430142741\n",
      "train loss:0.01861051568466495\n",
      "train loss:0.013851454005785704\n",
      "train loss:0.007889565450029596\n",
      "train loss:0.004906725353867586\n",
      "train loss:0.004777755979938469\n",
      "train loss:0.0033829676304701754\n",
      "train loss:0.004609919507744419\n",
      "train loss:0.0549119483343898\n",
      "train loss:0.0028522232502440864\n",
      "train loss:0.011279778819587728\n",
      "train loss:0.0033996815291860716\n",
      "train loss:0.005949945895122235\n",
      "train loss:0.011396847904037893\n",
      "train loss:0.0031627678822659273\n",
      "train loss:0.0010039097902691868\n",
      "train loss:0.0036267155432780475\n",
      "train loss:0.0069850283994915795\n",
      "train loss:0.004317401116884745\n",
      "train loss:0.0018249860177487584\n",
      "train loss:0.0027506460576072075\n",
      "train loss:0.015480542675521586\n",
      "train loss:0.0010645515296974326\n",
      "train loss:0.013839361025060672\n",
      "train loss:0.0011776595902835633\n",
      "train loss:0.0007587529417199541\n",
      "train loss:0.003095271297527903\n",
      "train loss:0.0070989612094893816\n",
      "train loss:0.004567255487699258\n",
      "train loss:0.0030504572113291576\n",
      "train loss:0.0072985929343279375\n",
      "train loss:0.009639510924041193\n",
      "train loss:0.0033700571142278356\n",
      "train loss:0.0004996764278525446\n",
      "train loss:0.01386869245334188\n",
      "train loss:0.015042353409179754\n",
      "train loss:0.0030411105830543167\n",
      "train loss:0.007246604647729892\n",
      "train loss:0.007074169470491126\n",
      "train loss:0.016157736118911442\n",
      "train loss:0.00041938345509215353\n",
      "train loss:0.004861494946038918\n",
      "train loss:0.03290560309304984\n",
      "train loss:0.015854777220157622\n",
      "train loss:0.0046759198452412725\n",
      "train loss:0.0027927390696771957\n",
      "train loss:0.012149115753270451\n",
      "train loss:0.013313822726529326\n",
      "train loss:0.0006355179866036098\n",
      "train loss:0.000731355146422046\n",
      "train loss:0.02413056852972302\n",
      "train loss:0.006095999028169073\n",
      "train loss:0.0038881493212619693\n",
      "train loss:0.03598356626005969\n",
      "train loss:0.0016285405762443389\n",
      "train loss:0.010915732424938682\n",
      "train loss:0.004161263785370383\n",
      "train loss:0.0040601417389373225\n",
      "train loss:0.019369923768086602\n",
      "train loss:0.004696011448137702\n",
      "train loss:0.0169205879304789\n",
      "train loss:0.008592498083437658\n",
      "train loss:0.017605628584831473\n",
      "train loss:0.00896130106992612\n",
      "train loss:0.004921111065750471\n",
      "train loss:0.0071467661325435895\n",
      "train loss:0.0034458899386747366\n",
      "train loss:0.008828161564550204\n",
      "train loss:0.011078840325357841\n",
      "train loss:0.0011471803456088098\n",
      "train loss:0.0024274647647833675\n",
      "train loss:0.004785916139777804\n",
      "train loss:0.003982105466108844\n",
      "train loss:0.004271075234436423\n",
      "train loss:0.06476577355834849\n",
      "train loss:0.0019174076372820387\n",
      "train loss:0.007713008322079562\n",
      "train loss:0.009985383648097423\n",
      "train loss:0.020294720052758626\n",
      "train loss:0.0025999637342136762\n",
      "train loss:0.013665521938686476\n",
      "train loss:0.010856752910189133\n",
      "train loss:0.0034811382948936615\n",
      "train loss:0.008735435774826683\n",
      "train loss:0.06525779802334931\n",
      "train loss:0.011799040495931017\n",
      "train loss:0.008661215171632397\n",
      "train loss:0.0038791323956757634\n",
      "train loss:0.005856683246254674\n",
      "train loss:0.061759597172017754\n",
      "train loss:0.003509627156275842\n",
      "train loss:0.0022485924453735312\n",
      "train loss:0.021413960220381018\n",
      "train loss:0.008875460360933837\n",
      "train loss:0.010698810222852281\n",
      "train loss:0.01861703675470329\n",
      "train loss:0.0025016194516413156\n",
      "train loss:0.001903925628422701\n",
      "train loss:0.0046322428071202655\n",
      "train loss:0.010258575260220006\n",
      "train loss:0.016801434250521215\n",
      "train loss:0.0029721963123446648\n",
      "train loss:0.0009736779006744793\n",
      "train loss:0.0028427470254606714\n",
      "train loss:0.007451415106574196\n",
      "train loss:0.01721237067579771\n",
      "train loss:0.009218202768250049\n",
      "train loss:0.0013893731342450456\n",
      "train loss:0.0014484960410190712\n",
      "train loss:0.0014065030320762953\n",
      "=== epoch:11, train acc:0.997, test acc:0.984 ===\n",
      "train loss:0.001232349148721051\n",
      "train loss:0.012822611580353922\n",
      "train loss:0.033433287343366834\n",
      "train loss:0.0052771071519587594\n",
      "train loss:0.00044618000401869777\n",
      "train loss:0.01829949963551316\n",
      "train loss:0.0008884940619522742\n",
      "train loss:0.002233528184200271\n",
      "train loss:0.003987708301446451\n",
      "train loss:0.005124654427482758\n",
      "train loss:0.0007765318259736542\n",
      "train loss:0.0009869734729132497\n",
      "train loss:0.022191690420651854\n",
      "train loss:0.0011660721889777544\n",
      "train loss:0.011657408480713448\n",
      "train loss:0.0031057280307920025\n",
      "train loss:0.03369636106213232\n",
      "train loss:0.0059316642406896\n",
      "train loss:0.005523060050565165\n",
      "train loss:0.004042691040810524\n",
      "train loss:0.0011581475627935032\n",
      "train loss:0.0032528191570403682\n",
      "train loss:0.011735498343228684\n",
      "train loss:0.02496402720655709\n",
      "train loss:0.0028806048276680683\n",
      "train loss:0.0033776189951334056\n",
      "train loss:0.03351673262032778\n",
      "train loss:0.002084306637007104\n",
      "train loss:0.012348402919723167\n",
      "train loss:0.0014350928737487314\n",
      "train loss:0.0028183809066978233\n",
      "train loss:0.003747655508784634\n",
      "train loss:0.0012473347001673957\n",
      "train loss:0.009680452012065782\n",
      "train loss:0.0049676250285777485\n",
      "train loss:0.0012350784364103446\n",
      "train loss:0.004346030206428383\n",
      "train loss:0.0028131108351925073\n",
      "train loss:0.0076227187553984796\n",
      "train loss:0.044734066688307685\n",
      "train loss:0.010400983588587341\n",
      "train loss:0.003182114670556199\n",
      "train loss:0.009557846430488692\n",
      "train loss:0.010540186483951148\n",
      "train loss:0.004181965361066137\n",
      "train loss:0.0012271475753673896\n",
      "train loss:0.005743516567350758\n",
      "train loss:0.004709093645629059\n",
      "train loss:0.005144831933216758\n",
      "train loss:0.004385942549883259\n",
      "train loss:0.01130669573806622\n",
      "train loss:0.002493674955724442\n",
      "train loss:0.021968561209051888\n",
      "train loss:0.005077037549476799\n",
      "train loss:0.004801200817681011\n",
      "train loss:0.0031355320587395335\n",
      "train loss:0.0074357772771607425\n",
      "train loss:0.043229409799861146\n",
      "train loss:0.011894955962109193\n",
      "train loss:0.0004788153183312787\n",
      "train loss:0.0011856968542659008\n",
      "train loss:0.004486611439860598\n",
      "train loss:0.006482853026487862\n",
      "train loss:0.0019946148606983277\n",
      "train loss:0.009592235521114666\n",
      "train loss:0.0015819571672485472\n",
      "train loss:0.04348484071602922\n",
      "train loss:0.009579100857956253\n",
      "train loss:0.0010230186844914593\n",
      "train loss:0.0066958193391287975\n",
      "train loss:0.0073866305968703205\n",
      "train loss:0.0030236707556189346\n",
      "train loss:0.009267571734987164\n",
      "train loss:0.006072424425022929\n",
      "train loss:0.0034799080941147896\n",
      "train loss:0.005490143144592945\n",
      "train loss:0.0030137759772626742\n",
      "train loss:0.010015832057970204\n",
      "train loss:0.0060412619415202975\n",
      "train loss:0.0016962320869119162\n",
      "train loss:0.017355695727255784\n",
      "train loss:0.011076444526458663\n",
      "train loss:0.003076878528923488\n",
      "train loss:0.006395593456201519\n",
      "train loss:0.01108452323098712\n",
      "train loss:0.0004897056453291389\n",
      "train loss:0.0007135386145005802\n",
      "train loss:0.02798113921571501\n",
      "train loss:0.0052819715857319715\n",
      "train loss:0.0052238401197933345\n",
      "train loss:0.011200537139581827\n",
      "train loss:0.00248019838815291\n",
      "train loss:0.0019230067703325394\n",
      "train loss:0.004272061269490119\n",
      "train loss:0.018138944756961844\n",
      "train loss:0.00907545182715863\n",
      "train loss:0.0069388899215476665\n",
      "train loss:0.0040045064851491315\n",
      "train loss:0.03503447073910101\n",
      "train loss:0.006068233264655806\n",
      "train loss:0.006806910969154903\n",
      "train loss:0.032472284103034245\n",
      "train loss:0.006596803518868762\n",
      "train loss:0.005740764430314136\n",
      "train loss:0.02214429813733841\n",
      "train loss:0.0037689133964477588\n",
      "train loss:0.0006061156278532451\n",
      "train loss:0.016551815775963038\n",
      "train loss:0.005169092313256641\n",
      "train loss:0.007278904686461467\n",
      "train loss:0.000988909513363588\n",
      "train loss:0.09718912199274295\n",
      "train loss:0.008921588842817116\n",
      "train loss:0.012247137316427221\n",
      "train loss:0.013438440830224198\n",
      "train loss:0.0027088584626370786\n",
      "train loss:0.04478096630991228\n",
      "train loss:0.004994214321127854\n",
      "train loss:0.009455062590269424\n",
      "train loss:0.001153062530378775\n",
      "train loss:0.01494268262185234\n",
      "train loss:0.0017177999717351946\n",
      "train loss:0.007284602424856817\n",
      "train loss:0.01695347295332511\n",
      "train loss:0.03068546519398219\n",
      "train loss:0.022144145453061856\n",
      "train loss:0.01995145106374762\n",
      "train loss:0.0027527573203941984\n",
      "train loss:0.01149564913310859\n",
      "train loss:0.0016947519211701347\n",
      "train loss:0.006700565481102403\n",
      "train loss:0.002590878015149693\n",
      "train loss:0.003713774118486664\n",
      "train loss:0.0039518175461741535\n",
      "train loss:0.007297449939329725\n",
      "train loss:0.004161851482455869\n",
      "train loss:0.001035794188592517\n",
      "train loss:0.001897391033221027\n",
      "train loss:0.00779271122851287\n",
      "train loss:0.00929782366886381\n",
      "train loss:0.00279751023352045\n",
      "train loss:0.005744899333208455\n",
      "train loss:0.011622975203316961\n",
      "train loss:0.04049160036549421\n",
      "train loss:0.00681096191616449\n",
      "train loss:0.004031321787422637\n",
      "train loss:0.0050364556309212705\n",
      "train loss:0.005792687019578097\n",
      "train loss:0.003760459651950504\n",
      "train loss:0.013007080251661246\n",
      "train loss:0.005366737364190296\n",
      "train loss:0.010925749317338744\n",
      "train loss:0.01053677889467486\n",
      "train loss:0.0049717992882962695\n",
      "train loss:0.0023686435848961685\n",
      "train loss:0.0017528305795833396\n",
      "train loss:0.0006321222241706449\n",
      "train loss:0.012826561611787366\n",
      "train loss:0.0015903251432227478\n",
      "train loss:0.0038967517322765033\n",
      "train loss:0.01132882003090929\n",
      "train loss:0.003976735238008669\n",
      "train loss:0.0009000754375990099\n",
      "train loss:0.0016529014200426953\n",
      "train loss:0.009322034077517808\n",
      "train loss:0.0034793993668907103\n",
      "train loss:0.004369331473998048\n",
      "train loss:0.0014650943471368441\n",
      "train loss:0.010488411524997713\n",
      "train loss:0.0028843210561496917\n",
      "train loss:0.003706671929062112\n",
      "train loss:0.0009403029749213832\n",
      "train loss:0.004244383961341647\n",
      "train loss:0.00454785267853757\n",
      "train loss:0.007926564448545077\n",
      "train loss:0.015110226219896581\n",
      "train loss:0.007964281504175889\n",
      "train loss:0.0016405917327868738\n",
      "train loss:0.003531635033652884\n",
      "train loss:0.00045578300788313704\n",
      "train loss:0.003129056726510199\n",
      "train loss:0.023053705653874136\n",
      "train loss:0.0009017951616367554\n",
      "train loss:0.001983510232203499\n",
      "train loss:0.001046836764982706\n",
      "train loss:0.009084322448736425\n",
      "train loss:0.0042432054688426556\n",
      "train loss:0.0010256543252841954\n",
      "train loss:0.005201389862851137\n",
      "train loss:0.0060239840634601005\n",
      "train loss:0.01229032949074314\n",
      "train loss:0.0023233272422469374\n",
      "train loss:0.008888156263375656\n",
      "train loss:0.001302447703557151\n",
      "train loss:0.0016855793095231623\n",
      "train loss:0.005659400580873181\n",
      "train loss:0.005971313642343914\n",
      "train loss:0.004178700657763201\n",
      "train loss:0.014871128202913763\n",
      "train loss:0.005090487789873108\n",
      "train loss:0.04553859778163141\n",
      "train loss:0.003266664081084647\n",
      "train loss:0.028117274870744457\n",
      "train loss:0.0018819972050879482\n",
      "train loss:0.004849466539331805\n",
      "train loss:0.0016095755529375228\n",
      "train loss:0.015056175443345745\n",
      "train loss:0.017838069999259232\n",
      "train loss:0.006844563235041067\n",
      "train loss:0.002913157038933442\n",
      "train loss:0.0015499517293373357\n",
      "train loss:0.025696386205508383\n",
      "train loss:0.0033169438552193135\n",
      "train loss:0.0015103240759719602\n",
      "train loss:0.006426958242232344\n",
      "train loss:0.003808091475447683\n",
      "train loss:0.00674192807801151\n",
      "train loss:0.0013989874989125274\n",
      "train loss:0.011724444709772361\n",
      "train loss:0.0016504345604248776\n",
      "train loss:0.021735918928546257\n",
      "train loss:0.008340494725321283\n",
      "train loss:0.00826878531760864\n",
      "train loss:0.07879279443951465\n",
      "train loss:0.0047780038227729325\n",
      "train loss:0.009719433082500397\n",
      "train loss:0.01596656640172841\n",
      "train loss:0.022823418983523122\n",
      "train loss:0.014679527557665899\n",
      "train loss:0.005341302425759137\n",
      "train loss:0.012204043679852188\n",
      "train loss:0.011070025941257122\n",
      "train loss:0.022196894236281624\n",
      "train loss:0.006578636732679931\n",
      "train loss:0.0023899034331431234\n",
      "train loss:0.004896929608789103\n",
      "train loss:0.001667385506596742\n",
      "train loss:0.007182800694542289\n",
      "train loss:0.006759474964611857\n",
      "train loss:0.00761280158005944\n",
      "train loss:0.0815250876788285\n",
      "train loss:0.002816722487734869\n",
      "train loss:0.0021221250079562957\n",
      "train loss:0.0009282371877149184\n",
      "train loss:0.0012709009431367704\n",
      "train loss:0.002266224895556316\n",
      "train loss:0.011236485074281885\n",
      "train loss:0.002620857656169218\n",
      "train loss:0.007362958092678942\n",
      "train loss:0.009774332081941376\n",
      "train loss:0.0021983550775501537\n",
      "train loss:0.0034987274147676266\n",
      "train loss:0.0019419980538533791\n",
      "train loss:0.01703886127223302\n",
      "train loss:0.003393847222702126\n",
      "train loss:0.015972764630356476\n",
      "train loss:0.012779791222042883\n",
      "train loss:0.006915234643019\n",
      "train loss:0.00827976641805008\n",
      "train loss:0.006186860430987702\n",
      "train loss:0.0007699986698602063\n",
      "train loss:0.004415262351425505\n",
      "train loss:0.02045682011616014\n",
      "train loss:0.0038109218008568696\n",
      "train loss:0.003833289895115077\n",
      "train loss:0.0035724647384504913\n",
      "train loss:0.0015025845814585271\n",
      "train loss:0.0014723341283680762\n",
      "train loss:0.0009449450171718237\n",
      "train loss:0.010343870910507601\n",
      "train loss:0.003196409513901219\n",
      "train loss:0.005248272623889649\n",
      "train loss:0.009441189201867468\n",
      "train loss:0.012935184492851429\n",
      "train loss:0.009586076254193509\n",
      "train loss:0.01885795726952154\n",
      "train loss:0.032029939230803446\n",
      "train loss:0.006941234823856185\n",
      "train loss:0.0018540147198736838\n",
      "train loss:0.00913991303252912\n",
      "train loss:0.010904125970850522\n",
      "train loss:0.009336247922710051\n",
      "train loss:0.0026443326138312724\n",
      "train loss:0.008154746957140016\n",
      "train loss:0.07351810672294458\n",
      "train loss:0.015486394896006865\n",
      "train loss:0.0014300150256840915\n",
      "train loss:0.009543690988630059\n",
      "train loss:0.004624257353322291\n",
      "train loss:0.00034421932560490067\n",
      "train loss:0.014631509905120427\n",
      "train loss:0.0031801099854246056\n",
      "train loss:0.0027576111236020887\n",
      "train loss:0.007319504580450798\n",
      "train loss:0.006653852226427294\n",
      "train loss:0.012791237122642421\n",
      "train loss:0.0022363685280389995\n",
      "train loss:0.008414259987779903\n",
      "train loss:0.00143030815672761\n",
      "train loss:0.002696220273522361\n",
      "train loss:0.0035831926731222387\n",
      "train loss:0.015034700483157035\n",
      "train loss:0.003124837436728724\n",
      "train loss:0.0038688142672372193\n",
      "train loss:0.0010185541421257052\n",
      "train loss:0.023045519586726668\n",
      "train loss:0.006417322164540507\n",
      "train loss:0.001506046341770213\n",
      "train loss:0.002941864084720648\n",
      "train loss:0.0006692325590852412\n",
      "train loss:0.00367566551725307\n",
      "train loss:0.006250946743124475\n",
      "train loss:0.0007547281872341506\n",
      "train loss:0.09801917215367498\n",
      "train loss:0.005109210774032016\n",
      "train loss:0.012593603901337493\n",
      "train loss:0.005099962201852615\n",
      "train loss:0.001433782230973575\n",
      "train loss:0.0058024159387146034\n",
      "train loss:0.009631910832080912\n",
      "train loss:0.005308364119135406\n",
      "train loss:0.0011689478139172002\n",
      "train loss:0.0026224972811357172\n",
      "train loss:0.007504047589053608\n",
      "train loss:0.006631371803852492\n",
      "train loss:0.0018083635844571976\n",
      "train loss:0.0121855185341962\n",
      "train loss:0.008107601824235661\n",
      "train loss:0.007581266668519521\n",
      "train loss:0.006457118355530751\n",
      "train loss:0.005454426445673271\n",
      "train loss:0.0062085235455606866\n",
      "train loss:0.005061878003906771\n",
      "train loss:0.03520903013513463\n",
      "train loss:0.004682405975129777\n",
      "train loss:0.003095046917914285\n",
      "train loss:0.0015449573364877483\n",
      "train loss:0.01668459481227286\n",
      "train loss:0.0060024342437772285\n",
      "train loss:0.002234662334288363\n",
      "train loss:0.001655126557163198\n",
      "train loss:0.004614833407495474\n",
      "train loss:0.005337618309043763\n",
      "train loss:0.002141704948422961\n",
      "train loss:0.009010388179645064\n",
      "train loss:0.01116952597954447\n",
      "train loss:0.000624796496381672\n",
      "train loss:0.003295676841531781\n",
      "train loss:0.002034601724948393\n",
      "train loss:0.0014268854520247856\n",
      "train loss:0.005346234217836151\n",
      "train loss:0.0009871075525819983\n",
      "train loss:0.006689632784689299\n",
      "train loss:0.0017646748211178115\n",
      "train loss:0.012755398273254866\n",
      "train loss:0.009062956753638142\n",
      "train loss:0.006752346505193803\n",
      "train loss:0.0007348748799074158\n",
      "train loss:0.005914735015049327\n",
      "train loss:0.005456060309182318\n",
      "train loss:0.015559029441246701\n",
      "train loss:0.0014945666472805294\n",
      "train loss:0.0031193932004172876\n",
      "train loss:0.012362420141847663\n",
      "train loss:0.0025899193964005677\n",
      "train loss:0.005309013562775887\n",
      "train loss:0.004806087025996373\n",
      "train loss:0.0004756276847934098\n",
      "train loss:0.0008570979318883825\n",
      "train loss:0.010370051209654431\n",
      "train loss:0.0012767735131165095\n",
      "train loss:0.0028078482271248293\n",
      "train loss:0.0013648892342736961\n",
      "train loss:0.012571993695148286\n",
      "train loss:0.00033984098618589987\n",
      "train loss:0.004304950983224115\n",
      "train loss:0.02760296010238367\n",
      "train loss:0.0018291815245187414\n",
      "train loss:0.005316589643545837\n",
      "train loss:0.028410064718948188\n",
      "train loss:0.0007973341362671613\n",
      "train loss:0.013202090134960234\n",
      "train loss:0.003434054575543441\n",
      "train loss:0.0032581476496672313\n",
      "train loss:0.0022736365609695905\n",
      "train loss:0.0013136156578833244\n",
      "train loss:0.009359868410412054\n",
      "train loss:0.013221973019212917\n",
      "train loss:0.0005070689935371293\n",
      "train loss:0.001809915860818724\n",
      "train loss:0.00238952485626672\n",
      "train loss:0.005352115187054185\n",
      "train loss:0.0013664909549792128\n",
      "train loss:0.002783810110921645\n",
      "train loss:0.0014270786354989265\n",
      "train loss:0.002824285712466569\n",
      "train loss:0.013321375324023861\n",
      "train loss:0.0020051578348357856\n",
      "train loss:0.012994191605954759\n",
      "train loss:0.0006843954955931523\n",
      "train loss:0.001861780031659381\n",
      "train loss:0.0031095102661917663\n",
      "train loss:0.0003636483073696081\n",
      "train loss:0.0043013780213693896\n",
      "train loss:0.024090584861275764\n",
      "train loss:0.0019228153349152683\n",
      "train loss:0.0014576983232741387\n",
      "train loss:0.009637071295993501\n",
      "train loss:0.000778594939961834\n",
      "train loss:0.003948471698557379\n",
      "train loss:0.0018463536370137957\n",
      "train loss:0.017926746419615863\n",
      "train loss:0.0008773486317330646\n",
      "train loss:0.002160006171055097\n",
      "train loss:0.0017729807716604152\n",
      "train loss:0.0006138960244454182\n",
      "train loss:0.006774689535178893\n",
      "train loss:0.0007924074396933607\n",
      "train loss:0.0006731999529760094\n",
      "train loss:0.009497961017764593\n",
      "train loss:0.0012599971607450116\n",
      "train loss:0.008344858878794482\n",
      "train loss:0.0014010608044201325\n",
      "train loss:0.0005748119804352711\n",
      "train loss:0.004984612141092091\n",
      "train loss:0.004473926156945472\n",
      "train loss:0.0006170761945819743\n",
      "train loss:0.027329589448803496\n",
      "train loss:0.00460651579798889\n",
      "train loss:0.0016479445751287614\n",
      "train loss:0.0021515425923833327\n",
      "train loss:0.0022126147113001944\n",
      "train loss:0.02766458944501511\n",
      "train loss:0.002892750161147461\n",
      "train loss:0.0026854520186509055\n",
      "train loss:0.0012800512570750162\n",
      "train loss:0.0011669505442998523\n",
      "train loss:0.011052699031588169\n",
      "train loss:0.004052140452536725\n",
      "train loss:0.009521311062919879\n",
      "train loss:0.0006484255454299936\n",
      "train loss:0.0032291638121210203\n",
      "train loss:0.0007862761011073407\n",
      "train loss:0.0006717165195218996\n",
      "train loss:0.002376619509424874\n",
      "train loss:0.006184399356422228\n",
      "train loss:0.005253438833298528\n",
      "train loss:0.005361890725806675\n",
      "train loss:0.005774477692628336\n",
      "train loss:0.023815061918807126\n",
      "train loss:0.0049500363517765026\n",
      "train loss:0.0026013739785403565\n",
      "train loss:0.0037416493842918506\n",
      "train loss:0.0012443182525105198\n",
      "train loss:0.002729063204554009\n",
      "train loss:0.0007876168009025137\n",
      "train loss:0.0006135538259895661\n",
      "train loss:0.00311407869449967\n",
      "train loss:0.005664918413448573\n",
      "train loss:0.0003287280195793861\n",
      "train loss:0.05635876997364925\n",
      "train loss:0.01786488600328005\n",
      "train loss:0.007060967109678185\n",
      "train loss:0.0022068866072384946\n",
      "train loss:0.005519790953933547\n",
      "train loss:0.0024910257961856287\n",
      "train loss:0.0009793635917710228\n",
      "train loss:0.001399881725434616\n",
      "train loss:0.005333884161062171\n",
      "train loss:0.013159114729436759\n",
      "train loss:0.023745748912546046\n",
      "train loss:0.0015410402513104384\n",
      "train loss:0.0007447778543928136\n",
      "train loss:0.00502906263537707\n",
      "train loss:0.018561387759771775\n",
      "train loss:0.005328009818528542\n",
      "train loss:0.0006694029159195626\n",
      "train loss:0.006067074078410554\n",
      "train loss:0.0009265652313055787\n",
      "train loss:0.007328485393524674\n",
      "train loss:0.001674670078125998\n",
      "train loss:0.004762913762232454\n",
      "train loss:0.006080404864410955\n",
      "train loss:0.001122035250861778\n",
      "train loss:0.002615617406477682\n",
      "train loss:0.004739278607300959\n",
      "train loss:0.0008916679205629714\n",
      "train loss:0.002116290411437104\n",
      "train loss:0.005585429647377301\n",
      "train loss:0.0029592927914971513\n",
      "train loss:0.0019784452632991153\n",
      "train loss:0.002013755405079963\n",
      "train loss:0.0012389890120162639\n",
      "train loss:0.002093904926846207\n",
      "train loss:0.006347452837941791\n",
      "train loss:0.0019775322943084207\n",
      "train loss:0.00465448187655152\n",
      "train loss:0.002271629801998829\n",
      "train loss:0.001009673793857711\n",
      "train loss:0.00149447795618042\n",
      "train loss:0.0007149843365404719\n",
      "train loss:0.004944672803477188\n",
      "train loss:0.008015343034018615\n",
      "train loss:0.001195734657879785\n",
      "train loss:0.0007644519506438904\n",
      "train loss:0.01291030688768237\n",
      "train loss:0.0007996177788959938\n",
      "train loss:0.025667848241801704\n",
      "train loss:0.009334815166296475\n",
      "train loss:0.0009243298319739963\n",
      "train loss:0.004580143570770201\n",
      "train loss:0.0002104489237946534\n",
      "train loss:0.0008252792258898803\n",
      "train loss:0.0021117295208710114\n",
      "train loss:0.005652365768968648\n",
      "train loss:0.0005299615968349731\n",
      "train loss:0.005887428696774214\n",
      "train loss:0.0028684313416096206\n",
      "train loss:0.009633964636525005\n",
      "train loss:0.0034304675062874945\n",
      "train loss:0.00018526575188728187\n",
      "train loss:0.006161558910071852\n",
      "train loss:0.01722186816401926\n",
      "train loss:0.0033958692215717533\n",
      "train loss:0.003008539146694145\n",
      "train loss:0.0003060989952465625\n",
      "train loss:0.0035789838703963665\n",
      "train loss:0.0002260896414158776\n",
      "train loss:0.001301818577544827\n",
      "train loss:0.006239644464421537\n",
      "train loss:0.0021337825044395946\n",
      "train loss:0.0021382930461626183\n",
      "train loss:0.004483918056582525\n",
      "train loss:0.004762969677577802\n",
      "train loss:0.004747399400748894\n",
      "train loss:0.0026297168048151874\n",
      "train loss:0.0010807911323022158\n",
      "train loss:0.002359375285280503\n",
      "train loss:0.031648317183141754\n",
      "train loss:0.003134972148529246\n",
      "train loss:0.0029423051551791136\n",
      "train loss:0.00582947923715119\n",
      "train loss:0.030394800671711804\n",
      "train loss:0.001623043771683657\n",
      "train loss:0.0020246635037042297\n",
      "train loss:0.004975448931835834\n",
      "train loss:0.0020555725892165296\n",
      "train loss:0.00684424193505279\n",
      "train loss:0.009208734475238607\n",
      "train loss:0.0022936207475477764\n",
      "train loss:0.01986545443122892\n",
      "train loss:0.05164634328867757\n",
      "train loss:0.0018553936447866732\n",
      "train loss:0.005006053159750722\n",
      "train loss:0.024106475515257717\n",
      "train loss:0.0008110675211770507\n",
      "train loss:0.0008276374140075752\n",
      "train loss:0.001014168619871475\n",
      "train loss:0.007203250172919836\n",
      "train loss:0.002729854551188833\n",
      "train loss:0.0016358400130274788\n",
      "train loss:0.002766444929745252\n",
      "train loss:0.014488132109571505\n",
      "train loss:0.026670021431504938\n",
      "train loss:0.021581969393000812\n",
      "train loss:0.016223864021923832\n",
      "train loss:0.024625895498510717\n",
      "train loss:0.004492576552413708\n",
      "train loss:0.005139870841156993\n",
      "train loss:0.0011567404733181699\n",
      "train loss:0.005354365572202548\n",
      "train loss:0.00243372177643604\n",
      "train loss:0.0038816457390627897\n",
      "train loss:0.022325543152744928\n",
      "train loss:0.008107273678449618\n",
      "train loss:0.0027778275369818306\n",
      "train loss:0.002908572305834588\n",
      "train loss:0.01134266224971162\n",
      "train loss:0.03846651993058822\n",
      "train loss:0.0003135332798308079\n",
      "train loss:0.005596794070802546\n",
      "train loss:0.009992881395779939\n",
      "train loss:0.0017987464107776746\n",
      "train loss:0.033523029294603046\n",
      "train loss:0.001085892932429316\n",
      "train loss:0.0003432107389073263\n",
      "train loss:0.005681719482881424\n",
      "train loss:0.006085932423376977\n",
      "train loss:0.003197954847775432\n",
      "train loss:0.009492867433088926\n",
      "train loss:0.006822168208414754\n",
      "train loss:0.010032700475923284\n",
      "train loss:0.0024754920459889806\n",
      "train loss:0.005200613311353963\n",
      "train loss:0.008409588902923688\n",
      "train loss:0.0010362066578461675\n",
      "train loss:0.010006966610217933\n",
      "train loss:0.0006270146967503753\n",
      "train loss:0.0026248831757126004\n",
      "train loss:0.0024461730703046135\n",
      "=== epoch:12, train acc:0.996, test acc:0.983 ===\n",
      "train loss:0.004733810425000539\n",
      "train loss:0.0031152039841315397\n",
      "train loss:0.0021383590672293066\n",
      "train loss:0.008391018815835802\n",
      "train loss:0.009377157302340613\n",
      "train loss:0.013055196355364462\n",
      "train loss:0.025954475352060143\n",
      "train loss:0.002545784431628389\n",
      "train loss:0.026846455097979\n",
      "train loss:0.0015716815368599676\n",
      "train loss:0.007038836277837151\n",
      "train loss:0.005367853377857928\n",
      "train loss:0.001408144673509636\n",
      "train loss:0.00089048977876015\n",
      "train loss:0.005255659188108635\n",
      "train loss:0.01729559484721522\n",
      "train loss:0.0015386455289670734\n",
      "train loss:0.0010060460804996029\n",
      "train loss:0.005377442515028863\n",
      "train loss:0.004131662440019147\n",
      "train loss:0.008420126560555255\n",
      "train loss:0.004554351696956996\n",
      "train loss:0.004368961513788329\n",
      "train loss:0.011011087416412123\n",
      "train loss:0.0037824745758548695\n",
      "train loss:0.0049893469702210706\n",
      "train loss:0.002346717552444692\n",
      "train loss:0.003436645618439346\n",
      "train loss:0.002195567896026042\n",
      "train loss:0.0008327265499416371\n",
      "train loss:0.006503567829538188\n",
      "train loss:0.0052730949006616875\n",
      "train loss:0.0018044579379937456\n",
      "train loss:0.015114130822676052\n",
      "train loss:0.010451605854769556\n",
      "train loss:0.005805787354394476\n",
      "train loss:0.0046621622736156415\n",
      "train loss:0.0011573201279624988\n",
      "train loss:0.010584781708095356\n",
      "train loss:0.010850679422251451\n",
      "train loss:0.005419685348303862\n",
      "train loss:0.001308981705633217\n",
      "train loss:0.008434707212605513\n",
      "train loss:0.010513014494362795\n",
      "train loss:0.0007062926963733626\n",
      "train loss:0.0035032967437717746\n",
      "train loss:0.015099645925074998\n",
      "train loss:0.007853271323739103\n",
      "train loss:0.004977222594189106\n",
      "train loss:0.0020282337674624876\n",
      "train loss:0.006865109502220029\n",
      "train loss:0.001441941437415008\n",
      "train loss:0.017186887184290137\n",
      "train loss:0.0005214913620225097\n",
      "train loss:0.003555701374623796\n",
      "train loss:0.0013030863168478765\n",
      "train loss:0.004006177333452809\n",
      "train loss:0.0013912712213745534\n",
      "train loss:0.004545329046114522\n",
      "train loss:0.006867951707775948\n",
      "train loss:0.002157957444947798\n",
      "train loss:0.0030847562882440043\n",
      "train loss:0.007141285652202159\n",
      "train loss:0.0013816501250738713\n",
      "train loss:0.001546389075279956\n",
      "train loss:0.0013444623121599286\n",
      "train loss:0.001890932289350049\n",
      "train loss:0.003382984656577964\n",
      "train loss:0.0008941789832428399\n",
      "train loss:0.0006176191198689538\n",
      "train loss:0.0009285868928179002\n",
      "train loss:0.0006969782019971134\n",
      "train loss:0.008297089363812028\n",
      "train loss:0.01675269889260885\n",
      "train loss:0.005278672379163894\n",
      "train loss:0.001342679431031678\n",
      "train loss:0.0010255058041938388\n",
      "train loss:0.007841875396563539\n",
      "train loss:0.005032468482423305\n",
      "train loss:0.011022493287808617\n",
      "train loss:0.002740273299565354\n",
      "train loss:0.0004966975047878604\n",
      "train loss:0.006725632805438571\n",
      "train loss:0.00529711097361104\n",
      "train loss:0.022145873520060054\n",
      "train loss:0.007277568550264944\n",
      "train loss:0.0019630029326236755\n",
      "train loss:0.009026641986550265\n",
      "train loss:0.011034788283122989\n",
      "train loss:0.005912110763761028\n",
      "train loss:0.006961557347996132\n",
      "train loss:0.004387446145461246\n",
      "train loss:0.0044971948526943565\n",
      "train loss:0.0013782244857812641\n",
      "train loss:0.0022075092759534388\n",
      "train loss:0.0065787927708967405\n",
      "train loss:0.00608870004583514\n",
      "train loss:0.0017223330465530673\n",
      "train loss:0.007706969693338662\n",
      "train loss:0.0004672328161167057\n",
      "train loss:0.0051379566619694636\n",
      "train loss:0.0024565740540591786\n",
      "train loss:0.020005705796334684\n",
      "train loss:0.009697421654185728\n",
      "train loss:0.00459091230777213\n",
      "train loss:0.008836350298006768\n",
      "train loss:0.008933589634761607\n",
      "train loss:0.0024641818150995805\n",
      "train loss:0.0036867249163895226\n",
      "train loss:0.0004534639640427541\n",
      "train loss:0.004581831311177733\n",
      "train loss:0.0013347346688716746\n",
      "train loss:0.008670728733167953\n",
      "train loss:0.007684586657419275\n",
      "train loss:0.001682808809319747\n",
      "train loss:0.0034411748363067894\n",
      "train loss:0.005705576484184316\n",
      "train loss:0.008071672872092933\n",
      "train loss:0.004738873105345342\n",
      "train loss:0.016735878900041006\n",
      "train loss:0.0012265732563460164\n",
      "train loss:0.0038516354359251328\n",
      "train loss:0.0004234205687181273\n",
      "train loss:0.005131225376924325\n",
      "train loss:0.023595890626988178\n",
      "train loss:0.0014033372691121518\n",
      "train loss:0.0007358298001634715\n",
      "train loss:0.007743951448768582\n",
      "train loss:0.007318548119971352\n",
      "train loss:0.0024225722276784035\n",
      "train loss:0.0010708977598826806\n",
      "train loss:0.010308717375818278\n",
      "train loss:0.0032865677923862974\n",
      "train loss:0.0017726854371915277\n",
      "train loss:0.005501368286569303\n",
      "train loss:0.004996659251121916\n",
      "train loss:0.028604821914552235\n",
      "train loss:0.0005524719347357624\n",
      "train loss:0.0010125495103252948\n",
      "train loss:0.008445242809230077\n",
      "train loss:0.0015563281051390176\n",
      "train loss:0.0028879401698290457\n",
      "train loss:0.001400669931583218\n",
      "train loss:0.00929448094118942\n",
      "train loss:0.011887523597145273\n",
      "train loss:0.0030590687850535103\n",
      "train loss:0.014496974718529135\n",
      "train loss:0.009236605703141188\n",
      "train loss:0.02137272732320015\n",
      "train loss:0.0014015333652070989\n",
      "train loss:0.0014239976344104668\n",
      "train loss:0.0010153452436244472\n",
      "train loss:0.0032244015503258856\n",
      "train loss:0.007313480879414941\n",
      "train loss:0.001833973871607266\n",
      "train loss:0.007044948404935965\n",
      "train loss:0.002644256509562346\n",
      "train loss:0.004331177077381657\n",
      "train loss:0.0036329071288505816\n",
      "train loss:0.002220073254599001\n",
      "train loss:0.0012697507541487535\n",
      "train loss:0.0016500122835440465\n",
      "train loss:0.003177030267658269\n",
      "train loss:0.011390867928920932\n",
      "train loss:0.0062970368777659535\n",
      "train loss:0.0009374794725922565\n",
      "train loss:0.0008979367623572866\n",
      "train loss:0.001518817604134639\n",
      "train loss:0.00021224018662787205\n",
      "train loss:0.002876441321586003\n",
      "train loss:0.0012156447894955574\n",
      "train loss:0.004781412941832309\n",
      "train loss:0.010407322567249347\n",
      "train loss:0.001875189338787129\n",
      "train loss:0.0014196427627996041\n",
      "train loss:0.002885182475777741\n",
      "train loss:0.004382323896943288\n",
      "train loss:0.002781562023050678\n",
      "train loss:0.006903793862085722\n",
      "train loss:0.0142932088254061\n",
      "train loss:0.00042169813646816606\n",
      "train loss:0.007503972912665921\n",
      "train loss:0.002251928660514002\n",
      "train loss:0.006745287072953616\n",
      "train loss:0.0038478140064688417\n",
      "train loss:0.009377667434932582\n",
      "train loss:0.005750272292511662\n",
      "train loss:0.006161422127050524\n",
      "train loss:0.000738589337132059\n",
      "train loss:0.00092534014868595\n",
      "train loss:0.001152225005930349\n",
      "train loss:0.06952470567513516\n",
      "train loss:0.0024029029156651545\n",
      "train loss:0.00134079796573828\n",
      "train loss:0.002050687817001192\n",
      "train loss:0.004733078535803415\n",
      "train loss:0.0006620841258012282\n",
      "train loss:0.0014806543194983903\n",
      "train loss:0.003547506939644337\n",
      "train loss:0.004749584908030088\n",
      "train loss:0.005501242724249546\n",
      "train loss:0.004585862013140054\n",
      "train loss:0.0013320681459118194\n",
      "train loss:0.001385749028831076\n",
      "train loss:0.0011099093708777227\n",
      "train loss:0.002746090357203644\n",
      "train loss:0.0027242262996920886\n",
      "train loss:0.007874313097648907\n",
      "train loss:0.004948981750161548\n",
      "train loss:0.005076233729514513\n",
      "train loss:0.007279427978089195\n",
      "train loss:0.011690237165203551\n",
      "train loss:0.0006151985581221428\n",
      "train loss:0.004305040947232847\n",
      "train loss:0.0017320030425760707\n",
      "train loss:0.0032150662759337562\n",
      "train loss:0.005176540789176588\n",
      "train loss:0.002423457589954471\n",
      "train loss:0.0012783810389190757\n",
      "train loss:0.004843947319639496\n",
      "train loss:0.006633218394498486\n",
      "train loss:0.0047776122853394895\n",
      "train loss:0.004880976705571283\n",
      "train loss:0.024158175361535958\n",
      "train loss:0.006815394462616942\n",
      "train loss:0.000593710241797169\n",
      "train loss:0.002917122461073994\n",
      "train loss:0.003225888571042912\n",
      "train loss:0.0004285362751024856\n",
      "train loss:0.0002454828960692397\n",
      "train loss:0.0026145908465727173\n",
      "train loss:0.00038487791863740377\n",
      "train loss:0.0016339870780853534\n",
      "train loss:0.0038132788020180234\n",
      "train loss:0.002253607517257012\n",
      "train loss:0.007289506330847993\n",
      "train loss:0.001762082364389892\n",
      "train loss:0.00029147379356692023\n",
      "train loss:0.0012117505738071567\n",
      "train loss:0.02956966363758719\n",
      "train loss:0.007142318036148163\n",
      "train loss:0.0013699629005631132\n",
      "train loss:0.015418446924627693\n",
      "train loss:0.002455411516859943\n",
      "train loss:0.0025598321226391455\n",
      "train loss:0.004821814028943856\n",
      "train loss:0.008025411124488173\n",
      "train loss:0.003851792738318327\n",
      "train loss:0.02287716910807009\n",
      "train loss:0.0015851544513228442\n",
      "train loss:0.0020162696961153\n",
      "train loss:0.0008213898183454556\n",
      "train loss:0.00226436080917333\n",
      "train loss:0.0020111034350507886\n",
      "train loss:0.0005247828723693619\n",
      "train loss:0.009572266346818396\n",
      "train loss:0.00046243952214135547\n",
      "train loss:0.00047927616143459946\n",
      "train loss:0.00021262930822204133\n",
      "train loss:0.002192738492951001\n",
      "train loss:0.00850109112239601\n",
      "train loss:0.0015320475125077803\n",
      "train loss:0.0033808964777102216\n",
      "train loss:0.0006079067337103771\n",
      "train loss:0.012903915055516134\n",
      "train loss:0.0027594580912806543\n",
      "train loss:0.00016458353739428247\n",
      "train loss:0.005301735045059517\n",
      "train loss:0.001406272941445859\n",
      "train loss:0.0003463157160616401\n",
      "train loss:0.002305037742007855\n",
      "train loss:0.001591620031033308\n",
      "train loss:0.0059470271260655405\n",
      "train loss:0.0003776555886558463\n",
      "train loss:0.001153665429871668\n",
      "train loss:0.0028377912361863167\n",
      "train loss:0.008502008326212174\n",
      "train loss:0.0023577896601825357\n",
      "train loss:0.0011000490872703195\n",
      "train loss:0.008621319432183689\n",
      "train loss:0.0007532761300248662\n",
      "train loss:0.006419269762295457\n",
      "train loss:0.001504498177064864\n",
      "train loss:0.0003751982385226399\n",
      "train loss:0.0108112692908152\n",
      "train loss:0.006403032235031334\n",
      "train loss:0.005808812117059625\n",
      "train loss:0.0010031441272479941\n",
      "train loss:0.014078259660034492\n",
      "train loss:0.0016860512166972615\n",
      "train loss:0.03027760714782041\n",
      "train loss:0.0011524005459700832\n",
      "train loss:0.0018919892109927228\n",
      "train loss:0.0014061299915143438\n",
      "train loss:0.00279083866590563\n",
      "train loss:0.004908741129398388\n",
      "train loss:0.009277319268744142\n",
      "train loss:0.001616679918678652\n",
      "train loss:0.0073441136105721425\n",
      "train loss:0.006145114574922609\n",
      "train loss:0.0019308301207343617\n",
      "train loss:0.0009700330684913342\n",
      "train loss:0.0012991185325003038\n",
      "train loss:0.0008596800253107431\n",
      "train loss:0.0054093605270423\n",
      "train loss:0.0017329606618463516\n",
      "train loss:0.008754150487554409\n",
      "train loss:0.0013329042004007116\n",
      "train loss:0.00454866008655293\n",
      "train loss:0.00035478751383949555\n",
      "train loss:0.0027297972237710195\n",
      "train loss:0.002508718498442425\n",
      "train loss:0.00995086161153263\n",
      "train loss:0.004791263297108852\n",
      "train loss:0.0047178152799396585\n",
      "train loss:0.0005344819343883249\n",
      "train loss:0.001193519061191109\n",
      "train loss:0.0007235221502455054\n",
      "train loss:0.0015063013697723558\n",
      "train loss:0.009863053751573623\n",
      "train loss:0.003765379971754768\n",
      "train loss:0.0009332435783455547\n",
      "train loss:0.00991227232307652\n",
      "train loss:0.0005183834851986809\n",
      "train loss:0.0025819850624700957\n",
      "train loss:0.0007782899644610565\n",
      "train loss:0.00021105615304225237\n",
      "train loss:0.001471441754731517\n",
      "train loss:0.006810710761385154\n",
      "train loss:0.0034706979452060003\n",
      "train loss:0.001533225556215866\n",
      "train loss:0.0019372571995108906\n",
      "train loss:0.0016051080483673451\n",
      "train loss:0.00029070643689906966\n",
      "train loss:0.001301896403641255\n",
      "train loss:0.004066467877986765\n",
      "train loss:0.005318618538468763\n",
      "train loss:0.003038334117380847\n",
      "train loss:0.003972367274780753\n",
      "train loss:0.0070573056710722445\n",
      "train loss:0.0010929442227602519\n",
      "train loss:0.007836991390758464\n",
      "train loss:0.0005980298177164687\n",
      "train loss:0.007304138345492511\n",
      "train loss:0.0007004301983137108\n",
      "train loss:0.00645244824608111\n",
      "train loss:0.0005650235022455234\n",
      "train loss:0.00545077260963714\n",
      "train loss:0.000577683647823875\n",
      "train loss:0.0012008964560641568\n",
      "train loss:0.0025013999505539234\n",
      "train loss:0.0007385198568892172\n",
      "train loss:0.0019232917365086\n",
      "train loss:0.006520876046838772\n",
      "train loss:0.0014260954438010702\n",
      "train loss:0.003418008639891402\n",
      "train loss:0.09240911631666338\n",
      "train loss:0.0016493372969078293\n",
      "train loss:0.005278614521486937\n",
      "train loss:0.000708437884508402\n",
      "train loss:0.0006053232276476242\n",
      "train loss:0.00046249040305518707\n",
      "train loss:0.027279695340504718\n",
      "train loss:0.006016564709879007\n",
      "train loss:0.0024200079934235544\n",
      "train loss:0.004280885748911321\n",
      "train loss:0.002195222436617578\n",
      "train loss:0.0005846753860480186\n",
      "train loss:0.0013235808072968181\n",
      "train loss:0.004672177886386826\n",
      "train loss:0.004837772725976852\n",
      "train loss:0.002712184166395598\n",
      "train loss:0.002816981166957078\n",
      "train loss:0.030923791430414686\n",
      "train loss:0.0031412787478615973\n",
      "train loss:0.0019762796394267577\n",
      "train loss:0.04189244173638164\n",
      "train loss:0.003124177168250931\n",
      "train loss:0.004700669952362593\n",
      "train loss:0.0018009370033093067\n",
      "train loss:0.007600623785446129\n",
      "train loss:0.004186192584459048\n",
      "train loss:0.0037295905277577707\n",
      "train loss:0.0008449075089679125\n",
      "train loss:0.0004603349272445376\n",
      "train loss:0.005759872903579088\n",
      "train loss:0.008077529725587519\n",
      "train loss:0.0017572394034235878\n",
      "train loss:0.0026478935061408477\n",
      "train loss:0.0010201139409763518\n",
      "train loss:0.005075106197696077\n",
      "train loss:0.00020280962067832455\n",
      "train loss:0.0031281070764232265\n",
      "train loss:0.00976843721064418\n",
      "train loss:0.0011645136879388411\n",
      "train loss:0.007971628293814917\n",
      "train loss:0.008204947654220392\n",
      "train loss:0.002884437524053976\n",
      "train loss:0.0032740907791395444\n",
      "train loss:0.0012652600712679716\n",
      "train loss:0.023627153253055727\n",
      "train loss:0.010701336695646227\n",
      "train loss:0.005622180203585897\n",
      "train loss:0.032596491284112836\n",
      "train loss:0.0031404018239384757\n",
      "train loss:0.0010628863908976932\n",
      "train loss:0.0008667830451334335\n",
      "train loss:0.0013121830864562315\n",
      "train loss:0.011948612890936804\n",
      "train loss:0.007312454987272565\n",
      "train loss:0.0009414850830466312\n",
      "train loss:0.00258006163066959\n",
      "train loss:0.0011171819822728664\n",
      "train loss:0.0035921972637046684\n",
      "train loss:0.004816020253272642\n",
      "train loss:0.0039051414618091252\n",
      "train loss:0.001169352307214714\n",
      "train loss:0.0003056338030473127\n",
      "train loss:0.00027750100029240253\n",
      "train loss:0.004990960306332944\n",
      "train loss:0.00342072720363689\n",
      "train loss:0.03521610464940717\n",
      "train loss:0.0023593526802615453\n",
      "train loss:0.002972726200063247\n",
      "train loss:0.003767572272114117\n",
      "train loss:0.006322979103157605\n",
      "train loss:0.0037383190077224845\n",
      "train loss:0.001805319646767099\n",
      "train loss:0.0028680504188398474\n",
      "train loss:0.0004915386173806865\n",
      "train loss:0.0006322884444861803\n",
      "train loss:0.0005764890793957228\n",
      "train loss:0.0080506265030399\n",
      "train loss:0.000800967645794269\n",
      "train loss:0.0018053449784103017\n",
      "train loss:0.0007111322210800375\n",
      "train loss:0.0008767583645218684\n",
      "train loss:0.007505800256533387\n",
      "train loss:0.010052675458223614\n",
      "train loss:0.003847493786652865\n",
      "train loss:0.0007506172786621784\n",
      "train loss:0.004185315498300933\n",
      "train loss:0.0019355400259376526\n",
      "train loss:0.0034121560586225473\n",
      "train loss:0.005314128026746027\n",
      "train loss:0.0031815991593156095\n",
      "train loss:0.06206889934796329\n",
      "train loss:0.0029381766105466257\n",
      "train loss:0.00019056689266638252\n",
      "train loss:0.0022600019392680104\n",
      "train loss:0.009221036655209916\n",
      "train loss:0.0007610711075767284\n",
      "train loss:0.0003237071183730061\n",
      "train loss:0.017167874432656945\n",
      "train loss:0.0011838022703815954\n",
      "train loss:0.009594816320957265\n",
      "train loss:0.002045696201348154\n",
      "train loss:0.0012156525057455817\n",
      "train loss:0.0012640590628359536\n",
      "train loss:0.006036175366159719\n",
      "train loss:0.0017053802536630856\n",
      "train loss:0.0025008469112753995\n",
      "train loss:0.03518262744111056\n",
      "train loss:0.0007478958332418789\n",
      "train loss:0.007205882838052406\n",
      "train loss:0.003984365624995197\n",
      "train loss:0.006152879523370877\n",
      "train loss:0.0008098037902453735\n",
      "train loss:0.0028537524432790985\n",
      "train loss:0.00385437000350685\n",
      "train loss:0.043677981130822705\n",
      "train loss:0.0010449035293665853\n",
      "train loss:0.0007450572505781064\n",
      "train loss:0.003576671164103456\n",
      "train loss:0.0029467843935182796\n",
      "train loss:0.004729252139480582\n",
      "train loss:0.01646066935266459\n",
      "train loss:0.006318578506371925\n",
      "train loss:0.002104673280556707\n",
      "train loss:0.0018745188587790567\n",
      "train loss:0.003814368065283931\n",
      "train loss:0.0042709662021834715\n",
      "train loss:0.0029954307449119033\n",
      "train loss:0.007941269310410138\n",
      "train loss:0.0029528919355753906\n",
      "train loss:0.0001251887477258799\n",
      "train loss:0.00028344097278201324\n",
      "train loss:0.00475088626431453\n",
      "train loss:0.016374295653804143\n",
      "train loss:0.007412094007487892\n",
      "train loss:0.0029425016801921285\n",
      "train loss:0.0035665461085658172\n",
      "train loss:0.007564561685287827\n",
      "train loss:0.0023749715969576353\n",
      "train loss:0.005835764168431956\n",
      "train loss:0.0027780206772265053\n",
      "train loss:0.0029496484680218075\n",
      "train loss:0.0005510737880052271\n",
      "train loss:0.004037834505204963\n",
      "train loss:0.0014054444845053664\n",
      "train loss:0.06631142444065895\n",
      "train loss:0.0010202191326790242\n",
      "train loss:0.0009172525750956804\n",
      "train loss:0.004877901769880742\n",
      "train loss:0.0035822698078090626\n",
      "train loss:0.00013781080700943658\n",
      "train loss:0.005610011408812368\n",
      "train loss:0.002829130647049601\n",
      "train loss:0.0014469054504663299\n",
      "train loss:0.006656973865652248\n",
      "train loss:0.0017210331310646015\n",
      "train loss:0.0018274375395545394\n",
      "train loss:0.0011750076545951297\n",
      "train loss:0.0014395447007210013\n",
      "train loss:0.000725960849138666\n",
      "train loss:0.0057566618838567505\n",
      "train loss:0.007149081719358166\n",
      "train loss:0.0017872796507230457\n",
      "train loss:0.0068885699843148915\n",
      "train loss:0.000772408610246039\n",
      "train loss:0.0006057399499812082\n",
      "train loss:0.0022906981445390603\n",
      "train loss:0.0016386989445904107\n",
      "train loss:0.015368733698365354\n",
      "train loss:0.0014216044303328717\n",
      "train loss:0.0008648175520691837\n",
      "train loss:0.0015825424299809744\n",
      "train loss:0.0013566625715127234\n",
      "train loss:0.0022048337294619936\n",
      "train loss:0.032962518886959696\n",
      "train loss:0.0021515540312735575\n",
      "train loss:0.004842819614223893\n",
      "train loss:0.0026280309634864503\n",
      "train loss:0.0043643335901161935\n",
      "train loss:0.005600532095385095\n",
      "train loss:0.0017748271971551428\n",
      "train loss:0.014723733811663222\n",
      "train loss:0.0239631113010227\n",
      "train loss:0.00023637176963135474\n",
      "train loss:0.008758794427605522\n",
      "train loss:0.0018149876565853554\n",
      "train loss:0.004549900617601568\n",
      "train loss:0.004681946004789648\n",
      "train loss:0.0008849970643645834\n",
      "train loss:0.0008133772681167765\n",
      "train loss:0.004031319359015972\n",
      "train loss:0.0012659931771421492\n",
      "train loss:0.012132250478629814\n",
      "train loss:0.017115841297161843\n",
      "train loss:0.00462756443809313\n",
      "train loss:0.000651442307366409\n",
      "train loss:0.00815347504330572\n",
      "train loss:0.00112783035997729\n",
      "train loss:0.01108368353999387\n",
      "train loss:0.0019624907463839714\n",
      "train loss:0.0019893502240848947\n",
      "train loss:0.004494337170045242\n",
      "train loss:0.0009596156550904593\n",
      "train loss:0.007280923370046637\n",
      "train loss:0.00037480570588361813\n",
      "train loss:0.009694308139485606\n",
      "train loss:0.005419900600604227\n",
      "train loss:0.0011110485324413192\n",
      "train loss:0.0021579113641295427\n",
      "train loss:0.0006630270952123619\n",
      "train loss:0.0030548988835592266\n",
      "train loss:0.015437263595516882\n",
      "train loss:0.004015567220852569\n",
      "train loss:0.003153670356224151\n",
      "train loss:0.03155394081631173\n",
      "train loss:0.005083895034781017\n",
      "train loss:0.0005358025376221401\n",
      "train loss:0.007653343864314467\n",
      "train loss:0.010799579536555508\n",
      "train loss:0.002756436610139987\n",
      "train loss:0.026454547986388355\n",
      "train loss:0.004291271685846884\n",
      "train loss:0.0036952914861819526\n",
      "train loss:0.009399053674977806\n",
      "train loss:0.002232968105704991\n",
      "train loss:0.010343546869247591\n",
      "train loss:0.0002530756773115765\n",
      "train loss:0.0034178905983534708\n",
      "train loss:0.002996796564834231\n",
      "train loss:0.0006286346437813794\n",
      "train loss:0.0013540466394337672\n",
      "train loss:0.0013365299686778106\n",
      "train loss:0.007417395984654569\n",
      "train loss:0.00912555992945509\n",
      "train loss:0.00401811967953616\n",
      "train loss:0.006528532850719757\n",
      "train loss:0.0008610240682421729\n",
      "train loss:0.0015537413275299027\n",
      "train loss:0.0004270575539809583\n",
      "train loss:0.0016269619317194428\n",
      "train loss:0.000925811162765858\n",
      "train loss:0.0019063410468959305\n",
      "train loss:0.052595085576732134\n",
      "train loss:0.0002607102377495376\n",
      "train loss:0.0010040800931889568\n",
      "=== epoch:13, train acc:0.997, test acc:0.984 ===\n",
      "train loss:0.0021239913337283366\n",
      "train loss:0.01677299592070136\n",
      "train loss:0.0006727236846283456\n",
      "train loss:0.0002987208427831982\n",
      "train loss:0.002471254982779868\n",
      "train loss:0.0023947476852676196\n",
      "train loss:0.003013936352896694\n",
      "train loss:0.019062313738068378\n",
      "train loss:0.0003998832615686365\n",
      "train loss:0.030026902165567124\n",
      "train loss:0.003042870336522006\n",
      "train loss:0.01464646747041432\n",
      "train loss:0.010834331889685005\n",
      "train loss:0.0003277896393344008\n",
      "train loss:0.010090336999994558\n",
      "train loss:0.006267899942014305\n",
      "train loss:0.005762204572495977\n",
      "train loss:0.0031919509153111396\n",
      "train loss:0.001205362087120346\n",
      "train loss:0.0003167039911486098\n",
      "train loss:0.0019440585776206143\n",
      "train loss:0.0005109358067019761\n",
      "train loss:0.0017679089372320417\n",
      "train loss:0.0320203145518639\n",
      "train loss:0.0015574927165519451\n",
      "train loss:0.003506826924114641\n",
      "train loss:0.00023588301197674766\n",
      "train loss:0.004598528596926509\n",
      "train loss:0.008009698108684772\n",
      "train loss:0.0024527322939099185\n",
      "train loss:0.008371074742048477\n",
      "train loss:0.004003598797789452\n",
      "train loss:0.004810281019341422\n",
      "train loss:0.0032917649652888497\n",
      "train loss:0.017291432979802557\n",
      "train loss:0.039208103999409764\n",
      "train loss:0.0772714880500089\n",
      "train loss:0.004434105096053666\n",
      "train loss:0.0009419773212384126\n",
      "train loss:0.0049958318505577415\n",
      "train loss:0.013181670701748034\n",
      "train loss:0.0005291591140855023\n",
      "train loss:0.014669788557913866\n",
      "train loss:0.017472606267673055\n",
      "train loss:0.0015120042481682732\n",
      "train loss:0.0011301125230837397\n",
      "train loss:0.0011562152435788792\n",
      "train loss:0.0034684576935720083\n",
      "train loss:0.002764343939934728\n",
      "train loss:0.003759794443342841\n",
      "train loss:0.004028926554847821\n",
      "train loss:0.001125203148074905\n",
      "train loss:0.009591930452648995\n",
      "train loss:0.004230987217518442\n",
      "train loss:0.0044449526324009895\n",
      "train loss:0.00937571104430499\n",
      "train loss:0.0037398669774446006\n",
      "train loss:0.003937870292957913\n",
      "train loss:0.0033106599656254425\n",
      "train loss:0.00034429535817857496\n",
      "train loss:0.0005584898565631951\n",
      "train loss:0.029839889493577295\n",
      "train loss:0.0031656202501095247\n",
      "train loss:0.0002454417957085911\n",
      "train loss:0.0023584612359791647\n",
      "train loss:0.00376968578764112\n",
      "train loss:0.025830341796231417\n",
      "train loss:0.013691212255709139\n",
      "train loss:0.0044398728647686144\n",
      "train loss:0.0007300248737589505\n",
      "train loss:0.005109625212810026\n",
      "train loss:0.006893419900193464\n",
      "train loss:0.005577308230772047\n",
      "train loss:0.037759308275399237\n",
      "train loss:0.0014538375912520793\n",
      "train loss:0.0025422366285959147\n",
      "train loss:0.0018216736649292037\n",
      "train loss:0.007833283389336525\n",
      "train loss:0.006471749193011209\n",
      "train loss:0.002912230980837096\n",
      "train loss:0.0024253273525285026\n",
      "train loss:0.00399092820278632\n",
      "train loss:0.0019611453741423877\n",
      "train loss:0.0015177641336659225\n",
      "train loss:0.002100576107687617\n",
      "train loss:0.010815395129234526\n",
      "train loss:0.0019261449986668073\n",
      "train loss:0.005332037584892129\n",
      "train loss:0.0008924404753827796\n",
      "train loss:0.008851043997251385\n",
      "train loss:0.002155004552954268\n",
      "train loss:0.0023885905903769863\n",
      "train loss:0.004090830993115751\n",
      "train loss:0.0011960969395170581\n",
      "train loss:0.038627719275257\n",
      "train loss:0.0017894733767225299\n",
      "train loss:0.002221643918297736\n",
      "train loss:0.01731268963136315\n",
      "train loss:0.002815007548131429\n",
      "train loss:0.002507634668560983\n",
      "train loss:0.0006805199238157877\n",
      "train loss:0.002472510302839794\n",
      "train loss:0.002490388418284208\n",
      "train loss:0.006624457270813635\n",
      "train loss:0.0015058022617027772\n",
      "train loss:0.002309760476865004\n",
      "train loss:0.024403143561824128\n",
      "train loss:0.009225778580296218\n",
      "train loss:0.004836130605538337\n",
      "train loss:0.00045645915847956865\n",
      "train loss:0.00955695590413142\n",
      "train loss:0.0040338142789884904\n",
      "train loss:0.0024023889820106534\n",
      "train loss:0.0013224517895635917\n",
      "train loss:0.00245594028596247\n",
      "train loss:0.005320997873422734\n",
      "train loss:0.0006178273504943441\n",
      "train loss:0.013048440628626786\n",
      "train loss:0.016689509500525126\n",
      "train loss:0.0024727166663888038\n",
      "train loss:0.005482187882632843\n",
      "train loss:0.003976162663549047\n",
      "train loss:0.0103219642965088\n",
      "train loss:0.006080073815747335\n",
      "train loss:0.003080474340649165\n",
      "train loss:0.05195327534224177\n",
      "train loss:0.002064964282537841\n",
      "train loss:0.0029647248219922383\n",
      "train loss:0.0008154741624540791\n",
      "train loss:0.003898529455603564\n",
      "train loss:0.0022700515129559107\n",
      "train loss:0.0015999480572635786\n",
      "train loss:0.02773343613763876\n",
      "train loss:0.004486479015859578\n",
      "train loss:0.0008856436847829687\n",
      "train loss:0.0035816374085002336\n",
      "train loss:0.002402021485263115\n",
      "train loss:0.00420892239057191\n",
      "train loss:0.0016721761659056228\n",
      "train loss:0.004255870700271775\n",
      "train loss:0.011163957168054598\n",
      "train loss:0.0028274434778493766\n",
      "train loss:0.003078337378696897\n",
      "train loss:0.01188439227418782\n",
      "train loss:0.0012177249412304533\n",
      "train loss:0.008711631209066494\n",
      "train loss:0.020840635009813875\n",
      "train loss:0.008470867548797485\n",
      "train loss:0.018694911991127678\n",
      "train loss:0.0020895679497337483\n",
      "train loss:0.05834286621087111\n",
      "train loss:0.0025702621210492377\n",
      "train loss:0.03804644338991545\n",
      "train loss:0.0004990540858324689\n",
      "train loss:0.0021801630208491836\n",
      "train loss:0.0033609597624387677\n",
      "train loss:0.0019151912499371679\n",
      "train loss:0.004352231441429396\n",
      "train loss:0.0005973633229981145\n",
      "train loss:0.0004170049405780117\n",
      "train loss:0.0013303082571340438\n",
      "train loss:0.0019737184289573126\n",
      "train loss:0.004589721724433994\n",
      "train loss:0.021159451821872687\n",
      "train loss:0.0019310446257160855\n",
      "train loss:0.000331614055041126\n",
      "train loss:0.006348510149347773\n",
      "train loss:0.004137519629199089\n",
      "train loss:0.0019515675078603594\n",
      "train loss:0.008168945546269823\n",
      "train loss:0.014330580556286956\n",
      "train loss:0.003545459955692557\n",
      "train loss:0.0009518189046046867\n",
      "train loss:0.00035599745812144853\n",
      "train loss:0.0006701372760066013\n",
      "train loss:0.008196347539413014\n",
      "train loss:0.0001768692906957818\n",
      "train loss:0.0014671694919638576\n",
      "train loss:0.0007897551861529518\n",
      "train loss:0.0007048702304318868\n",
      "train loss:0.0018635970964878106\n",
      "train loss:0.005638727789519949\n",
      "train loss:0.009804021301529057\n",
      "train loss:0.0017357991390058805\n",
      "train loss:0.0015750992727751967\n",
      "train loss:0.0035740024434800077\n",
      "train loss:0.004512353672009023\n",
      "train loss:0.00026186332024147086\n",
      "train loss:0.004488572848574686\n",
      "train loss:0.0038148047053457325\n",
      "train loss:0.0034721606376820475\n",
      "train loss:0.0018834081517668773\n",
      "train loss:0.011031702219443097\n",
      "train loss:0.0005473706604599032\n",
      "train loss:0.009486206860312511\n",
      "train loss:0.0018425114370353255\n",
      "train loss:0.011432771937035175\n",
      "train loss:0.004614988435970787\n",
      "train loss:0.002993816727118777\n",
      "train loss:0.004734401419950029\n",
      "train loss:0.003490196260640427\n",
      "train loss:0.0005844978358313199\n",
      "train loss:0.0027533872482765546\n",
      "train loss:0.005069639363073803\n",
      "train loss:0.0006697384182516765\n",
      "train loss:0.009714992709455052\n",
      "train loss:0.0017421334543772985\n",
      "train loss:0.005712923615916637\n",
      "train loss:0.00510608711158238\n",
      "train loss:0.007111507170040066\n",
      "train loss:0.001296415962188506\n",
      "train loss:0.0013950322396683703\n",
      "train loss:0.003916077885640419\n",
      "train loss:0.0061697994268079745\n",
      "train loss:0.0016569121115606598\n",
      "train loss:0.010238732490000997\n",
      "train loss:0.003303079461894192\n",
      "train loss:0.0030985084936636298\n",
      "train loss:0.006323108724770103\n",
      "train loss:0.001410265313614496\n",
      "train loss:0.005089485224220983\n",
      "train loss:0.0018707058465792764\n",
      "train loss:0.0016256809161521231\n",
      "train loss:0.0019404213809453424\n",
      "train loss:0.005176105406477006\n",
      "train loss:0.005846692551014471\n",
      "train loss:0.001102882298704593\n",
      "train loss:0.001171469588539456\n",
      "train loss:0.000617887143641742\n",
      "train loss:0.005667358723164824\n",
      "train loss:0.005910212264953154\n",
      "train loss:0.00682031640122141\n",
      "train loss:0.00035245442272182904\n",
      "train loss:0.0004200363049918839\n",
      "train loss:0.001996914945994211\n",
      "train loss:0.0012765255024882601\n",
      "train loss:0.00858028921133961\n",
      "train loss:0.002580701605629795\n",
      "train loss:0.0025345861399869224\n",
      "train loss:0.006852013194055865\n",
      "train loss:0.005620353201913313\n",
      "train loss:0.00038598674135529324\n",
      "train loss:0.0007692034335528048\n",
      "train loss:0.001737183907630721\n",
      "train loss:0.0019526642436265911\n",
      "train loss:0.011005105395344305\n",
      "train loss:0.0017278759514277784\n",
      "train loss:0.00348254334488863\n",
      "train loss:0.0025689617608507\n",
      "train loss:5.378807316757707e-05\n",
      "train loss:0.01034069727514978\n",
      "train loss:0.0007894854838480792\n",
      "train loss:0.0008984771810290331\n",
      "train loss:0.00428693374706693\n",
      "train loss:0.009462585057181784\n",
      "train loss:0.0003697264477976507\n",
      "train loss:0.00274346980467231\n",
      "train loss:0.0029481296153255248\n",
      "train loss:0.01932485953620112\n",
      "train loss:0.0009766537147339201\n",
      "train loss:0.0004490444562469079\n",
      "train loss:0.002457283946414658\n",
      "train loss:0.0004895237320952013\n",
      "train loss:0.0025901394693602546\n",
      "train loss:0.0029528787478787\n",
      "train loss:0.0006250299438200381\n",
      "train loss:8.104467471757233e-05\n",
      "train loss:0.0009810730371821016\n",
      "train loss:0.004306722318972587\n",
      "train loss:0.00029111727506261133\n",
      "train loss:0.007116359704786833\n",
      "train loss:0.0018016525804492429\n",
      "train loss:0.0019794534032022606\n",
      "train loss:0.0012606260961595603\n",
      "train loss:0.0038466961391275733\n",
      "train loss:0.0023738750798231096\n",
      "train loss:0.06235375686917476\n",
      "train loss:0.0024318024484805464\n",
      "train loss:0.0035935670113115942\n",
      "train loss:0.0005411204871290123\n",
      "train loss:0.0013053492977119039\n",
      "train loss:0.004868293127204109\n",
      "train loss:0.005077786989536476\n",
      "train loss:0.004862270173389307\n",
      "train loss:0.001480073621086069\n",
      "train loss:0.0008952502993281022\n",
      "train loss:0.023524842083101204\n",
      "train loss:0.00032798457371927285\n",
      "train loss:0.0026975717604007794\n",
      "train loss:0.0003772267948302531\n",
      "train loss:0.00559103821927806\n",
      "train loss:0.0010447670885748515\n",
      "train loss:0.024365074383609603\n",
      "train loss:0.00027978639482756227\n",
      "train loss:0.0025218944738373285\n",
      "train loss:0.009277660775463225\n",
      "train loss:0.0018160488520411087\n",
      "train loss:0.004138918637262741\n",
      "train loss:0.03759751359903595\n",
      "train loss:0.001723396751328665\n",
      "train loss:0.001609870879770177\n",
      "train loss:0.009127749687780441\n",
      "train loss:0.0009661680862453334\n",
      "train loss:0.004814131707274812\n",
      "train loss:0.0009396520657873653\n",
      "train loss:8.560174072864824e-05\n",
      "train loss:0.017244811128919364\n",
      "train loss:0.0004504865583138009\n",
      "train loss:0.025481975097946096\n",
      "train loss:0.0055974110408449915\n",
      "train loss:0.00468067686697857\n",
      "train loss:0.0038195340525181026\n",
      "train loss:0.004948294000950208\n",
      "train loss:0.0044382592208964625\n",
      "train loss:0.010653326640253279\n",
      "train loss:0.0017806004782523945\n",
      "train loss:0.014729723973628206\n",
      "train loss:0.00028502467916632364\n",
      "train loss:0.0020164902654365604\n",
      "train loss:0.001597320599086971\n",
      "train loss:0.0025697042246580395\n",
      "train loss:0.01917732000411071\n",
      "train loss:0.003384267857148129\n",
      "train loss:0.0012623894778188232\n",
      "train loss:0.00273471604028206\n",
      "train loss:0.0018223499183941305\n",
      "train loss:0.002358785298763416\n",
      "train loss:0.0027424300841742237\n",
      "train loss:0.0006474706405795723\n",
      "train loss:0.0016790242420485493\n",
      "train loss:0.0010725799146694944\n",
      "train loss:0.008599891457527025\n",
      "train loss:0.0011986634622511622\n",
      "train loss:0.003700835133785457\n",
      "train loss:0.0033632111895174326\n",
      "train loss:0.007511510985970964\n",
      "train loss:0.001819529393291471\n",
      "train loss:0.0002571323669217645\n",
      "train loss:0.005779571699005173\n",
      "train loss:0.00018841819133372662\n",
      "train loss:0.005720912605094313\n",
      "train loss:0.000541081645318693\n",
      "train loss:0.0038363680223962144\n",
      "train loss:0.005612839156521107\n",
      "train loss:0.002371567999009488\n",
      "train loss:0.001043773038756037\n",
      "train loss:0.0026425985558932515\n",
      "train loss:0.003381610392611893\n",
      "train loss:0.0009081902227279444\n",
      "train loss:0.00391063931621467\n",
      "train loss:0.0005093835912893971\n",
      "train loss:0.0016057791077449194\n",
      "train loss:0.0003683564848061085\n",
      "train loss:0.00381735561143704\n",
      "train loss:0.0005208054882762518\n",
      "train loss:0.0006176890932258281\n",
      "train loss:0.0009830108830956499\n",
      "train loss:0.003486445836613467\n",
      "train loss:0.0017365401047574273\n",
      "train loss:0.0007530559174008676\n",
      "train loss:0.050597471759577475\n",
      "train loss:0.00010513723164631485\n",
      "train loss:0.0004173405401192195\n",
      "train loss:0.0010928906789583485\n",
      "train loss:0.002111479799072288\n",
      "train loss:0.001166238753850062\n",
      "train loss:0.004323579525958658\n",
      "train loss:0.00045089667977490396\n",
      "train loss:0.003727320306418524\n",
      "train loss:0.0018290927182273323\n",
      "train loss:0.002808341053030388\n",
      "train loss:0.012495812070671915\n",
      "train loss:0.001320102698483157\n",
      "train loss:0.0007730396456960891\n",
      "train loss:0.0005749305494574688\n",
      "train loss:0.00029797208944782656\n",
      "train loss:0.003629775681293529\n",
      "train loss:0.015786380283378827\n",
      "train loss:0.00851869459315068\n",
      "train loss:0.0027408684525624684\n",
      "train loss:0.0007224645838616227\n",
      "train loss:0.00031245767166835183\n",
      "train loss:0.022138086449786903\n",
      "train loss:0.0016325603681802666\n",
      "train loss:0.004332782643139528\n",
      "train loss:0.0025906801969888525\n",
      "train loss:0.0019354347669648336\n",
      "train loss:0.0004939296519484906\n",
      "train loss:0.006049601823072229\n",
      "train loss:0.004965172819862801\n",
      "train loss:0.02043103804683723\n",
      "train loss:0.0025413236552791884\n",
      "train loss:0.0033290832031424894\n",
      "train loss:0.004443811144243741\n",
      "train loss:0.0020725340559026345\n",
      "train loss:0.001918258831953153\n",
      "train loss:0.006465441311478672\n",
      "train loss:0.0016659517972948666\n",
      "train loss:0.006794824549533625\n",
      "train loss:0.011811871613248823\n",
      "train loss:0.002540745108636967\n",
      "train loss:0.009303082530768815\n",
      "train loss:0.0022168198535845833\n",
      "train loss:0.00258627493026902\n",
      "train loss:0.001496272065436766\n",
      "train loss:0.008368404382998965\n",
      "train loss:0.0009966171429792539\n",
      "train loss:0.0004547516305715384\n",
      "train loss:0.010558153915975265\n",
      "train loss:0.002493862737095723\n",
      "train loss:0.004495867348526382\n",
      "train loss:0.0013762108742003292\n",
      "train loss:0.0017926838222402299\n",
      "train loss:0.006683442034345103\n",
      "train loss:0.0012092538816852098\n",
      "train loss:0.0021749358877489232\n",
      "train loss:0.00849999390534264\n",
      "train loss:0.005203229807709293\n",
      "train loss:0.000388422623307052\n",
      "train loss:0.0013076740241757335\n",
      "train loss:0.003375939408738401\n",
      "train loss:0.001477070430319554\n",
      "train loss:0.003480047376745215\n",
      "train loss:0.0003185443269427161\n",
      "train loss:0.00299118337062624\n",
      "train loss:0.005167258676687246\n",
      "train loss:0.004045617017183534\n",
      "train loss:0.008824041994774146\n",
      "train loss:0.006481762951812204\n",
      "train loss:0.0008710546378550827\n",
      "train loss:0.0007566558938857887\n",
      "train loss:0.0035675938490840114\n",
      "train loss:0.0030650798440268984\n",
      "train loss:0.0032107726804519766\n",
      "train loss:0.01139174615831976\n",
      "train loss:0.008075796418657017\n",
      "train loss:0.00324321740155249\n",
      "train loss:0.004085919412498078\n",
      "train loss:0.0026320561978386823\n",
      "train loss:0.01753540496861791\n",
      "train loss:4.628751869211678e-05\n",
      "train loss:0.002903164173398302\n",
      "train loss:0.0008995435297988591\n",
      "train loss:0.00622641536459591\n",
      "train loss:0.006143231958629769\n",
      "train loss:0.015638731323218472\n",
      "train loss:0.013671299479522066\n",
      "train loss:0.0012385177852246188\n",
      "train loss:0.001965739003587106\n",
      "train loss:0.010510778031353576\n",
      "train loss:0.0058228264295996\n",
      "train loss:0.0047025915032966\n",
      "train loss:0.0021618237802758316\n",
      "train loss:0.0005410104487096294\n",
      "train loss:0.00044969681086972745\n",
      "train loss:0.0025649190790278664\n",
      "train loss:0.0018310828981529454\n",
      "train loss:0.0035383298349694657\n",
      "train loss:0.00013527166890754875\n",
      "train loss:0.004428618182594543\n",
      "train loss:0.0035115335794663484\n",
      "train loss:0.0030444776564690825\n",
      "train loss:0.0005250316214010176\n",
      "train loss:0.0007755159976415557\n",
      "train loss:0.005201331344188448\n",
      "train loss:0.005453886697871673\n",
      "train loss:0.0046611373503470796\n",
      "train loss:0.003795622702063559\n",
      "train loss:0.0001494087215766995\n",
      "train loss:0.012838981056424954\n",
      "train loss:0.00031915742914197335\n",
      "train loss:0.000384084629960354\n",
      "train loss:0.007291011487990843\n",
      "train loss:0.0008179103572030165\n",
      "train loss:0.0009212634971855154\n",
      "train loss:0.0025049203895315263\n",
      "train loss:0.00120141796887837\n",
      "train loss:0.0008452715620097394\n",
      "train loss:0.0022222726994003835\n",
      "train loss:0.013874247669924167\n",
      "train loss:0.011724569530268143\n",
      "train loss:0.003219452132776367\n",
      "train loss:0.006505487219863329\n",
      "train loss:0.005711177625036891\n",
      "train loss:0.016172805994292015\n",
      "train loss:0.01799402043607096\n",
      "train loss:0.011083002192499834\n",
      "train loss:0.002592166731005907\n",
      "train loss:0.004752954618206118\n",
      "train loss:0.01140033173744817\n",
      "train loss:0.0034177373915772686\n",
      "train loss:0.0014189325692250312\n",
      "train loss:0.00037934512808815364\n",
      "train loss:0.00040027407940736994\n",
      "train loss:0.008330403685172657\n",
      "train loss:0.0017560577439228787\n",
      "train loss:0.025391384100660167\n",
      "train loss:0.00882469008110361\n",
      "train loss:0.0008649229829648048\n",
      "train loss:0.003781353970312641\n",
      "train loss:0.0010100199228151858\n",
      "train loss:0.0002907783619838835\n",
      "train loss:0.003971147887172181\n",
      "train loss:0.0017814810565542883\n",
      "train loss:0.0008480400544542857\n",
      "train loss:0.0033194815108885957\n",
      "train loss:0.006079432943476833\n",
      "train loss:0.05084110577475972\n",
      "train loss:0.0013002488680280636\n",
      "train loss:0.007831284960973798\n",
      "train loss:0.015609729270898069\n",
      "train loss:0.000513534384446084\n",
      "train loss:0.0008841852701350133\n",
      "train loss:0.002432654806359579\n",
      "train loss:0.002653171926095499\n",
      "train loss:0.003305397926348364\n",
      "train loss:0.0008999354306693513\n",
      "train loss:0.0018097768481585535\n",
      "train loss:0.001548772049223649\n",
      "train loss:0.0005094488402632346\n",
      "train loss:0.02319150685244662\n",
      "train loss:0.0013922826460213286\n",
      "train loss:0.0028822106569627508\n",
      "train loss:0.0017129942141974803\n",
      "train loss:0.001353967016816014\n",
      "train loss:0.00519914686612619\n",
      "train loss:0.03192373603519999\n",
      "train loss:0.004507055304847968\n",
      "train loss:0.00031655972130314585\n",
      "train loss:0.0023891056424118974\n",
      "train loss:0.0015483526783193948\n",
      "train loss:0.005008678551060831\n",
      "train loss:0.0021531780039578236\n",
      "train loss:0.011106525960474947\n",
      "train loss:0.000481407731277755\n",
      "train loss:0.006235521964933267\n",
      "train loss:0.0213193962492118\n",
      "train loss:0.002097469482435386\n",
      "train loss:0.0024474275561333046\n",
      "train loss:0.002722280633212486\n",
      "train loss:0.0018937682503802\n",
      "train loss:0.010318914535294732\n",
      "train loss:0.0020991983520460868\n",
      "train loss:0.0006738833414996103\n",
      "train loss:0.0025841931700231946\n",
      "train loss:0.00893213344409687\n",
      "train loss:0.0036721631475868733\n",
      "train loss:0.00437773061950055\n",
      "train loss:0.006441438632277688\n",
      "train loss:0.0021307080902035185\n",
      "train loss:0.0029665821320715457\n",
      "train loss:0.003129985137090484\n",
      "train loss:0.003053002491905436\n",
      "train loss:0.0009438182455186154\n",
      "train loss:0.00115825674493617\n",
      "train loss:0.004780634023364957\n",
      "train loss:0.0010270376989124244\n",
      "train loss:0.00020790825445111182\n",
      "train loss:0.004274349460653316\n",
      "train loss:0.004167197536341414\n",
      "train loss:0.0009773346768735308\n",
      "train loss:0.0023379032663142334\n",
      "train loss:0.0005043392393189105\n",
      "train loss:0.001904536257793493\n",
      "train loss:0.003118596031941038\n",
      "train loss:0.0007451647642327111\n",
      "train loss:0.003782095057204853\n",
      "train loss:0.00590082748150377\n",
      "train loss:0.02500741453914665\n",
      "train loss:0.0044461610019440375\n",
      "train loss:0.008959918309152133\n",
      "train loss:0.004132467284458918\n",
      "train loss:0.0038721103194163165\n",
      "train loss:0.0010188767796754578\n",
      "train loss:0.0008710280374588365\n",
      "train loss:0.002742725142482641\n",
      "train loss:0.006551013015201823\n",
      "train loss:0.00782398946678589\n",
      "train loss:0.0011727807495522655\n",
      "train loss:0.003555365419075387\n",
      "train loss:0.0027827826080151974\n",
      "train loss:0.0004563666417763609\n",
      "train loss:0.0003736311281389386\n",
      "train loss:0.0023394684749422348\n",
      "train loss:0.016805730139242157\n",
      "train loss:0.00355612653593488\n",
      "train loss:0.00099640415730113\n",
      "train loss:0.0043325200329114265\n",
      "train loss:0.005289266437159403\n",
      "train loss:0.0024783464961727493\n",
      "train loss:0.005280299980193399\n",
      "train loss:0.0007651744599954809\n",
      "train loss:0.0009837965650241305\n",
      "train loss:0.0002061543873730395\n",
      "train loss:0.00032747293019496656\n",
      "train loss:0.0032624192396885606\n",
      "train loss:0.009392480536982694\n",
      "train loss:0.00016407271443258863\n",
      "train loss:0.0006144888856744398\n",
      "train loss:0.00028770295451954883\n",
      "=== epoch:14, train acc:0.998, test acc:0.987 ===\n",
      "train loss:0.0011568884566465004\n",
      "train loss:0.004248597000137147\n",
      "train loss:0.0017556988445729194\n",
      "train loss:0.0015915880977757898\n",
      "train loss:0.0027849750669996487\n",
      "train loss:0.0068386167218836445\n",
      "train loss:0.016744461556482588\n",
      "train loss:0.003350978093219753\n",
      "train loss:0.0011840613134004748\n",
      "train loss:0.0057215584486254726\n",
      "train loss:0.0006669927789120455\n",
      "train loss:0.012079788007623972\n",
      "train loss:0.002785995241986385\n",
      "train loss:0.0016965568809552767\n",
      "train loss:0.0033710312233367064\n",
      "train loss:0.0008619768169945106\n",
      "train loss:0.0014379834187723502\n",
      "train loss:0.05473370603123047\n",
      "train loss:0.0032298956560627905\n",
      "train loss:0.002725139640297795\n",
      "train loss:0.000623071112866816\n",
      "train loss:0.002122858719544595\n",
      "train loss:0.004610874679872278\n",
      "train loss:0.0025992703865288062\n",
      "train loss:0.002804263947475738\n",
      "train loss:0.002601570891254038\n",
      "train loss:0.0030035425097302156\n",
      "train loss:0.00549849927290586\n",
      "train loss:0.004020046438108022\n",
      "train loss:0.0025201347509146684\n",
      "train loss:0.001961093861386146\n",
      "train loss:0.0003914890624110396\n",
      "train loss:0.0013605547664372645\n",
      "train loss:0.0018617032723679627\n",
      "train loss:0.013712907105765205\n",
      "train loss:0.0035801538993636053\n",
      "train loss:0.0006435601137109789\n",
      "train loss:0.0005908455683395564\n",
      "train loss:0.005797812090816725\n",
      "train loss:0.0031422014588792044\n",
      "train loss:0.00537018348055669\n",
      "train loss:0.003287198133386486\n",
      "train loss:0.0003282886675783861\n",
      "train loss:0.00047784041050806677\n",
      "train loss:0.004219719842663825\n",
      "train loss:0.00288277879707409\n",
      "train loss:0.0011410040771575255\n",
      "train loss:0.0016009224580754403\n",
      "train loss:0.006052996162546105\n",
      "train loss:0.0020605382863877536\n",
      "train loss:0.0017564175534290707\n",
      "train loss:0.002192289667902765\n",
      "train loss:0.003020647063718944\n",
      "train loss:6.013882955472405e-05\n",
      "train loss:0.002769575520964105\n",
      "train loss:0.0014091951517347075\n",
      "train loss:0.00039083652364421163\n",
      "train loss:0.001328582023679232\n",
      "train loss:0.0016057773968134895\n",
      "train loss:6.752333363216749e-05\n",
      "train loss:0.003803881750721936\n",
      "train loss:0.0019243318745085255\n",
      "train loss:0.0002108346968058241\n",
      "train loss:0.027088473946763898\n",
      "train loss:0.0023037561627724796\n",
      "train loss:0.006067109496151785\n",
      "train loss:0.0004918423712473718\n",
      "train loss:0.0020963516941290544\n",
      "train loss:0.00035645713642013714\n",
      "train loss:0.0037834516824360076\n",
      "train loss:0.0016681725000266602\n",
      "train loss:0.0089457582081993\n",
      "train loss:0.006837230189794653\n",
      "train loss:0.004489517384727011\n",
      "train loss:0.0008589933622756062\n",
      "train loss:0.0016111106536537403\n",
      "train loss:0.003342578038376604\n",
      "train loss:0.001157557089217528\n",
      "train loss:0.008562112727967482\n",
      "train loss:0.0020058870053226287\n",
      "train loss:0.002095924691013958\n",
      "train loss:0.002617698335180095\n",
      "train loss:0.009443468760382776\n",
      "train loss:0.007078857516526098\n",
      "train loss:0.0024641066493872976\n",
      "train loss:0.003865747359066757\n",
      "train loss:0.0004090460803800511\n",
      "train loss:0.003718601861680967\n",
      "train loss:0.0023257068613201813\n",
      "train loss:0.0010121448018997627\n",
      "train loss:0.006607223197422013\n",
      "train loss:0.004256430734363859\n",
      "train loss:0.001519146393290155\n",
      "train loss:0.008501593265997144\n",
      "train loss:0.0034616147554465764\n",
      "train loss:0.009763815624378265\n",
      "train loss:0.007853947989938983\n",
      "train loss:0.0311940389434008\n",
      "train loss:0.0012403424028664739\n",
      "train loss:0.0056136705956774755\n",
      "train loss:0.0013451807302851742\n",
      "train loss:0.003160611393797264\n",
      "train loss:0.005570519995177163\n",
      "train loss:0.01945730736397612\n",
      "train loss:0.009155554884738017\n",
      "train loss:0.003091838534239127\n",
      "train loss:0.0005577896401482193\n",
      "train loss:0.0011320058239522547\n",
      "train loss:0.0007837969761131178\n",
      "train loss:0.0036524776641600233\n",
      "train loss:0.0028116160697402985\n",
      "train loss:0.005855868762443531\n",
      "train loss:0.0028754919246898034\n",
      "train loss:0.0012490101752417853\n",
      "train loss:0.007805607042672311\n",
      "train loss:0.003041256899514377\n",
      "train loss:0.0017121198767895644\n",
      "train loss:0.016497910877488527\n",
      "train loss:0.0025136648905089982\n",
      "train loss:0.0013386383181837682\n",
      "train loss:0.0028247065105544466\n",
      "train loss:0.002211562598883385\n",
      "train loss:0.0029095968402570267\n",
      "train loss:0.004927790967430178\n",
      "train loss:0.003522894868044303\n",
      "train loss:0.0017803806331308963\n",
      "train loss:0.0003800223026473166\n",
      "train loss:0.004624782195855761\n",
      "train loss:0.009280129926447257\n",
      "train loss:0.002558119148385551\n",
      "train loss:0.0031606762577617185\n",
      "train loss:0.0038844792531980005\n",
      "train loss:0.0024163050076912314\n",
      "train loss:0.0019493798053657127\n",
      "train loss:0.0013954654634433584\n",
      "train loss:0.0027768494227783085\n",
      "train loss:0.0018903279512151605\n",
      "train loss:0.004217687383457547\n",
      "train loss:0.0032755970876941713\n",
      "train loss:0.0027593644899782787\n",
      "train loss:0.002110749223355439\n",
      "train loss:0.001102347546421037\n",
      "train loss:0.0033137901388766745\n",
      "train loss:0.016953196235676482\n",
      "train loss:0.0016343026035398707\n",
      "train loss:0.0025161038697714285\n",
      "train loss:0.004241724159544134\n",
      "train loss:0.003341880213454604\n",
      "train loss:0.0002352526329778957\n",
      "train loss:0.0030848313525991705\n",
      "train loss:0.008101165016318246\n",
      "train loss:0.0033958143688623886\n",
      "train loss:0.000884061664398204\n",
      "train loss:0.002964877352104966\n",
      "train loss:0.0011202776014608271\n",
      "train loss:0.0017163901586270266\n",
      "train loss:0.0017023515105337528\n",
      "train loss:0.003508091131350535\n",
      "train loss:0.003915932667548723\n",
      "train loss:0.0006733196435750316\n",
      "train loss:0.008075472300275355\n",
      "train loss:0.0008501233783009919\n",
      "train loss:0.0008470468463580408\n",
      "train loss:0.0033827976053104358\n",
      "train loss:0.001885398951240428\n",
      "train loss:0.0046138516857891195\n",
      "train loss:0.0008495083249708557\n",
      "train loss:0.001094737893588322\n",
      "train loss:0.0010724304644333027\n",
      "train loss:0.0013176218942884785\n",
      "train loss:0.003708308462367692\n",
      "train loss:0.0007897284214482444\n",
      "train loss:0.0034861829613806374\n",
      "train loss:0.002087409192478083\n",
      "train loss:0.007780693009809903\n",
      "train loss:0.024192820104855028\n",
      "train loss:0.0006600214127870676\n",
      "train loss:0.009415653659607975\n",
      "train loss:0.0025689359716979665\n",
      "train loss:0.00393205111630978\n",
      "train loss:0.004172948674900163\n",
      "train loss:0.0011637889432505096\n",
      "train loss:0.007324577274317775\n",
      "train loss:0.00294394325135058\n",
      "train loss:0.005615182783102213\n",
      "train loss:0.004827077242100221\n",
      "train loss:0.0012066797147043647\n",
      "train loss:0.0002991977914758603\n",
      "train loss:0.01710430918898812\n",
      "train loss:0.0056888531408884465\n",
      "train loss:0.0019677117246079097\n",
      "train loss:0.0006429142861498548\n",
      "train loss:0.0038560383853353136\n",
      "train loss:0.004988486307610886\n",
      "train loss:0.0008758168724100138\n",
      "train loss:0.0034118844685346104\n",
      "train loss:0.00276734656201401\n",
      "train loss:0.004943577549296218\n",
      "train loss:0.003105992971766532\n",
      "train loss:0.002306433722862823\n",
      "train loss:0.002256697667721418\n",
      "train loss:0.008091839360414674\n",
      "train loss:0.0013692193972600463\n",
      "train loss:0.0006776253989389325\n",
      "train loss:0.004085281630920874\n",
      "train loss:0.002969809237882412\n",
      "train loss:0.0012424110432374617\n",
      "train loss:0.0014771830115484616\n",
      "train loss:0.001842975321445694\n",
      "train loss:0.01263875289210155\n",
      "train loss:0.004305748390951828\n",
      "train loss:0.0030694209069872165\n",
      "train loss:0.00186512310292181\n",
      "train loss:0.0011726262548460303\n",
      "train loss:0.041779653917753484\n",
      "train loss:0.0027801057648646093\n",
      "train loss:0.0043797500368110195\n",
      "train loss:0.003127654491156296\n",
      "train loss:0.002791601359307687\n",
      "train loss:0.005090606873609951\n",
      "train loss:0.0017520245896064368\n",
      "train loss:0.0017360224065932459\n",
      "train loss:0.0035529270728844615\n",
      "train loss:0.0005557457675346076\n",
      "train loss:0.010229079155710117\n",
      "train loss:0.002522009468992089\n",
      "train loss:0.005192401866554726\n",
      "train loss:0.0018137872058097027\n",
      "train loss:0.010320501189072432\n",
      "train loss:0.002775365012924586\n",
      "train loss:0.0006133271661320835\n",
      "train loss:0.00033085511424134676\n",
      "train loss:0.001427202535553742\n",
      "train loss:0.001413086294057093\n",
      "train loss:0.0017375602194682354\n",
      "train loss:0.00945846761435713\n",
      "train loss:0.001736189578934217\n",
      "train loss:0.0020422183072163844\n",
      "train loss:0.005133310522555521\n",
      "train loss:0.011267531835309142\n",
      "train loss:0.0022826486113835033\n",
      "train loss:0.0020875303415797135\n",
      "train loss:0.004906670290552358\n",
      "train loss:0.0014134579253347206\n",
      "train loss:0.0020073738059422394\n",
      "train loss:0.0007848497564937365\n",
      "train loss:0.0017695280469693028\n",
      "train loss:0.00016860852316158252\n",
      "train loss:0.0027659447424174966\n",
      "train loss:0.00039957630622075336\n",
      "train loss:0.0006587890242981646\n",
      "train loss:0.005307005169887524\n",
      "train loss:0.01681998770681752\n",
      "train loss:0.0016539643764621205\n",
      "train loss:0.004606727355722653\n",
      "train loss:0.0030342733041705755\n",
      "train loss:0.0009925251459272225\n",
      "train loss:0.0024779779168545317\n",
      "train loss:0.0024366436530930467\n",
      "train loss:0.003012574173835618\n",
      "train loss:0.0006552545476690881\n",
      "train loss:0.0004773548678306478\n",
      "train loss:0.0034874804675404042\n",
      "train loss:0.000240549196881551\n",
      "train loss:0.003605993201130982\n",
      "train loss:0.0021260365855910782\n",
      "train loss:0.003664153027889688\n",
      "train loss:0.0008250928279385525\n",
      "train loss:0.0032004490060653008\n",
      "train loss:0.0012891703740260288\n",
      "train loss:0.001239700765476066\n",
      "train loss:0.0031896881730372273\n",
      "train loss:2.292919568987544e-05\n",
      "train loss:0.007533650089174868\n",
      "train loss:0.0016878147814901954\n",
      "train loss:0.011688039031603472\n",
      "train loss:0.004910951480189882\n",
      "train loss:0.0025063310336668945\n",
      "train loss:0.0016781192677128923\n",
      "train loss:0.00032254267383153646\n",
      "train loss:0.0036189789078810946\n",
      "train loss:0.023458406225037365\n",
      "train loss:0.00014051552292041768\n",
      "train loss:0.0319530374226247\n",
      "train loss:0.002822060438979491\n",
      "train loss:9.826436871270633e-05\n",
      "train loss:0.0001484071122436057\n",
      "train loss:0.00363091457000915\n",
      "train loss:0.014329685873677027\n",
      "train loss:0.0010141897970039316\n",
      "train loss:0.0003850296002215134\n",
      "train loss:0.0007020601256694657\n",
      "train loss:0.0037320909526660827\n",
      "train loss:0.0026970538058112025\n",
      "train loss:0.0029494039167148544\n",
      "train loss:0.0005829616994850064\n",
      "train loss:0.0013897546169444402\n",
      "train loss:7.908051697546177e-05\n",
      "train loss:0.0036188372299810275\n",
      "train loss:0.00027916048553539735\n",
      "train loss:0.0009228061283253361\n",
      "train loss:0.0012794904000607895\n",
      "train loss:0.0002148182836966384\n",
      "train loss:0.015799450902229534\n",
      "train loss:0.0013899153698082977\n",
      "train loss:0.006568465621641173\n",
      "train loss:0.0005906151589113772\n",
      "train loss:0.0036528013988390328\n",
      "train loss:0.03493279044771546\n",
      "train loss:0.01793924395763382\n",
      "train loss:0.0016000287356724768\n",
      "train loss:0.00420090475771947\n",
      "train loss:0.006853036013798842\n",
      "train loss:0.009302141358782661\n",
      "train loss:0.0010088206417504725\n",
      "train loss:0.0015011539873991877\n",
      "train loss:0.000826351265905662\n",
      "train loss:0.0034305053988437427\n",
      "train loss:0.0024694155232124876\n",
      "train loss:0.00536273034384764\n",
      "train loss:0.025578359918234702\n",
      "train loss:0.0014729289756802435\n",
      "train loss:0.001253968191287833\n",
      "train loss:0.023919508025799393\n",
      "train loss:0.0018201796595613221\n",
      "train loss:0.0011238757319209892\n",
      "train loss:0.0007213146497208992\n",
      "train loss:0.002302407852152063\n",
      "train loss:0.0008950601960364659\n",
      "train loss:0.0031589345904997866\n",
      "train loss:0.00036765797242775607\n",
      "train loss:0.0023343271165054786\n",
      "train loss:0.004775796744658408\n",
      "train loss:0.0018448608040284422\n",
      "train loss:0.0029510726976744476\n",
      "train loss:0.0009469414148751535\n",
      "train loss:0.0023046314751191473\n",
      "train loss:0.0007218459895600099\n",
      "train loss:0.0016220928696448677\n",
      "train loss:0.003478103710422101\n",
      "train loss:0.0002806708113174085\n",
      "train loss:0.003136774244035735\n",
      "train loss:0.0004328826942219031\n",
      "train loss:0.0006719075572112724\n",
      "train loss:0.004727686115199868\n",
      "train loss:0.002395168623145463\n",
      "train loss:0.00426079121840989\n",
      "train loss:0.001294704906211298\n",
      "train loss:0.0021548488907362005\n",
      "train loss:0.0012223768775800879\n",
      "train loss:0.00034482157034475193\n",
      "train loss:0.0017030821883485378\n",
      "train loss:0.007605450904846125\n",
      "train loss:0.002109665513623166\n",
      "train loss:0.0033647649096546924\n",
      "train loss:0.0003346520552536358\n",
      "train loss:0.0012231784338763675\n",
      "train loss:0.0019389520518469003\n",
      "train loss:0.0013630560979039034\n",
      "train loss:0.0006880408448759216\n",
      "train loss:0.0025177084544397072\n",
      "train loss:0.012027148320635252\n",
      "train loss:0.0005638600060810444\n",
      "train loss:0.004942929841000534\n",
      "train loss:0.0017920383641550673\n",
      "train loss:0.0022010034733059135\n",
      "train loss:0.005560598555092999\n",
      "train loss:0.0012778207382051197\n",
      "train loss:0.0004090727050552304\n",
      "train loss:0.002583190857562838\n",
      "train loss:0.0017172932442303618\n",
      "train loss:0.00893153301305315\n",
      "train loss:0.00156372067024313\n",
      "train loss:0.008127970743261355\n",
      "train loss:0.0010794424910742792\n",
      "train loss:0.004599609800753624\n",
      "train loss:0.00021826850260980803\n",
      "train loss:0.0009399780584991917\n",
      "train loss:0.002512612704032365\n",
      "train loss:0.0016032187175471246\n",
      "train loss:0.0005647169992792736\n",
      "train loss:0.0012383311288695425\n",
      "train loss:0.0010057631266460485\n",
      "train loss:6.776039280767384e-05\n",
      "train loss:0.019383680116912782\n",
      "train loss:0.0013045774480534786\n",
      "train loss:0.0034108484172208154\n",
      "train loss:0.0019372478372995223\n",
      "train loss:0.004193997267133646\n",
      "train loss:0.0018193063060273629\n",
      "train loss:0.0006330069464528172\n",
      "train loss:0.00465365485473904\n",
      "train loss:0.018112992639304264\n",
      "train loss:0.000979914927514194\n",
      "train loss:0.0010835318003150777\n",
      "train loss:0.0022254975334817825\n",
      "train loss:0.00307216640695896\n",
      "train loss:0.006152046847163913\n",
      "train loss:0.007362339671858567\n",
      "train loss:0.0023461122040854252\n",
      "train loss:0.00011486670736664484\n",
      "train loss:0.004330054847569424\n",
      "train loss:0.006356826277636588\n",
      "train loss:0.00445021756037491\n",
      "train loss:0.000476573310951987\n",
      "train loss:0.0004770902987443177\n",
      "train loss:0.00039803118203098254\n",
      "train loss:0.004098916232155277\n",
      "train loss:0.0016018178297539042\n",
      "train loss:0.0017366097227462908\n",
      "train loss:0.004590540335571163\n",
      "train loss:4.8857099625057976e-05\n",
      "train loss:0.0026625989152288627\n",
      "train loss:0.002017775359775917\n",
      "train loss:0.0001267728587951314\n",
      "train loss:0.0013403256931110571\n",
      "train loss:0.0019685234039283636\n",
      "train loss:0.0026476176304780887\n",
      "train loss:0.0004659803413826862\n",
      "train loss:0.0006642297211912319\n",
      "train loss:0.0010826244656754402\n",
      "train loss:0.0013483686399228547\n",
      "train loss:0.006076998469805532\n",
      "train loss:0.00032889899717698237\n",
      "train loss:0.0023067234017566333\n",
      "train loss:0.00126243237284309\n",
      "train loss:0.0024899170960703053\n",
      "train loss:0.0004383940205843255\n",
      "train loss:0.002251865693162325\n",
      "train loss:0.00032521216521612227\n",
      "train loss:0.0033797638367358924\n",
      "train loss:0.0045642113168579326\n",
      "train loss:0.0009977608994281755\n",
      "train loss:0.001791381491809661\n",
      "train loss:0.0002007042734531423\n",
      "train loss:0.003531698606361435\n",
      "train loss:0.0011498165377605916\n",
      "train loss:0.00012939679495337856\n",
      "train loss:0.007078043746131919\n",
      "train loss:0.005625509037856979\n",
      "train loss:0.0007742938767948096\n",
      "train loss:0.0035787664669704694\n",
      "train loss:0.003235510520341701\n",
      "train loss:0.013978703124403137\n",
      "train loss:0.0008964030522543295\n",
      "train loss:0.0008415956765060562\n",
      "train loss:0.001329467449499648\n",
      "train loss:0.002326482492882588\n",
      "train loss:0.010586459368837236\n",
      "train loss:0.0015338521720161201\n",
      "train loss:0.0091617425868126\n",
      "train loss:0.003977006507458082\n",
      "train loss:0.019979788695190473\n",
      "train loss:0.02491248212749255\n",
      "train loss:0.0012562696034398551\n",
      "train loss:0.005814031060637024\n",
      "train loss:0.0061703584251869645\n",
      "train loss:0.001652407056051494\n",
      "train loss:0.002133865814101134\n",
      "train loss:0.0005490019224623115\n",
      "train loss:0.00010366538956760989\n",
      "train loss:0.003935589175218808\n",
      "train loss:0.0005431534919766217\n",
      "train loss:0.013402600600462118\n",
      "train loss:0.00228828550023072\n",
      "train loss:0.006665680794409283\n",
      "train loss:0.0009570325139753558\n",
      "train loss:0.0005494756253361607\n",
      "train loss:0.02906888309694096\n",
      "train loss:0.0003014449121243573\n",
      "train loss:0.0008947919804334115\n",
      "train loss:0.0014513890454124134\n",
      "train loss:0.005354715600856534\n",
      "train loss:0.0029703687526972254\n",
      "train loss:0.000458727096094955\n",
      "train loss:0.0004470869423366393\n",
      "train loss:0.001482241193139364\n",
      "train loss:0.0019554910882107037\n",
      "train loss:0.0006975504623070802\n",
      "train loss:0.004395109219575645\n",
      "train loss:0.00020878385354727237\n",
      "train loss:0.0005328829990823294\n",
      "train loss:0.005698031568518079\n",
      "train loss:0.0010976850123597113\n",
      "train loss:0.0011576666424775934\n",
      "train loss:0.0031793125794475096\n",
      "train loss:0.002970214085074896\n",
      "train loss:0.0031778280763033145\n",
      "train loss:0.0004827298227718564\n",
      "train loss:0.0123297889138471\n",
      "train loss:0.004667180489745694\n",
      "train loss:0.0016652478905219523\n",
      "train loss:0.0029794035301892565\n",
      "train loss:0.007194525958935408\n",
      "train loss:0.002115113980595243\n",
      "train loss:0.0023257640735249354\n",
      "train loss:0.001737346216587812\n",
      "train loss:0.0002744673540391824\n",
      "train loss:0.0005409245598875155\n",
      "train loss:0.00019049831841507097\n",
      "train loss:0.0002176420806108994\n",
      "train loss:0.007121947870375527\n",
      "train loss:0.0006253699662037442\n",
      "train loss:0.005887278200483448\n",
      "train loss:0.006123466025602522\n",
      "train loss:0.001484095449851527\n",
      "train loss:0.005905016235004254\n",
      "train loss:0.001254819879638506\n",
      "train loss:0.0010817860265773173\n",
      "train loss:0.005659913686862394\n",
      "train loss:0.0009045002253672236\n",
      "train loss:0.0037811943212353533\n",
      "train loss:0.0004216533222511321\n",
      "train loss:0.00037342268881354536\n",
      "train loss:0.06317339272403281\n",
      "train loss:0.0013499755076457549\n",
      "train loss:0.0014466652118470572\n",
      "train loss:0.004146858419248318\n",
      "train loss:0.0014690974187852091\n",
      "train loss:0.0020112613367312183\n",
      "train loss:0.0004158029352674273\n",
      "train loss:0.04505051571638516\n",
      "train loss:0.0009674261101782688\n",
      "train loss:0.0022902739575838575\n",
      "train loss:0.005831772504296448\n",
      "train loss:0.01107525908773517\n",
      "train loss:0.005774780581767295\n",
      "train loss:0.010403275098682385\n",
      "train loss:0.014273913209271308\n",
      "train loss:0.0023252261670374482\n",
      "train loss:0.0033358489074648058\n",
      "train loss:0.00019332829332527573\n",
      "train loss:0.006766577907391847\n",
      "train loss:0.0008251466508703506\n",
      "train loss:0.0007829077056820484\n",
      "train loss:0.0028504171354497504\n",
      "train loss:0.003103517819936657\n",
      "train loss:0.006639968778294551\n",
      "train loss:0.006701933039219058\n",
      "train loss:0.00265598301278388\n",
      "train loss:0.002621241082272982\n",
      "train loss:0.004830033189739068\n",
      "train loss:0.001986469474769114\n",
      "train loss:0.002723036902108677\n",
      "train loss:0.0006253671384265692\n",
      "train loss:0.0006837883515607991\n",
      "train loss:0.005766396885773656\n",
      "train loss:0.001173960162291729\n",
      "train loss:0.0037679412581013345\n",
      "train loss:0.0025762090788563428\n",
      "train loss:0.000904236583435507\n",
      "train loss:0.0012515214415343776\n",
      "train loss:0.002143885663993603\n",
      "train loss:0.0017232620448165096\n",
      "train loss:0.0012299769168739754\n",
      "train loss:0.004281416812952105\n",
      "train loss:0.0009133726587591223\n",
      "train loss:0.008693558064151247\n",
      "train loss:0.0019861052169135025\n",
      "train loss:0.011629681246844808\n",
      "train loss:0.0009694651343910859\n",
      "train loss:0.0034192275298262163\n",
      "train loss:0.0005988585763987176\n",
      "train loss:0.0011186882103604416\n",
      "train loss:0.001121852605200572\n",
      "train loss:0.004851700847798558\n",
      "train loss:0.007855713855748686\n",
      "train loss:0.0023175500083310978\n",
      "train loss:0.004753077405800542\n",
      "train loss:0.0014950231430838517\n",
      "train loss:0.0037552363418426006\n",
      "train loss:0.00202122064107478\n",
      "train loss:0.002384222580775583\n",
      "train loss:0.0063831207039885315\n",
      "train loss:0.000458019549343302\n",
      "train loss:0.0009262926981951387\n",
      "train loss:0.023242200963526995\n",
      "train loss:0.0021281210185811724\n",
      "train loss:0.006020886513375199\n",
      "train loss:0.00020081097647465344\n",
      "train loss:0.0005631462904107038\n",
      "train loss:0.0012827353687270037\n",
      "train loss:0.0011326209136717801\n",
      "train loss:0.0025797387976284063\n",
      "train loss:0.003044655837806187\n",
      "train loss:0.0037517825247900274\n",
      "train loss:0.001650986409506279\n",
      "train loss:0.0007602318004151788\n",
      "train loss:0.010962998261654065\n",
      "train loss:0.001410292461486884\n",
      "train loss:0.00019215961973814865\n",
      "train loss:0.0011679607016934087\n",
      "train loss:0.0013717180269439354\n",
      "train loss:0.001355691876287852\n",
      "train loss:0.0007494727786398153\n",
      "train loss:0.0007681189097877818\n",
      "train loss:0.0024297488016617346\n",
      "train loss:0.0014900873673380386\n",
      "train loss:0.0012456209980670867\n",
      "train loss:0.0045110242737723474\n",
      "=== epoch:15, train acc:0.998, test acc:0.982 ===\n",
      "train loss:0.0006600254632952901\n",
      "train loss:0.0011075597115850636\n",
      "train loss:0.003210986626363062\n",
      "train loss:0.001241355452376701\n",
      "train loss:0.001373011198571674\n",
      "train loss:0.0021762334668046506\n",
      "train loss:0.00016681537491760684\n",
      "train loss:0.0004496551426522477\n",
      "train loss:0.0027541173492989152\n",
      "train loss:0.009347838577528543\n",
      "train loss:7.112868174951055e-05\n",
      "train loss:0.00039725993533010873\n",
      "train loss:0.005763166254748177\n",
      "train loss:0.00039876104756049387\n",
      "train loss:0.0027549563257728924\n",
      "train loss:0.0019259210404680318\n",
      "train loss:0.0014671451307387354\n",
      "train loss:0.002928317851126364\n",
      "train loss:0.0006654809522754348\n",
      "train loss:0.003921216020454717\n",
      "train loss:0.00047591836663669147\n",
      "train loss:0.001749018088008194\n",
      "train loss:0.0031815683527069453\n",
      "train loss:0.005580418290031752\n",
      "train loss:0.0013939730907653887\n",
      "train loss:0.00011104503777863801\n",
      "train loss:0.000710743771344835\n",
      "train loss:0.00033205917718191096\n",
      "train loss:0.0003259555546995802\n",
      "train loss:0.0011417629699635393\n",
      "train loss:0.0002597745446196042\n",
      "train loss:0.0019440158885123287\n",
      "train loss:0.006036733218532463\n",
      "train loss:0.0023371540769571574\n",
      "train loss:0.0013131522722433899\n",
      "train loss:0.0031034546773308063\n",
      "train loss:0.0003133816356935992\n",
      "train loss:0.0009205032165312234\n",
      "train loss:0.0007297228873515719\n",
      "train loss:7.299532546319859e-05\n",
      "train loss:0.001378775880517689\n",
      "train loss:5.26903120694224e-05\n",
      "train loss:0.001501828866695818\n",
      "train loss:5.9884368186364465e-05\n",
      "train loss:0.0015504085651244731\n",
      "train loss:0.0010620551506681604\n",
      "train loss:0.0037229119195763545\n",
      "train loss:0.00308109467554771\n",
      "train loss:0.0020112010204127615\n",
      "train loss:0.0012487001140613324\n",
      "train loss:0.006172254908092455\n",
      "train loss:0.000949053001781481\n",
      "train loss:0.0001914975164493796\n",
      "train loss:0.002838386095622033\n",
      "train loss:0.00391536706525339\n",
      "train loss:0.002458257715778795\n",
      "train loss:0.00030097756098746673\n",
      "train loss:0.00026640649198184254\n",
      "train loss:0.0002682232502945293\n",
      "train loss:0.0013969165539686184\n",
      "train loss:0.0041380552110537585\n",
      "train loss:0.0003410159790571806\n",
      "train loss:0.0011688109377813217\n",
      "train loss:0.003689984612809834\n",
      "train loss:0.0003264141111281049\n",
      "train loss:0.0035519509198899406\n",
      "train loss:0.00018066258303881444\n",
      "train loss:0.000607401460187599\n",
      "train loss:0.00013235883886491805\n",
      "train loss:0.014448950744517533\n",
      "train loss:0.0011747892281545262\n",
      "train loss:0.00027435562279049723\n",
      "train loss:0.002668458795155773\n",
      "train loss:0.0013045833732233379\n",
      "train loss:0.00036128951356832447\n",
      "train loss:0.00039690872271217105\n",
      "train loss:0.0032509460446774156\n",
      "train loss:0.0021538608263459475\n",
      "train loss:0.0005253784543439178\n",
      "train loss:0.002160428277748502\n",
      "train loss:0.0004812628799575816\n",
      "train loss:0.002005966394846494\n",
      "train loss:0.0024507524821453693\n",
      "train loss:0.0012780152057800009\n",
      "train loss:0.0009337969933711965\n",
      "train loss:0.0009164917652790005\n",
      "train loss:0.011156909176200774\n",
      "train loss:0.0007929333174157397\n",
      "train loss:0.0016177150014921563\n",
      "train loss:0.0016022761048518858\n",
      "train loss:0.0018320767615093454\n",
      "train loss:0.00160327370194166\n",
      "train loss:0.0016080885097465563\n",
      "train loss:0.0006249163207327393\n",
      "train loss:0.011800671899533632\n",
      "train loss:0.007969706647985207\n",
      "train loss:0.0017308205044621711\n",
      "train loss:0.0007031814240586867\n",
      "train loss:0.00393078015378956\n",
      "train loss:0.0042605173669562615\n",
      "train loss:0.004910893430888619\n",
      "train loss:0.0005681076042173966\n",
      "train loss:0.00017499078330213963\n",
      "train loss:0.0003969341515521531\n",
      "train loss:0.0015979350805097395\n",
      "train loss:0.0012545065739335659\n",
      "train loss:3.986691553087415e-05\n",
      "train loss:0.0005803144154553184\n",
      "train loss:0.006194690933189511\n",
      "train loss:0.0024460794537930976\n",
      "train loss:0.001302804354550045\n",
      "train loss:0.008238688731642397\n",
      "train loss:0.0014380464256092344\n",
      "train loss:0.00020205867360426552\n",
      "train loss:0.0009071996745823434\n",
      "train loss:0.0029482165325235565\n",
      "train loss:0.0015831477633517852\n",
      "train loss:0.0009862454832418634\n",
      "train loss:0.00035396118690636877\n",
      "train loss:0.0008629803206038222\n",
      "train loss:0.0018488304989423441\n",
      "train loss:0.0037738005509895074\n",
      "train loss:0.0058208128137453045\n",
      "train loss:0.0008457705331885347\n",
      "train loss:0.0009252465451410855\n",
      "train loss:0.00043482067779497205\n",
      "train loss:0.00040429245693282957\n",
      "train loss:0.0038153794493042694\n",
      "train loss:0.0009365414198089694\n",
      "train loss:0.000988198939703051\n",
      "train loss:0.004692290422988384\n",
      "train loss:0.0003291005960305219\n",
      "train loss:0.0033864850549818392\n",
      "train loss:0.0011085693484369948\n",
      "train loss:0.0007121902747191536\n",
      "train loss:0.0018841433569579435\n",
      "train loss:0.006448755941085755\n",
      "train loss:0.002210162318911659\n",
      "train loss:0.004385514867430859\n",
      "train loss:0.0023711219430008956\n",
      "train loss:0.001830513239396997\n",
      "train loss:0.002046433462922\n",
      "train loss:0.0033273602700110228\n",
      "train loss:0.00038981607061580083\n",
      "train loss:0.0013041879784006498\n",
      "train loss:2.52604845730031e-05\n",
      "train loss:0.0017177091199606163\n",
      "train loss:0.0003596796728422821\n",
      "train loss:0.001464594315495616\n",
      "train loss:0.00637031437074289\n",
      "train loss:0.010384666844253241\n",
      "train loss:0.0003189427129937115\n",
      "train loss:0.004788136037380039\n",
      "train loss:0.0037868959491032506\n",
      "train loss:0.0008993331233347349\n",
      "train loss:0.001644554157735328\n",
      "train loss:0.0011663087468142322\n",
      "train loss:0.002927702796291677\n",
      "train loss:0.0008471658376055919\n",
      "train loss:0.00032014180301717475\n",
      "train loss:0.0031047015178835504\n",
      "train loss:0.025344091217543965\n",
      "train loss:0.0033622411977417893\n",
      "train loss:0.002592730342722161\n",
      "train loss:0.0009098273398001146\n",
      "train loss:0.0046115334190559186\n",
      "train loss:0.0008766748271629333\n",
      "train loss:0.00397269688035685\n",
      "train loss:0.0018207369785353646\n",
      "train loss:0.0007989527880465836\n",
      "train loss:0.00042377425219378823\n",
      "train loss:0.0017772499134487507\n",
      "train loss:0.0036422086277603095\n",
      "train loss:0.0005632552160884016\n",
      "train loss:0.0019502139244835035\n",
      "train loss:0.00192430716364944\n",
      "train loss:0.004403278318522632\n",
      "train loss:0.0015079405715878837\n",
      "train loss:0.00017464819786242806\n",
      "train loss:0.0018755622580558019\n",
      "train loss:0.0004698059511699628\n",
      "train loss:0.0009218787619365393\n",
      "train loss:0.00797857691520006\n",
      "train loss:0.0012205499486796338\n",
      "train loss:0.00015644169834463595\n",
      "train loss:0.0010530503431793387\n",
      "train loss:0.0035409914030788254\n",
      "train loss:0.0008888828308504078\n",
      "train loss:0.00013720901719287774\n",
      "train loss:0.0013001395863736901\n",
      "train loss:0.002404198380106884\n",
      "train loss:0.0008998832628485681\n",
      "train loss:0.0043594072973580315\n",
      "train loss:0.00016084187684749874\n",
      "train loss:0.00028360944227848\n",
      "train loss:0.0016302541498407841\n",
      "train loss:0.0015233602035027338\n",
      "train loss:0.00014215638876970908\n",
      "train loss:0.00020664054256397957\n",
      "train loss:0.0005740641235169559\n",
      "train loss:0.003542369167070949\n",
      "train loss:0.0002928423601925158\n",
      "train loss:0.00011159370224622841\n",
      "train loss:0.004172302564158405\n",
      "train loss:0.0006164277049830462\n",
      "train loss:0.0003099997936121\n",
      "train loss:0.0019952216426460576\n",
      "train loss:0.002484347414771222\n",
      "train loss:0.0009741699580996925\n",
      "train loss:0.0022355759999516042\n",
      "train loss:0.00021700538306969824\n",
      "train loss:0.0008454984749851804\n",
      "train loss:0.003990379354695588\n",
      "train loss:0.0011246283663728437\n",
      "train loss:0.00041311048082328224\n",
      "train loss:0.0006688548657060713\n",
      "train loss:0.0003913438067074088\n",
      "train loss:0.04082242187422354\n",
      "train loss:0.002123439715635675\n",
      "train loss:0.00037330440997344346\n",
      "train loss:0.0035053064791822834\n",
      "train loss:0.0008810634106738526\n",
      "train loss:0.0016854278081083584\n",
      "train loss:0.001938761641526721\n",
      "train loss:0.0008766994862347685\n",
      "train loss:0.01090239618461471\n",
      "train loss:0.000979119179157119\n",
      "train loss:0.0004579829158050731\n",
      "train loss:0.0004495216672889323\n",
      "train loss:0.0002177048742940435\n",
      "train loss:0.002584794061846218\n",
      "train loss:0.0014637444770944283\n",
      "train loss:0.00028331959373807095\n",
      "train loss:0.0004900901733014056\n",
      "train loss:0.0016386392252002197\n",
      "train loss:0.0015259906823563154\n",
      "train loss:0.028404194429726828\n",
      "train loss:0.0004978529284299159\n",
      "train loss:0.002697383505567689\n",
      "train loss:0.0003941683537081868\n",
      "train loss:0.0009054077212898487\n",
      "train loss:0.0003320339496093366\n",
      "train loss:0.0012546101686515864\n",
      "train loss:7.727651664544879e-05\n",
      "train loss:0.001150602322406815\n",
      "train loss:0.012585152100906938\n",
      "train loss:0.00019428536369952843\n",
      "train loss:0.0004529299488410539\n",
      "train loss:0.004614881289937603\n",
      "train loss:0.001876555689407532\n",
      "train loss:0.0013201532909575494\n",
      "train loss:0.002671133065684749\n",
      "train loss:0.0006840621531678873\n",
      "train loss:0.00030766663693473494\n",
      "train loss:0.00037405249350922653\n",
      "train loss:0.008682864172833333\n",
      "train loss:0.0005702672871841191\n",
      "train loss:0.002943588507320079\n",
      "train loss:0.00020630733455996039\n",
      "train loss:0.00016843201979513937\n",
      "train loss:0.0021410738042925235\n",
      "train loss:0.0006418836075173528\n",
      "train loss:0.0004894791573206807\n",
      "train loss:0.0002496804349642654\n",
      "train loss:0.005619947691682532\n",
      "train loss:0.00038015888354852375\n",
      "train loss:0.004516996124232568\n",
      "train loss:0.0011901706579813401\n",
      "train loss:0.001524541507401147\n",
      "train loss:0.011530560481100037\n",
      "train loss:0.0017799076927587333\n",
      "train loss:0.0019407252525696984\n",
      "train loss:0.0023960356245020923\n",
      "train loss:0.038256537867518946\n",
      "train loss:0.0003778816090121049\n",
      "train loss:0.003918823967337782\n",
      "train loss:0.0008681220044303711\n",
      "train loss:0.0008892348624018693\n",
      "train loss:0.0011025887043398992\n",
      "train loss:0.00022031118804399227\n",
      "train loss:0.0002886237593042557\n",
      "train loss:0.0014956234164619883\n",
      "train loss:0.0005473895783697576\n",
      "train loss:0.0024578530131677116\n",
      "train loss:0.00022775471070243046\n",
      "train loss:0.00010721244641491464\n",
      "train loss:0.0014954435078047134\n",
      "train loss:0.0018340954840530934\n",
      "train loss:0.013946951680458277\n",
      "train loss:0.0016438956090404507\n",
      "train loss:0.0006288647910459488\n",
      "train loss:0.025194287121562766\n",
      "train loss:0.0002931150042338981\n",
      "train loss:0.003490400385855458\n",
      "train loss:0.0007505374970978705\n",
      "train loss:0.0006147622886331453\n",
      "train loss:0.0017404419080489927\n",
      "train loss:0.0015842472126729912\n",
      "train loss:0.0019727041553996\n",
      "train loss:0.00047177976573538387\n",
      "train loss:0.0012618937282739945\n",
      "train loss:0.00118157529469509\n",
      "train loss:0.00020869464124432052\n",
      "train loss:0.0010717047746135046\n",
      "train loss:0.0018583572099772896\n",
      "train loss:0.000610363732598066\n",
      "train loss:0.0013855684949466646\n",
      "train loss:0.0005430845239666742\n",
      "train loss:0.004363887423032476\n",
      "train loss:0.0025357338379418575\n",
      "train loss:0.0036131673120350233\n",
      "train loss:0.0008868767430736525\n",
      "train loss:0.003131219109160013\n",
      "train loss:0.027751993101711073\n",
      "train loss:0.000305983852288046\n",
      "train loss:0.0006604109112774182\n",
      "train loss:0.008572471177724967\n",
      "train loss:7.615979841414094e-05\n",
      "train loss:0.0012367939379646288\n",
      "train loss:0.0003834749620592447\n",
      "train loss:0.01781295345487259\n",
      "train loss:0.0002967869196184387\n",
      "train loss:0.0013894490403648949\n",
      "train loss:0.014726618666299722\n",
      "train loss:0.007342023007451165\n",
      "train loss:0.005107407316854803\n",
      "train loss:0.01228095211065304\n",
      "train loss:0.002022317942972401\n",
      "train loss:0.00026912509881650256\n",
      "train loss:0.0008208949753371539\n",
      "train loss:0.0008767194663298551\n",
      "train loss:0.0008344646366839814\n",
      "train loss:0.0006868922707203483\n",
      "train loss:0.0027739900486248976\n",
      "train loss:0.006506099662161558\n",
      "train loss:0.001917526089392843\n",
      "train loss:0.0038644801510233596\n",
      "train loss:0.004434031970766164\n",
      "train loss:0.003564982364917515\n",
      "train loss:0.0111946139029822\n",
      "train loss:0.002968502389949979\n",
      "train loss:0.0009042257000038413\n",
      "train loss:0.00032718129622105843\n",
      "train loss:0.0024602122289690727\n",
      "train loss:0.00013657861777358047\n",
      "train loss:0.0022710936979132104\n",
      "train loss:0.0010759505727078863\n",
      "train loss:0.0013317751327586674\n",
      "train loss:0.00729808070345885\n",
      "train loss:0.004078245566734451\n",
      "train loss:0.001880899781654433\n",
      "train loss:0.0032949567628847726\n",
      "train loss:0.0025471718708423054\n",
      "train loss:0.0012089025131563368\n",
      "train loss:0.003378108147707737\n",
      "train loss:0.003468785734873688\n",
      "train loss:8.902638189439288e-05\n",
      "train loss:0.020612407423223997\n",
      "train loss:0.0020084248521587277\n",
      "train loss:0.0023913672649975405\n",
      "train loss:0.0005292443162164475\n",
      "train loss:0.0023436161469528403\n",
      "train loss:0.0007631419158631059\n",
      "train loss:0.0027978060668217035\n",
      "train loss:0.0020835998328963172\n",
      "train loss:0.0001699881504570561\n",
      "train loss:0.0033574861334386462\n",
      "train loss:0.026223351325784117\n",
      "train loss:0.0016623570127785713\n",
      "train loss:0.0023139261819860066\n",
      "train loss:0.0007038584592162231\n",
      "train loss:0.00024240609109641806\n",
      "train loss:0.0013808388468300485\n",
      "train loss:0.002096594132571759\n",
      "train loss:0.0019176536707827985\n",
      "train loss:0.0007960009497013322\n",
      "train loss:0.0006781479088123234\n",
      "train loss:0.0010057956014155177\n",
      "train loss:0.0010464954697459944\n",
      "train loss:0.007683023185795843\n",
      "train loss:0.002963463060475025\n",
      "train loss:0.00027173597766954607\n",
      "train loss:0.0010803495060975503\n",
      "train loss:6.698018939928452e-05\n",
      "train loss:0.00038447521241488875\n",
      "train loss:0.0022457949333774922\n",
      "train loss:0.004060839809868139\n",
      "train loss:0.0013205168193861103\n",
      "train loss:0.0007929900946793178\n",
      "train loss:0.004857156680928493\n",
      "train loss:0.0008679010482717048\n",
      "train loss:0.002769292575371604\n",
      "train loss:0.0036896506529553227\n",
      "train loss:0.007916579310554788\n",
      "train loss:0.0004374235333622007\n",
      "train loss:0.0017854104980563029\n",
      "train loss:0.0012236134009571643\n",
      "train loss:0.0007402161172046258\n",
      "train loss:0.001184423405212043\n",
      "train loss:0.0021076835305654396\n",
      "train loss:0.002946807386054103\n",
      "train loss:0.000860753138079391\n",
      "train loss:0.0009883195570840105\n",
      "train loss:0.02198651976056398\n",
      "train loss:0.001763297897174963\n",
      "train loss:0.0015952551784159743\n",
      "train loss:0.0014456440141626193\n",
      "train loss:0.00890879412066767\n",
      "train loss:0.007046662606779437\n",
      "train loss:0.003725792683071277\n",
      "train loss:0.0012012983998242815\n",
      "train loss:0.0018556722338467365\n",
      "train loss:0.00014473509485679048\n",
      "train loss:0.00881246521764723\n",
      "train loss:0.00030092434538702475\n",
      "train loss:0.00017237018049334518\n",
      "train loss:0.003078926096557781\n",
      "train loss:0.003035202012146557\n",
      "train loss:0.0005225681898269202\n",
      "train loss:0.0067821409337319\n",
      "train loss:0.0028708920434564666\n",
      "train loss:0.00017218648878058811\n",
      "train loss:0.0026935318613555224\n",
      "train loss:0.0005913915463666105\n",
      "train loss:0.001749928995783171\n",
      "train loss:0.00018051547259995\n",
      "train loss:0.005133003736122176\n",
      "train loss:0.0015344192898100652\n",
      "train loss:0.0006926712958921531\n",
      "train loss:0.0007494133332477193\n",
      "train loss:0.00042951914257287306\n",
      "train loss:0.0005684169603814406\n",
      "train loss:0.0005224109447397321\n",
      "train loss:0.0004044415721014891\n",
      "train loss:0.001124072635329288\n",
      "train loss:0.0008138510641993784\n",
      "train loss:0.0024285375841294823\n",
      "train loss:0.0007668248752922111\n",
      "train loss:0.0009767644020618684\n",
      "train loss:0.006733053647525763\n",
      "train loss:0.008134048491814322\n",
      "train loss:0.02609034536611094\n",
      "train loss:0.0027604070934862156\n",
      "train loss:0.0008520286384548867\n",
      "train loss:0.0005502108566535105\n",
      "train loss:0.0006365038147164493\n",
      "train loss:0.0003554768441923872\n",
      "train loss:0.0013351040776616503\n",
      "train loss:0.004901953511525969\n",
      "train loss:0.0011271596712042482\n",
      "train loss:6.907906734705653e-05\n",
      "train loss:0.00012033585587652095\n",
      "train loss:0.0012045412203213859\n",
      "train loss:0.0015774386718420124\n",
      "train loss:0.002271927643812811\n",
      "train loss:0.007504130223554011\n",
      "train loss:0.0012198831726167652\n",
      "train loss:0.0027626532814764566\n",
      "train loss:0.0004247633932955737\n",
      "train loss:0.00040466112976074247\n",
      "train loss:0.005468432129958054\n",
      "train loss:0.0063142379161268405\n",
      "train loss:0.0015071458903812562\n",
      "train loss:0.0007363822921349388\n",
      "train loss:0.00046535401545968124\n",
      "train loss:0.004103083941168093\n",
      "train loss:0.0013607186615247756\n",
      "train loss:0.00022969163384678324\n",
      "train loss:0.001490292237163239\n",
      "train loss:0.0009458991991956531\n",
      "train loss:0.01581356657624991\n",
      "train loss:0.000482713351843264\n",
      "train loss:0.005402922792780171\n",
      "train loss:0.0005177513258659773\n",
      "train loss:0.003452010691526935\n",
      "train loss:0.0006441309884253285\n",
      "train loss:0.005339108916443445\n",
      "train loss:0.00014942390872585935\n",
      "train loss:0.0013889200197640184\n",
      "train loss:0.0015271158698271017\n",
      "train loss:0.00735771560438765\n",
      "train loss:0.003208310677055912\n",
      "train loss:0.0002185669658863629\n",
      "train loss:0.0002737230172676822\n",
      "train loss:0.004152358471458946\n",
      "train loss:0.0005553044703629374\n",
      "train loss:0.000192633140661347\n",
      "train loss:0.0008406624208613516\n",
      "train loss:0.0014547052529520377\n",
      "train loss:7.422792053368963e-05\n",
      "train loss:0.02288700358667442\n",
      "train loss:0.0021981657065454165\n",
      "train loss:0.0007954957410020272\n",
      "train loss:0.0017502789299422138\n",
      "train loss:0.005608353735665789\n",
      "train loss:0.0009204998964396355\n",
      "train loss:0.0012516862583069142\n",
      "train loss:0.001158742127833839\n",
      "train loss:0.002300838334292196\n",
      "train loss:0.0012273752474547589\n",
      "train loss:0.0018200353622261755\n",
      "train loss:0.0002758759125628434\n",
      "train loss:0.003028943580644877\n",
      "train loss:0.006973368953041458\n",
      "train loss:0.0013552460086904685\n",
      "train loss:0.0012088297215156545\n",
      "train loss:0.0013895410117212295\n",
      "train loss:0.0043794453666412825\n",
      "train loss:0.00048804938623939247\n",
      "train loss:0.0017159169565196311\n",
      "train loss:0.0003815746059253977\n",
      "train loss:0.0014558313743454386\n",
      "train loss:0.00031917942340818867\n",
      "train loss:0.0014721609813194785\n",
      "train loss:9.960393074151251e-05\n",
      "train loss:0.003682176440522664\n",
      "train loss:0.001406187926634306\n",
      "train loss:0.0035918763261957674\n",
      "train loss:0.009114933353331111\n",
      "train loss:0.002433914479061428\n",
      "train loss:0.0007375218517934641\n",
      "train loss:0.004144345347059683\n",
      "train loss:0.0013380277380616319\n",
      "train loss:0.002665766827160745\n",
      "train loss:0.00046047007021387734\n",
      "train loss:0.0001499608968604902\n",
      "train loss:0.0003026165363931984\n",
      "train loss:0.0017673396177644045\n",
      "train loss:0.0011833000497666344\n",
      "train loss:0.003681958748702342\n",
      "train loss:0.0001596297068083511\n",
      "train loss:0.005201618961509939\n",
      "train loss:0.0004441997006895714\n",
      "train loss:0.003086928815552933\n",
      "train loss:0.0013624271614147427\n",
      "train loss:0.0017128676787234573\n",
      "train loss:0.0009525512363587885\n",
      "train loss:0.00539487440162609\n",
      "train loss:0.002855283772250729\n",
      "train loss:0.0021426838549522785\n",
      "train loss:0.006438115156031292\n",
      "train loss:0.0009564174216542701\n",
      "train loss:0.0007528435993165606\n",
      "train loss:0.0027685254221806953\n",
      "train loss:0.008291604605479057\n",
      "train loss:0.00250451214580895\n",
      "train loss:0.0012225151922908217\n",
      "train loss:0.009296457002275238\n",
      "train loss:0.0015989289355842725\n",
      "train loss:0.00760227591279445\n",
      "train loss:0.0005979447571953973\n",
      "train loss:0.0030921204636587107\n",
      "train loss:0.00026518444207752933\n",
      "train loss:0.0012099079334175927\n",
      "train loss:0.003925905009545879\n",
      "train loss:0.006553268198811536\n",
      "train loss:0.001693223535891917\n",
      "train loss:0.0017596545329619138\n",
      "train loss:0.004768511802930689\n",
      "train loss:0.003537923203640701\n",
      "train loss:0.0010418023613453073\n",
      "train loss:0.0015391456069918327\n",
      "train loss:0.02646487865935091\n",
      "train loss:0.0004139469908370982\n",
      "train loss:0.002465890298493783\n",
      "train loss:0.0011348560314388015\n",
      "train loss:0.0015989864689915823\n",
      "train loss:0.0053716904597438195\n",
      "train loss:0.0008595373375424314\n",
      "train loss:0.002378094439387551\n",
      "train loss:0.002302054104284837\n",
      "train loss:0.0018250313167511418\n",
      "train loss:0.00032879924494270534\n",
      "train loss:0.00018082403522093945\n",
      "train loss:0.0007912085025343238\n",
      "train loss:0.002118502214185657\n",
      "train loss:0.00552787584471229\n",
      "train loss:0.0012847222577927972\n",
      "train loss:0.0006812602152884042\n",
      "train loss:0.007358091470193045\n",
      "train loss:0.003560587842291617\n",
      "train loss:0.0001548759944209001\n",
      "train loss:0.020054049506478724\n",
      "train loss:0.007017032870738444\n",
      "train loss:0.0027882390455636424\n",
      "train loss:0.0007346351709349056\n",
      "train loss:0.0005263258426413776\n",
      "train loss:0.004107833329906581\n",
      "train loss:0.0009418125146993625\n",
      "train loss:0.0017426144151270492\n",
      "train loss:0.0010929980086836469\n",
      "train loss:0.017152654563766356\n",
      "train loss:0.010279091950261365\n",
      "train loss:0.003708728079218776\n",
      "train loss:0.0032094044364552226\n",
      "train loss:0.0014590383228976875\n",
      "train loss:0.00029613991927088484\n",
      "train loss:0.004941008194872019\n",
      "train loss:0.025276695589899836\n",
      "train loss:0.0019566094479352915\n",
      "=== epoch:16, train acc:1.0, test acc:0.982 ===\n",
      "train loss:0.004300999785098118\n",
      "train loss:0.009836751191980941\n",
      "train loss:0.0021313419510831694\n",
      "train loss:0.0025898637602191988\n",
      "train loss:0.000898549163728872\n",
      "train loss:0.0007125959982170623\n",
      "train loss:0.0017223197314893929\n",
      "train loss:0.0010868599391309756\n",
      "train loss:0.002522286574660167\n",
      "train loss:0.002766695727309088\n",
      "train loss:0.008873029010623097\n",
      "train loss:0.003051456631591971\n",
      "train loss:0.002735997125017051\n",
      "train loss:0.0005072665259204284\n",
      "train loss:0.0001398302205792683\n",
      "train loss:0.0006030741510826302\n",
      "train loss:0.0013662102614000172\n",
      "train loss:0.00479612969778696\n",
      "train loss:0.004997384240371009\n",
      "train loss:0.0021612544355141235\n",
      "train loss:0.00982178734092605\n",
      "train loss:0.019916142462580177\n",
      "train loss:0.0005914838332156695\n",
      "train loss:0.006602845312577821\n",
      "train loss:0.014176455966057249\n",
      "train loss:0.0022553474315927424\n",
      "train loss:0.001498110148929494\n",
      "train loss:0.004463362271935542\n",
      "train loss:0.003175817476408486\n",
      "train loss:0.005027058041312139\n",
      "train loss:0.0021395121540013964\n",
      "train loss:0.008207242936739429\n",
      "train loss:0.00358071887700991\n",
      "train loss:0.00597596254408479\n",
      "train loss:0.00027325698120083346\n",
      "train loss:0.000727044710939542\n",
      "train loss:0.001034544230776666\n",
      "train loss:0.0012880343183332488\n",
      "train loss:0.0024074259980924506\n",
      "train loss:0.010596658955708998\n",
      "train loss:0.0007610973753098284\n",
      "train loss:7.418551634025776e-05\n",
      "train loss:0.0037082204233065464\n",
      "train loss:0.0010090453972784862\n",
      "train loss:0.0019391969521042232\n",
      "train loss:0.008100188737892526\n",
      "train loss:0.019302786874129486\n",
      "train loss:0.0006790649423658545\n",
      "train loss:0.002984434281346807\n",
      "train loss:0.001090048442369227\n",
      "train loss:0.00035526190012925493\n",
      "train loss:0.011485246126781621\n",
      "train loss:0.005953872251937566\n",
      "train loss:0.002844643781519883\n",
      "train loss:0.012325730531988493\n",
      "train loss:0.021102766058321586\n",
      "train loss:0.0587820424627492\n",
      "train loss:0.0078009978340934346\n",
      "train loss:0.019465403361286535\n",
      "train loss:0.0007878852883062007\n",
      "train loss:0.0011089401529096286\n",
      "train loss:0.003107643487349162\n",
      "train loss:0.009550556174232927\n",
      "train loss:0.0042914658770787885\n",
      "train loss:0.005839550247781706\n",
      "train loss:0.014076193988525063\n",
      "train loss:0.0018988887984046088\n",
      "train loss:0.006828641014812325\n",
      "train loss:0.0017623386208193736\n",
      "train loss:0.0006453335619384021\n",
      "train loss:0.0005333514310705827\n",
      "train loss:0.012042057727953496\n",
      "train loss:0.001492420749239413\n",
      "train loss:0.0027945227127184075\n",
      "train loss:0.013938643629387945\n",
      "train loss:0.0021913185596415525\n",
      "train loss:0.022211628604444165\n",
      "train loss:0.002455014431656319\n",
      "train loss:0.005935135609264372\n",
      "train loss:0.0008670620560613405\n",
      "train loss:0.003848138593972897\n",
      "train loss:0.0020169615057722802\n",
      "train loss:0.0001117508139434432\n",
      "train loss:0.0015927307745422887\n",
      "train loss:0.001469422770734947\n",
      "train loss:0.0010362328910332671\n",
      "train loss:0.020811963741539645\n",
      "train loss:0.00510095552970741\n",
      "train loss:0.00042637607520669673\n",
      "train loss:0.0042737147929066575\n",
      "train loss:0.009632532669728107\n",
      "train loss:0.013701854837705429\n",
      "train loss:0.0046396620372076295\n",
      "train loss:0.0018293440553877039\n",
      "train loss:0.008663554265922328\n",
      "train loss:0.00024321954522846758\n",
      "train loss:0.0008777295642748528\n",
      "train loss:0.0013014967328869923\n",
      "train loss:0.0034570166210388612\n",
      "train loss:0.0080374045046887\n",
      "train loss:0.0043762046618437\n",
      "train loss:0.00291576445272037\n",
      "train loss:0.0035239916912588294\n",
      "train loss:0.0012139882142950865\n",
      "train loss:0.003354641561857104\n",
      "train loss:0.00011429136854098426\n",
      "train loss:0.00011989936449784811\n",
      "train loss:0.003638426207469061\n",
      "train loss:0.007886119533071506\n",
      "train loss:0.0016860824486423657\n",
      "train loss:0.0018349356780009338\n",
      "train loss:0.002508263015014085\n",
      "train loss:0.00023910689114562664\n",
      "train loss:0.003523576468197669\n",
      "train loss:0.0021240643054959304\n",
      "train loss:0.001248534450871669\n",
      "train loss:0.0019244879553055884\n",
      "train loss:0.002347765856708469\n",
      "train loss:0.00018867566563378644\n",
      "train loss:0.0011641048247145985\n",
      "train loss:0.0001339279173791295\n",
      "train loss:0.0032848920554036437\n",
      "train loss:0.010929850812771782\n",
      "train loss:0.0018218940376384621\n",
      "train loss:0.005610592245756151\n",
      "train loss:0.00037135088198324035\n",
      "train loss:0.0007521871609096078\n",
      "train loss:0.0016618368299918757\n",
      "train loss:0.002677715517968142\n",
      "train loss:0.0034221365491190494\n",
      "train loss:6.400618294941633e-05\n",
      "train loss:0.0015821834311160528\n",
      "train loss:0.008008562804669214\n",
      "train loss:0.0013611424410791538\n",
      "train loss:0.0005785620637072608\n",
      "train loss:0.0017832817946631105\n",
      "train loss:0.0019564712753231063\n",
      "train loss:9.915095311227614e-05\n",
      "train loss:0.008781475199324396\n",
      "train loss:0.0001545528489571515\n",
      "train loss:0.0010684073573869635\n",
      "train loss:0.013643637423693513\n",
      "train loss:0.0003322174240332364\n",
      "train loss:0.0006692800326534153\n",
      "train loss:0.0016503903362593533\n",
      "train loss:0.0009505835829873383\n",
      "train loss:0.01191356996742565\n",
      "train loss:0.004344264307504253\n",
      "train loss:0.011923166812763598\n",
      "train loss:0.0014144439114593\n",
      "train loss:0.00014350155842247778\n",
      "train loss:0.002605092326282349\n",
      "train loss:0.0005572517330193424\n",
      "train loss:0.0008807727982878337\n",
      "train loss:0.0009210447431326845\n",
      "train loss:0.002727165387232599\n",
      "train loss:0.003281667217262108\n",
      "train loss:0.00019066321557163534\n",
      "train loss:0.0003684411735755083\n",
      "train loss:0.0054458174898209\n",
      "train loss:0.002494969168643181\n",
      "train loss:0.003922215296351023\n",
      "train loss:0.0018924308892485139\n",
      "train loss:0.0016135967308347418\n",
      "train loss:0.0024821870732728763\n",
      "train loss:0.00032938261395257094\n",
      "train loss:0.003061944810339728\n",
      "train loss:0.00022475515368246897\n",
      "train loss:0.0016914136725989799\n",
      "train loss:0.0002466354802978827\n",
      "train loss:0.014648713714950287\n",
      "train loss:0.001034100620183627\n",
      "train loss:0.0053034363098256685\n",
      "train loss:0.0010540082860469853\n",
      "train loss:0.002176018611256766\n",
      "train loss:0.0019241820090036641\n",
      "train loss:0.0033481572971749063\n",
      "train loss:0.011991600849488037\n",
      "train loss:0.0017156565676382971\n",
      "train loss:0.0014703553322808454\n",
      "train loss:0.00737047778896956\n",
      "train loss:0.00021192180981588447\n",
      "train loss:4.0492414285203166e-05\n",
      "train loss:0.004088733151880676\n",
      "train loss:0.0019279037233106166\n",
      "train loss:0.00029467891337490456\n",
      "train loss:0.0024528906976469715\n",
      "train loss:0.0026849574140717456\n",
      "train loss:0.0020184997469535233\n",
      "train loss:0.0020523855116798906\n",
      "train loss:0.003502750428317077\n",
      "train loss:0.0019233416103426463\n",
      "train loss:0.004929664903976625\n",
      "train loss:0.004516523788992034\n",
      "train loss:0.0031182261747966346\n",
      "train loss:0.0009055303401770334\n",
      "train loss:0.00559340855458797\n",
      "train loss:0.010771395736852674\n",
      "train loss:0.0002777569744225486\n",
      "train loss:0.0020570780402020946\n",
      "train loss:0.0004993372357695\n",
      "train loss:0.0010515833639892823\n",
      "train loss:0.0046197663474166865\n",
      "train loss:0.0034667146662736566\n",
      "train loss:0.004176231811678393\n",
      "train loss:0.0012767497302853033\n",
      "train loss:0.0002576057610305741\n",
      "train loss:0.0025850440579422\n",
      "train loss:0.0022188383549916315\n",
      "train loss:0.009868855942768864\n",
      "train loss:0.004086001988741539\n",
      "train loss:0.013463469034488376\n",
      "train loss:0.0052606751807260845\n",
      "train loss:0.013902442819100549\n",
      "train loss:0.001131770315980587\n",
      "train loss:0.00076017306826163\n",
      "train loss:0.0018543323937978366\n",
      "train loss:0.0008381315375953378\n",
      "train loss:0.0014275091386008484\n",
      "train loss:0.0008497624973741137\n",
      "train loss:0.0007586717286699702\n",
      "train loss:0.004192759276813201\n",
      "train loss:0.0035285415086730664\n",
      "train loss:0.0029450963292231734\n",
      "train loss:0.00012130147491466265\n",
      "train loss:0.0007509977371464865\n",
      "train loss:0.0005441475162046125\n",
      "train loss:0.004087905122986722\n",
      "train loss:0.0031679473580522926\n",
      "train loss:0.00306111503257991\n",
      "train loss:0.0006083789918732062\n",
      "train loss:0.0006340659473705005\n",
      "train loss:0.012932581610557608\n",
      "train loss:0.000536332122921565\n",
      "train loss:0.0007226072578891714\n",
      "train loss:0.002454706797968075\n",
      "train loss:0.0018762531877785536\n",
      "train loss:0.0001372267961173102\n",
      "train loss:0.021016972404387206\n",
      "train loss:0.005670989570465499\n",
      "train loss:0.0005252287050078949\n",
      "train loss:0.00036543176950485837\n",
      "train loss:6.608169857862841e-05\n",
      "train loss:7.663258702332518e-05\n",
      "train loss:0.0050486285992708095\n",
      "train loss:0.001168913322485312\n",
      "train loss:0.0027728616001664185\n",
      "train loss:0.001341234360248861\n",
      "train loss:0.0005422542998321396\n",
      "train loss:0.0011830441871444187\n",
      "train loss:0.0013441640859136105\n",
      "train loss:0.0007967995445260921\n",
      "train loss:0.0009487054499647689\n",
      "train loss:0.0001949191250323502\n",
      "train loss:0.0030839876023918637\n",
      "train loss:0.0009790206744488693\n",
      "train loss:0.00035164503005693696\n",
      "train loss:0.002021998526339222\n",
      "train loss:0.0011066742847351185\n",
      "train loss:0.0018683814717942004\n",
      "train loss:0.0013949443199237153\n",
      "train loss:0.00033043627106904935\n",
      "train loss:0.002222726812673341\n",
      "train loss:0.0002813257430210373\n",
      "train loss:0.0009633695743495091\n",
      "train loss:0.0007571820332184827\n",
      "train loss:0.0008034736111487165\n",
      "train loss:0.00034206182788578857\n",
      "train loss:0.002703090724283668\n",
      "train loss:0.0010855666399833058\n",
      "train loss:0.0004892199563747536\n",
      "train loss:0.012227306340212251\n",
      "train loss:0.001592355026823342\n",
      "train loss:0.0007467121095714928\n",
      "train loss:0.0005121053962458675\n",
      "train loss:3.713184934150102e-05\n",
      "train loss:0.0015065008174646802\n",
      "train loss:0.00016955732081374672\n",
      "train loss:0.0011464450728697329\n",
      "train loss:0.00046172849590634486\n",
      "train loss:0.0021823602970968054\n",
      "train loss:0.002383923393090561\n",
      "train loss:0.0003678298056222546\n",
      "train loss:0.00011189664631841257\n",
      "train loss:0.007085264933547674\n",
      "train loss:0.02043052824272288\n",
      "train loss:0.00039257564880709436\n",
      "train loss:0.0006019499532964336\n",
      "train loss:0.002808814564788982\n",
      "train loss:0.0032552253849167435\n",
      "train loss:0.00023388223877175753\n",
      "train loss:0.00017393389212674973\n",
      "train loss:0.0023488067402128864\n",
      "train loss:0.000618955709129478\n",
      "train loss:0.0005007380603614152\n",
      "train loss:0.002925630422300823\n",
      "train loss:0.0010740337630011183\n",
      "train loss:0.00019387447643516006\n",
      "train loss:0.000281069330306082\n",
      "train loss:0.002283314985610194\n",
      "train loss:0.0029074453657476395\n",
      "train loss:0.0003642727190765969\n",
      "train loss:0.00019536854016001217\n",
      "train loss:0.0031590406425577567\n",
      "train loss:0.0029168073552943426\n",
      "train loss:0.0005936269545437794\n",
      "train loss:0.0068107083823154705\n",
      "train loss:0.00017800890491835749\n",
      "train loss:0.00035066324855696405\n",
      "train loss:0.025694442922589614\n",
      "train loss:0.018665219836532715\n",
      "train loss:0.0006328873162680753\n",
      "train loss:0.000802618256876379\n",
      "train loss:6.326658387116087e-05\n",
      "train loss:0.0013232837756123603\n",
      "train loss:0.001300581027871512\n",
      "train loss:0.0032423541118542913\n",
      "train loss:0.0001009672543724785\n",
      "train loss:0.0008420604397002461\n",
      "train loss:0.0025022214972974443\n",
      "train loss:0.0014064287494657515\n",
      "train loss:0.0024522386177720967\n",
      "train loss:0.0018108613346314705\n",
      "train loss:0.002139350071690072\n",
      "train loss:0.0015175751682007142\n",
      "train loss:0.0006141638963565544\n",
      "train loss:0.00023845881316121793\n",
      "train loss:0.0007455998546326852\n",
      "train loss:0.006318290380412213\n",
      "train loss:0.00019194668911625998\n",
      "train loss:0.0009720527736746008\n",
      "train loss:0.0006100656036116158\n",
      "train loss:0.0002171716440264999\n",
      "train loss:0.0004827833561803356\n",
      "train loss:0.0009736812520185172\n",
      "train loss:0.00045925242793093944\n",
      "train loss:0.004925364313553124\n",
      "train loss:0.004318527191242747\n",
      "train loss:0.0010333410103612328\n",
      "train loss:0.00014489972495724136\n",
      "train loss:0.0016610989823587824\n",
      "train loss:0.03365799054141724\n",
      "train loss:0.0020555862698303394\n",
      "train loss:6.180456459306858e-05\n",
      "train loss:0.0021767247282184253\n",
      "train loss:0.007588881607937716\n",
      "train loss:0.002869931784024815\n",
      "train loss:0.0019569597770033426\n",
      "train loss:0.0004900587463260037\n",
      "train loss:0.001001884932863512\n",
      "train loss:0.004704171317639244\n",
      "train loss:0.0008318678903309441\n",
      "train loss:0.0011242190905193608\n",
      "train loss:0.0008176082689058181\n",
      "train loss:0.00032253203715785117\n",
      "train loss:0.0018734557204691821\n",
      "train loss:0.0008781878617036909\n",
      "train loss:4.797821260508283e-05\n",
      "train loss:0.0007971352869226736\n",
      "train loss:0.0019487599586069987\n",
      "train loss:0.0001543224760461102\n",
      "train loss:0.0006201924465938449\n",
      "train loss:0.012509540703906658\n",
      "train loss:0.0002490800167846547\n",
      "train loss:0.0021728085995980618\n",
      "train loss:0.001814389771799023\n",
      "train loss:0.0031089235884211247\n",
      "train loss:0.00019965765592228548\n",
      "train loss:0.0011528556911332161\n",
      "train loss:0.0006885997940508648\n",
      "train loss:0.00023660741233148122\n",
      "train loss:0.0029583849499768616\n",
      "train loss:0.003284729341755156\n",
      "train loss:0.0023654834564404755\n",
      "train loss:0.0005733527732948069\n",
      "train loss:0.00019442460134309336\n",
      "train loss:0.000520850488026151\n",
      "train loss:0.0002179475690541037\n",
      "train loss:0.0005938127820599274\n",
      "train loss:0.004904803909519169\n",
      "train loss:0.0025517349872995975\n",
      "train loss:0.0029580633278841607\n",
      "train loss:0.0011816097352502692\n",
      "train loss:0.0012595395551079623\n",
      "train loss:0.001723995598654196\n",
      "train loss:0.0008106903521605535\n",
      "train loss:0.0034240090811156754\n",
      "train loss:0.0005122845111185316\n",
      "train loss:0.001403195962286376\n",
      "train loss:0.0012507340510302437\n",
      "train loss:0.002067667774741699\n",
      "train loss:0.0007662748966641281\n",
      "train loss:0.003941999974154341\n",
      "train loss:0.009085909588716702\n",
      "train loss:0.022426566691768596\n",
      "train loss:4.8879957128854964e-05\n",
      "train loss:0.0007947087670131516\n",
      "train loss:0.008081137912972759\n",
      "train loss:0.002505713531002759\n",
      "train loss:0.0009494026345722126\n",
      "train loss:0.0053214358459235155\n",
      "train loss:0.0012480511929233848\n",
      "train loss:0.0001231285421372855\n",
      "train loss:0.0012447168856055731\n",
      "train loss:0.0010847747432601988\n",
      "train loss:0.002366485523514228\n",
      "train loss:5.971887435999617e-05\n",
      "train loss:0.0007372846557746582\n",
      "train loss:0.004692177618246576\n",
      "train loss:0.006038317524109389\n",
      "train loss:0.0037373858636069656\n",
      "train loss:0.00821768731341433\n",
      "train loss:0.0027187296193758666\n",
      "train loss:0.0036627093454489152\n",
      "train loss:0.0011734743443635512\n",
      "train loss:0.0017376321845644419\n",
      "train loss:2.0885019931193117e-05\n",
      "train loss:0.0009259404730190924\n",
      "train loss:0.007781387793587996\n",
      "train loss:0.004760670718653434\n",
      "train loss:0.0021383936310786925\n",
      "train loss:0.0014518462170701363\n",
      "train loss:0.001586312986809457\n",
      "train loss:7.822232170282377e-05\n",
      "train loss:0.0004511117529796787\n",
      "train loss:0.00036346970402459307\n",
      "train loss:0.008128378602321372\n",
      "train loss:0.0009087557887022361\n",
      "train loss:0.0021561008693200935\n",
      "train loss:0.0004210394159545833\n",
      "train loss:0.0006098436750631171\n",
      "train loss:5.161759680540426e-05\n",
      "train loss:0.004392701038063669\n",
      "train loss:0.007025947159149235\n",
      "train loss:0.00044786300771361946\n",
      "train loss:0.0010315375632375047\n",
      "train loss:0.0006999234014500462\n",
      "train loss:0.0011269694168100223\n",
      "train loss:0.0010251458661480316\n",
      "train loss:0.0036998288460406183\n",
      "train loss:0.0038200692585743205\n",
      "train loss:0.004278778710809548\n",
      "train loss:0.0015151966191543366\n",
      "train loss:0.0005913904351822395\n",
      "train loss:0.004970393414199695\n",
      "train loss:0.002847558817619726\n",
      "train loss:0.00335108735049062\n",
      "train loss:0.002592770544764807\n",
      "train loss:0.0006818747713106056\n",
      "train loss:0.00013505070360597454\n",
      "train loss:0.002934261210855828\n",
      "train loss:0.0007363339963748661\n",
      "train loss:0.0007099562593427928\n",
      "train loss:0.0013493425480542243\n",
      "train loss:0.005737237187597306\n",
      "train loss:0.0036445809689363575\n",
      "train loss:0.0002044443340674315\n",
      "train loss:0.0029173135550294394\n",
      "train loss:0.0014233746066548447\n",
      "train loss:0.0021570202874188556\n",
      "train loss:2.05611903561321e-05\n",
      "train loss:0.0006562639607341595\n",
      "train loss:0.00048183876895854083\n",
      "train loss:0.0008065688228469988\n",
      "train loss:0.0003955964441691316\n",
      "train loss:0.0019289442318068146\n",
      "train loss:0.001955884119067226\n",
      "train loss:0.00010833383950768907\n",
      "train loss:0.0031631600660845614\n",
      "train loss:0.0008851499546814444\n",
      "train loss:0.001595219210207716\n",
      "train loss:0.0005313360531946874\n",
      "train loss:0.0024249193988791448\n",
      "train loss:0.0013482291203474675\n",
      "train loss:0.005248338683183345\n",
      "train loss:0.0007292879266998733\n",
      "train loss:0.0010714020186919704\n",
      "train loss:0.0004916382963863242\n",
      "train loss:0.0018726894013910878\n",
      "train loss:0.00014875241490551057\n",
      "train loss:0.0009114790582084551\n",
      "train loss:0.0012264631869093158\n",
      "train loss:0.003250407575305734\n",
      "train loss:0.002838292534227662\n",
      "train loss:0.0019663727057757153\n",
      "train loss:0.00038218803799335184\n",
      "train loss:0.00039209862504074086\n",
      "train loss:0.0025507738612923765\n",
      "train loss:0.005331510158592187\n",
      "train loss:0.001413660577227862\n",
      "train loss:0.0009212982742308643\n",
      "train loss:0.003947191341495075\n",
      "train loss:0.0016585727749066068\n",
      "train loss:0.005940346930828015\n",
      "train loss:0.013337405116978973\n",
      "train loss:0.0015520127662535183\n",
      "train loss:0.0010509559073973685\n",
      "train loss:0.0002765848152151852\n",
      "train loss:0.0027716850698186785\n",
      "train loss:3.331136965369177e-05\n",
      "train loss:0.0005689618317011189\n",
      "train loss:0.00020579445892636412\n",
      "train loss:0.00039185228673324917\n",
      "train loss:0.00034269311962220156\n",
      "train loss:0.0036426617893342617\n",
      "train loss:0.00019911882308410302\n",
      "train loss:0.00022082410204149992\n",
      "train loss:0.00017882631465974397\n",
      "train loss:5.643800421950303e-05\n",
      "train loss:0.0027143335698604494\n",
      "train loss:0.0012334675571989589\n",
      "train loss:0.0001985046338783868\n",
      "train loss:0.0005399117029018884\n",
      "train loss:0.0001898572538108269\n",
      "train loss:0.0025198767887884364\n",
      "train loss:0.005898036491052916\n",
      "train loss:0.0006554418731786038\n",
      "train loss:0.0008170254633446479\n",
      "train loss:0.0015978872241816972\n",
      "train loss:4.5935648579065264e-05\n",
      "train loss:0.0001797206157567444\n",
      "train loss:0.004068643183213572\n",
      "train loss:0.0013366574551648108\n",
      "train loss:0.0002659455066535137\n",
      "train loss:0.002377069269694044\n",
      "train loss:0.0009648152193867367\n",
      "train loss:0.0007526234167414162\n",
      "train loss:0.0003664336616697681\n",
      "train loss:0.0026448193852988077\n",
      "train loss:0.001371912712927853\n",
      "train loss:0.0044022719508138184\n",
      "train loss:0.00011704081242651055\n",
      "train loss:0.00016810010978909626\n",
      "train loss:0.0004963293233807928\n",
      "train loss:0.0002963674270958844\n",
      "train loss:0.00040917925548062604\n",
      "train loss:7.542053340966207e-05\n",
      "train loss:0.0014527297606501967\n",
      "train loss:0.001956981273757727\n",
      "train loss:0.0008775158756898194\n",
      "train loss:0.0022849087936512654\n",
      "train loss:0.0030409863486306875\n",
      "train loss:0.002894900976262001\n",
      "train loss:0.0038653602659315866\n",
      "train loss:0.001475368523502157\n",
      "train loss:0.0066319870954850155\n",
      "train loss:0.0008782638588816982\n",
      "train loss:0.001952375861462352\n",
      "train loss:0.0001854198307932098\n",
      "train loss:0.0008389592070041464\n",
      "train loss:0.001015754829512937\n",
      "train loss:0.0006310200293870568\n",
      "train loss:6.251479083113196e-05\n",
      "train loss:0.0014562126043668965\n",
      "train loss:0.0024671536141096864\n",
      "train loss:0.0002643001620820072\n",
      "train loss:0.0007277510015677194\n",
      "train loss:0.01497802769693929\n",
      "train loss:0.00026125115363622234\n",
      "train loss:0.0002518370280570883\n",
      "train loss:0.00027525140752237854\n",
      "train loss:0.0016264437731891466\n",
      "train loss:0.0013846295200888844\n",
      "train loss:0.0017130369095146683\n",
      "train loss:0.0015137723752606284\n",
      "train loss:0.00012807727990316126\n",
      "train loss:0.0002021203877550293\n",
      "train loss:0.001298471484822825\n",
      "train loss:0.0019048897336686572\n",
      "train loss:0.0040987715537540974\n",
      "train loss:0.00034467191667610986\n",
      "train loss:0.00013864213958963135\n",
      "train loss:0.0029766645443661016\n",
      "train loss:0.0002701950645593573\n",
      "train loss:0.0001241901502689531\n",
      "train loss:0.0013486528958308003\n",
      "train loss:0.00222474915293406\n",
      "train loss:0.0006738406278039696\n",
      "train loss:0.0005820258220885558\n",
      "train loss:3.0543199074775364e-05\n",
      "train loss:0.0006503948202354491\n",
      "train loss:0.00048418278297092344\n",
      "train loss:0.0017396487376064899\n",
      "train loss:0.0010858218869137315\n",
      "train loss:0.0038016691447802825\n",
      "train loss:0.002106582971333079\n",
      "train loss:3.9039390842354754e-05\n",
      "train loss:0.00022817916455312073\n",
      "train loss:0.0013009715726327853\n",
      "train loss:0.005774680346858923\n",
      "train loss:0.00038871736850303214\n",
      "train loss:0.00047070001024214996\n",
      "train loss:9.605093272488605e-05\n",
      "train loss:0.0008408795773721247\n",
      "train loss:0.001137556661582265\n",
      "train loss:0.0009280564773452909\n",
      "train loss:0.0016202456876969005\n",
      "train loss:3.8173674889838786e-05\n",
      "train loss:0.0005515359586230525\n",
      "train loss:0.0043005878843927784\n",
      "=== epoch:17, train acc:0.999, test acc:0.983 ===\n",
      "train loss:0.0030632865821960914\n",
      "train loss:0.00027351865660870273\n",
      "train loss:0.0014531916734219629\n",
      "train loss:0.0004471398370870992\n",
      "train loss:0.002821249028126955\n",
      "train loss:0.0012164030346258484\n",
      "train loss:8.548916771928832e-05\n",
      "train loss:0.0010010390673305682\n",
      "train loss:0.00013515116758947539\n",
      "train loss:0.0006201050647842606\n",
      "train loss:0.0003480839603300761\n",
      "train loss:0.001371856345394341\n",
      "train loss:0.0007694080123893716\n",
      "train loss:0.007528956839414742\n",
      "train loss:0.0013765254299851599\n",
      "train loss:0.0006366551454875317\n",
      "train loss:0.00942709524496189\n",
      "train loss:0.0015470857674378966\n",
      "train loss:0.0009306362157265813\n",
      "train loss:0.005046821886993465\n",
      "train loss:5.532805170438803e-05\n",
      "train loss:0.0007240284104280647\n",
      "train loss:0.0019085107166173115\n",
      "train loss:0.00011532521161642353\n",
      "train loss:8.467257309661088e-05\n",
      "train loss:0.0008330150701375653\n",
      "train loss:0.001128055540730504\n",
      "train loss:0.0003163604785568826\n",
      "train loss:0.001453043805410618\n",
      "train loss:0.004062044357023557\n",
      "train loss:0.0025337073422401425\n",
      "train loss:0.0009382592270976875\n",
      "train loss:0.0006536181475581243\n",
      "train loss:0.002530260943951647\n",
      "train loss:0.000331069554149779\n",
      "train loss:0.0014875064579041827\n",
      "train loss:0.002283484038787532\n",
      "train loss:0.0007560518432355966\n",
      "train loss:0.005197637631111711\n",
      "train loss:0.0020670717291133398\n",
      "train loss:0.001592461658086461\n",
      "train loss:0.0002874543855974615\n",
      "train loss:0.0004820595759457831\n",
      "train loss:0.0011208539440974725\n",
      "train loss:9.355998001958525e-05\n",
      "train loss:0.00013908053973317217\n",
      "train loss:0.0011919955441213913\n",
      "train loss:0.004212614941170895\n",
      "train loss:0.0006550081859984018\n",
      "train loss:0.00014593997671268896\n",
      "train loss:9.190785871264641e-05\n",
      "train loss:0.0013649778354846865\n",
      "train loss:0.0010602052474324318\n",
      "train loss:0.0004085480955007\n",
      "train loss:0.0001995777363546617\n",
      "train loss:0.002138193194448532\n",
      "train loss:0.0003452183696873678\n",
      "train loss:0.00013234677705269913\n",
      "train loss:0.0026890904185979036\n",
      "train loss:0.0003585906668142805\n",
      "train loss:0.0015420381887591911\n",
      "train loss:0.0010883198306297893\n",
      "train loss:0.0009667094845119276\n",
      "train loss:0.003353140234406419\n",
      "train loss:0.0014673324413803845\n",
      "train loss:0.0002831385847481562\n",
      "train loss:0.00015528931541704917\n",
      "train loss:0.00014889703822668256\n",
      "train loss:0.00033676514487285573\n",
      "train loss:0.0004429775634423564\n",
      "train loss:0.0001085804357840408\n",
      "train loss:0.0034598866292180576\n",
      "train loss:0.0005835270801687386\n",
      "train loss:0.0003764298710502556\n",
      "train loss:1.3244369375561024e-05\n",
      "train loss:0.0007624583430873688\n",
      "train loss:0.00017669394381771727\n",
      "train loss:0.0006379790231330981\n",
      "train loss:0.0007133721103476212\n",
      "train loss:6.044700915121785e-05\n",
      "train loss:0.0003143404478969765\n",
      "train loss:6.155886144877169e-05\n",
      "train loss:0.0015747996384111904\n",
      "train loss:0.002293805377421221\n",
      "train loss:0.0009328899336265419\n",
      "train loss:0.0016470180965710765\n",
      "train loss:0.0006999504605310079\n",
      "train loss:0.0012422275238225915\n",
      "train loss:0.00011249826866002665\n",
      "train loss:0.00040720333999101394\n",
      "train loss:0.00031780307899893117\n",
      "train loss:0.0017458091522841348\n",
      "train loss:0.001328772423418691\n",
      "train loss:0.005024402631408792\n",
      "train loss:0.0006581884722235077\n",
      "train loss:0.0005103373369917051\n",
      "train loss:0.0006690537906702419\n",
      "train loss:0.00016774337017378907\n",
      "train loss:0.0034775637355804008\n",
      "train loss:0.0013588515527610634\n",
      "train loss:0.0008802173521397531\n",
      "train loss:0.0006950957442372997\n",
      "train loss:0.0012674343415860383\n",
      "train loss:0.0001437948435308207\n",
      "train loss:0.0016157617224703846\n",
      "train loss:0.001185635380513098\n",
      "train loss:0.00013948943643393724\n",
      "train loss:0.002106014860615256\n",
      "train loss:0.00031299684165694884\n",
      "train loss:0.001638912233597238\n",
      "train loss:0.0010599740633951825\n",
      "train loss:7.369204795086593e-05\n",
      "train loss:0.0002834883531201147\n",
      "train loss:0.0006852453312490366\n",
      "train loss:4.807824680737167e-05\n",
      "train loss:0.0002508991712868272\n",
      "train loss:0.00025529641734780103\n",
      "train loss:0.0010009724374169886\n",
      "train loss:0.0004113945063581476\n",
      "train loss:0.0027653437409240916\n",
      "train loss:9.426030958555125e-05\n",
      "train loss:0.0008770679628286474\n",
      "train loss:0.0002023310307579976\n",
      "train loss:0.001234310552583909\n",
      "train loss:0.0010457044390967045\n",
      "train loss:0.006883379793291442\n",
      "train loss:0.0016279680562203231\n",
      "train loss:0.007616319949649294\n",
      "train loss:0.00033644662607638687\n",
      "train loss:0.0005604438742884575\n",
      "train loss:0.00021401590304990053\n",
      "train loss:0.0051053688754020546\n",
      "train loss:0.001971736283725858\n",
      "train loss:0.0008719798402455244\n",
      "train loss:0.0013366495571739438\n",
      "train loss:0.002193056908898222\n",
      "train loss:0.0034841194792878394\n",
      "train loss:0.0007678138032383055\n",
      "train loss:0.0009641918055273408\n",
      "train loss:0.0004394214627106309\n",
      "train loss:0.0007205214471783429\n",
      "train loss:0.0028265231580259453\n",
      "train loss:0.00021379611905834856\n",
      "train loss:0.0003265336053822589\n",
      "train loss:0.018403085423921894\n",
      "train loss:0.00025010141532306806\n",
      "train loss:0.00047510748837783167\n",
      "train loss:0.0005517958236660127\n",
      "train loss:0.0023512322933264863\n",
      "train loss:0.003394307366625162\n",
      "train loss:0.00941008983746279\n",
      "train loss:0.004020404174014756\n",
      "train loss:0.0001758409329331852\n",
      "train loss:0.0002272588404837923\n",
      "train loss:0.0003264995194593177\n",
      "train loss:0.000634554919653534\n",
      "train loss:0.0077064396793274995\n",
      "train loss:0.0011444158209756776\n",
      "train loss:0.0015905739741680744\n",
      "train loss:0.003243306929111026\n",
      "train loss:0.0006458782176488425\n",
      "train loss:0.0001853666340822123\n",
      "train loss:0.0017755799974903182\n",
      "train loss:0.00013141728342477193\n",
      "train loss:0.001778420890218053\n",
      "train loss:0.010149671601788582\n",
      "train loss:0.0002494803607620252\n",
      "train loss:0.0008478395843376671\n",
      "train loss:0.0017071169240975225\n",
      "train loss:0.0014973489258207954\n",
      "train loss:0.00022577311781708535\n",
      "train loss:0.0035739593602203035\n",
      "train loss:0.01258387636777922\n",
      "train loss:0.0017391380329391754\n",
      "train loss:0.00022512463800124433\n",
      "train loss:9.668195659039617e-05\n",
      "train loss:0.0016024396064292362\n",
      "train loss:0.0017517083558246096\n",
      "train loss:0.0015509363678507713\n",
      "train loss:0.0024595860426121067\n",
      "train loss:0.015351822789677306\n",
      "train loss:0.0013481473385831672\n",
      "train loss:0.0010321225150868706\n",
      "train loss:0.00020750897364455462\n",
      "train loss:0.00025272449472707676\n",
      "train loss:0.0002542883153282326\n",
      "train loss:0.0023879429564889324\n",
      "train loss:0.0005243811077481882\n",
      "train loss:0.0026390810700885653\n",
      "train loss:0.0005442745662988971\n",
      "train loss:0.036170443545798035\n",
      "train loss:0.0015736006577391846\n",
      "train loss:0.00014947918768994938\n",
      "train loss:0.002545269423487777\n",
      "train loss:0.00045547697431130347\n",
      "train loss:0.003699220647427997\n",
      "train loss:0.0026600242899606236\n",
      "train loss:0.0030463574605647586\n",
      "train loss:0.003017006052628444\n",
      "train loss:0.000766690863524996\n",
      "train loss:0.006506937567992016\n",
      "train loss:0.004056692390555499\n",
      "train loss:0.011910288980343502\n",
      "train loss:0.0005046665517484407\n",
      "train loss:0.001995091349296256\n",
      "train loss:0.000894462930667048\n",
      "train loss:9.34945098693855e-05\n",
      "train loss:0.0026468030198235444\n",
      "train loss:0.0019215526314583491\n",
      "train loss:0.01734736873437597\n",
      "train loss:0.000532871460023424\n",
      "train loss:0.003986536343806454\n",
      "train loss:0.0008218240015638846\n",
      "train loss:0.003956099313858141\n",
      "train loss:0.0002900509008714777\n",
      "train loss:0.001944129636686394\n",
      "train loss:0.01527351390409867\n",
      "train loss:0.001669666374200944\n",
      "train loss:0.0038675875378998553\n",
      "train loss:0.0010126733302684354\n",
      "train loss:0.0003886001901860791\n",
      "train loss:0.0030106474876161068\n",
      "train loss:0.004307018948282824\n",
      "train loss:0.0009851902792367927\n",
      "train loss:0.0027273360180287893\n",
      "train loss:0.004145379135674299\n",
      "train loss:0.0019747625084289813\n",
      "train loss:0.00020655876897574538\n",
      "train loss:0.0014994629221056236\n",
      "train loss:0.0014320838389174507\n",
      "train loss:0.0008802856728855253\n",
      "train loss:0.0016281059656403029\n",
      "train loss:0.0017260625331180215\n",
      "train loss:0.002353464392504612\n",
      "train loss:0.0017095283039699457\n",
      "train loss:0.0006379608013769625\n",
      "train loss:0.00011131087898003923\n",
      "train loss:0.000129276664279095\n",
      "train loss:0.002037815296396239\n",
      "train loss:0.0005056724887918175\n",
      "train loss:0.0007478525853793812\n",
      "train loss:0.007283701377744768\n",
      "train loss:0.0006617506222632901\n",
      "train loss:0.00020275790813564958\n",
      "train loss:0.0004421207573393829\n",
      "train loss:0.0029452330868185734\n",
      "train loss:2.9617527870334386e-05\n",
      "train loss:0.003362721746641571\n",
      "train loss:0.004163739130290761\n",
      "train loss:0.00026447803227852665\n",
      "train loss:0.0007091918675130116\n",
      "train loss:0.0016825843954313274\n",
      "train loss:0.000645562469473832\n",
      "train loss:0.0010362328837523163\n",
      "train loss:0.0014663402156922145\n",
      "train loss:0.0013863771436092513\n",
      "train loss:0.0018498226300798126\n",
      "train loss:0.000908575211196012\n",
      "train loss:0.001433436492437358\n",
      "train loss:0.00024174608132994678\n",
      "train loss:0.00127944486958816\n",
      "train loss:0.0006152138542463567\n",
      "train loss:0.002331966928069191\n",
      "train loss:0.002645201125232849\n",
      "train loss:0.0015058056855908222\n",
      "train loss:0.004248704627173376\n",
      "train loss:0.001518282285469268\n",
      "train loss:0.001523085650055912\n",
      "train loss:0.004680058440470219\n",
      "train loss:0.004947326828946171\n",
      "train loss:0.000277644062624311\n",
      "train loss:0.002306665668158356\n",
      "train loss:0.0007209364580169\n",
      "train loss:0.001239525880187278\n",
      "train loss:0.001043119176302508\n",
      "train loss:4.5425489201499344e-05\n",
      "train loss:0.008067520855180678\n",
      "train loss:0.00017840908113685345\n",
      "train loss:0.002144008009921662\n",
      "train loss:0.002623928210594307\n",
      "train loss:0.002939453416747606\n",
      "train loss:8.194432657038789e-05\n",
      "train loss:0.0012248690584207283\n",
      "train loss:0.004854743455420948\n",
      "train loss:0.0022252123813807125\n",
      "train loss:0.004952929263065133\n",
      "train loss:0.0011805791486261582\n",
      "train loss:0.0017577975982788884\n",
      "train loss:0.0007968520074241482\n",
      "train loss:0.0004275949415100398\n",
      "train loss:0.000597551139002613\n",
      "train loss:0.0001717102004137862\n",
      "train loss:0.0018818733626838227\n",
      "train loss:0.0012511005768376935\n",
      "train loss:0.0006223137269336447\n",
      "train loss:0.0016198490794395422\n",
      "train loss:0.0017144102943305029\n",
      "train loss:0.0012389664865911337\n",
      "train loss:0.0005379351405624718\n",
      "train loss:0.0013122246157502475\n",
      "train loss:0.00046182019038783196\n",
      "train loss:2.2441321342617735e-05\n",
      "train loss:0.0008146512305741398\n",
      "train loss:0.0011252296534622112\n",
      "train loss:0.001692811112034552\n",
      "train loss:3.035149426320348e-05\n",
      "train loss:0.00613414302004532\n",
      "train loss:0.00011155774438506528\n",
      "train loss:2.9965680867460884e-05\n",
      "train loss:0.0009160936288277615\n",
      "train loss:0.0003081823178531964\n",
      "train loss:7.37872101702131e-05\n",
      "train loss:0.0009637041789433286\n",
      "train loss:0.0010848439018238164\n",
      "train loss:0.000369536152032165\n",
      "train loss:0.00034429730327603826\n",
      "train loss:0.010645633445005504\n",
      "train loss:0.0028278493348114523\n",
      "train loss:0.000759890802727814\n",
      "train loss:0.003901425183526036\n",
      "train loss:0.0030060386585588268\n",
      "train loss:0.00047440428687138477\n",
      "train loss:0.002888364809390269\n",
      "train loss:0.0005485511024660515\n",
      "train loss:0.0006540739548537943\n",
      "train loss:0.008711759071370105\n",
      "train loss:4.9006984674622775e-05\n",
      "train loss:0.0015319940935136242\n",
      "train loss:0.0005922605506992457\n",
      "train loss:0.0010167249926259963\n",
      "train loss:0.00027457976722947277\n",
      "train loss:0.0005042943815080304\n",
      "train loss:3.054757311124719e-05\n",
      "train loss:0.0009603625681298464\n",
      "train loss:0.0004790171721612839\n",
      "train loss:0.0002073298857831577\n",
      "train loss:0.0014493103892673142\n",
      "train loss:0.0007129000031578441\n",
      "train loss:0.001115789720914524\n",
      "train loss:0.0004038726444346806\n",
      "train loss:0.0013886507196188944\n",
      "train loss:0.002592694641776642\n",
      "train loss:0.0002997728668954592\n",
      "train loss:0.0041351715349990685\n",
      "train loss:0.00046202772367405476\n",
      "train loss:0.00011822176721780146\n",
      "train loss:0.0013137552942214456\n",
      "train loss:0.021413374967223394\n",
      "train loss:0.006570933596802381\n",
      "train loss:0.0008105135864757597\n",
      "train loss:2.769199252002984e-05\n",
      "train loss:0.00150215282636999\n",
      "train loss:0.0013629177877202204\n",
      "train loss:0.0010045774784208385\n",
      "train loss:0.00045069372517952526\n",
      "train loss:0.00029966654780360405\n",
      "train loss:0.0004053530638876877\n",
      "train loss:0.003931043456432468\n",
      "train loss:0.0015685905399393891\n",
      "train loss:0.0006717431276703827\n",
      "train loss:0.0004512074573001769\n",
      "train loss:0.0007881643114798034\n",
      "train loss:0.0007976781255194248\n",
      "train loss:0.0011035689680300327\n",
      "train loss:0.0063637845763827305\n",
      "train loss:0.0002408307486426029\n",
      "train loss:0.0006809756421449931\n",
      "train loss:0.0005825138396404023\n",
      "train loss:0.0007031405203989084\n",
      "train loss:0.0007567530154939321\n",
      "train loss:0.004259902117056994\n",
      "train loss:0.002481118679468402\n",
      "train loss:0.0006951167596684657\n",
      "train loss:0.00010235441365160001\n",
      "train loss:0.0006703345863756497\n",
      "train loss:0.0009128724982599256\n",
      "train loss:0.045518782137209966\n",
      "train loss:0.001058139735531118\n",
      "train loss:0.00010533398707881462\n",
      "train loss:0.0009725530154912186\n",
      "train loss:0.00015441495672408026\n",
      "train loss:0.04357301749905369\n",
      "train loss:0.0011207854336819166\n",
      "train loss:0.001401930936529905\n",
      "train loss:0.0021959564562322566\n",
      "train loss:0.0014180443189296113\n",
      "train loss:0.000982042748454225\n",
      "train loss:0.0001614740753336893\n",
      "train loss:0.0019304375668871177\n",
      "train loss:0.0015220591234787451\n",
      "train loss:0.0017581806250717485\n",
      "train loss:4.370560228889576e-05\n",
      "train loss:0.00018861844234759756\n",
      "train loss:0.003221384476490029\n",
      "train loss:0.002058896021091577\n",
      "train loss:0.0005293070391215681\n",
      "train loss:0.0006564412781317415\n",
      "train loss:0.0011005976884258918\n",
      "train loss:8.700200070952119e-05\n",
      "train loss:0.0005786585904407726\n",
      "train loss:0.0012070848518323312\n",
      "train loss:0.0002630155581941774\n",
      "train loss:0.003328593481740896\n",
      "train loss:8.999994361918678e-05\n",
      "train loss:0.002734535407887658\n",
      "train loss:0.005587120609069748\n",
      "train loss:0.003723357009927653\n",
      "train loss:0.00016457191671607162\n",
      "train loss:0.0012282200128889398\n",
      "train loss:0.0002493536415473193\n",
      "train loss:0.001424475738935481\n",
      "train loss:0.00018066096763059514\n",
      "train loss:0.0030281221440541777\n",
      "train loss:0.000371867844538478\n",
      "train loss:0.0009640477325460314\n",
      "train loss:1.9354001228586177e-05\n",
      "train loss:0.003508969734857625\n",
      "train loss:0.0006210686565468161\n",
      "train loss:0.006327691096877245\n",
      "train loss:0.0012135617059948172\n",
      "train loss:0.00021451284016697685\n",
      "train loss:0.0003266909961717302\n",
      "train loss:0.0006704848245968765\n",
      "train loss:0.005046253435828604\n",
      "train loss:0.0006489248599219067\n",
      "train loss:0.00019706171967759454\n",
      "train loss:0.0003746940283370158\n",
      "train loss:3.28378313843412e-05\n",
      "train loss:8.955755029981381e-05\n",
      "train loss:0.006700653198103712\n",
      "train loss:0.0009027982087665376\n",
      "train loss:0.0008145742016606862\n",
      "train loss:4.443373307781372e-05\n",
      "train loss:0.0011997092235901882\n",
      "train loss:0.0013383892436984505\n",
      "train loss:0.0014224386896072453\n",
      "train loss:0.011973576069814579\n",
      "train loss:0.0065145623814266565\n",
      "train loss:5.7726761889119945e-05\n",
      "train loss:0.004468737955177581\n",
      "train loss:0.006773487784668863\n",
      "train loss:0.0018595990929295189\n",
      "train loss:0.0013500733999461637\n",
      "train loss:0.0017300611035229085\n",
      "train loss:0.0003141683013180267\n",
      "train loss:0.0026165165358131137\n",
      "train loss:0.0012036180469455122\n",
      "train loss:0.000317241947535955\n",
      "train loss:0.0010757827343177585\n",
      "train loss:0.0025448048200550012\n",
      "train loss:0.004896956816402875\n",
      "train loss:0.006478422146826645\n",
      "train loss:0.004130614601551717\n",
      "train loss:0.00039476362116822863\n",
      "train loss:0.004518301842469752\n",
      "train loss:0.0008086524715699886\n",
      "train loss:0.001792706238183599\n",
      "train loss:0.0023338406336025487\n",
      "train loss:0.004109273206822695\n",
      "train loss:0.00010780559076710998\n",
      "train loss:0.0021136990098239946\n",
      "train loss:0.0025683857837115864\n",
      "train loss:0.0006640201698057278\n",
      "train loss:0.0016530014438481773\n",
      "train loss:0.0038421854785155233\n",
      "train loss:0.016443183373752952\n",
      "train loss:0.0028847328936549206\n",
      "train loss:0.0011974964042633916\n",
      "train loss:0.0777808680978879\n",
      "train loss:0.00030372396892399696\n",
      "train loss:8.632862727437176e-05\n",
      "train loss:0.0009427266941540998\n",
      "train loss:0.0003305456284200725\n",
      "train loss:0.00114227113483326\n",
      "train loss:0.0020145460161730476\n",
      "train loss:0.0025847139730082597\n",
      "train loss:0.004069014197004447\n",
      "train loss:0.0008925411714552962\n",
      "train loss:0.0010118868763473758\n",
      "train loss:0.004914869362303644\n",
      "train loss:0.0017550109741818456\n",
      "train loss:0.0026034685484163087\n",
      "train loss:6.389905387919174e-05\n",
      "train loss:0.0031787034133947813\n",
      "train loss:0.00016602987312859917\n",
      "train loss:0.003920129343728195\n",
      "train loss:0.0009920969327992298\n",
      "train loss:0.0001350775642584922\n",
      "train loss:0.0006747958733651513\n",
      "train loss:0.00025869538782662815\n",
      "train loss:0.002496066336353746\n",
      "train loss:0.00028447512851325073\n",
      "train loss:0.00012316309199067276\n",
      "train loss:0.0043544104565839984\n",
      "train loss:0.01275264986210221\n",
      "train loss:0.0004276685433275614\n",
      "train loss:0.00040834634879042284\n",
      "train loss:0.005455612764858462\n",
      "train loss:0.00013272488556473883\n",
      "train loss:0.003602505930927843\n",
      "train loss:0.0009078738650588711\n",
      "train loss:0.002194986872097548\n",
      "train loss:0.00301889766747229\n",
      "train loss:0.0002025145912749085\n",
      "train loss:0.012681856537092607\n",
      "train loss:0.00020908493384247338\n",
      "train loss:0.00014539660218327395\n",
      "train loss:0.0031831152402867565\n",
      "train loss:0.0012024475494542765\n",
      "train loss:0.001977573191827375\n",
      "train loss:0.0007938745658872095\n",
      "train loss:0.0018592226436477768\n",
      "train loss:0.0037037896266675285\n",
      "train loss:0.002339988482655705\n",
      "train loss:0.0006160269656355682\n",
      "train loss:0.0007865877708115789\n",
      "train loss:0.0012333750832417589\n",
      "train loss:0.0008565890677853923\n",
      "train loss:0.0040332568480395585\n",
      "train loss:0.0007738885350766217\n",
      "train loss:0.00038845581183827673\n",
      "train loss:0.0002098911573894212\n",
      "train loss:0.0012538193179138422\n",
      "train loss:0.0021206348374488543\n",
      "train loss:0.008025931457074333\n",
      "train loss:0.0012478733215733262\n",
      "train loss:0.0002910208950150369\n",
      "train loss:0.0004256948695003324\n",
      "train loss:0.0002516917387932298\n",
      "train loss:0.005509837235020014\n",
      "train loss:0.0016496465956431516\n",
      "train loss:0.0005393588318010795\n",
      "train loss:0.009246086335156934\n",
      "train loss:0.0005774447973253766\n",
      "train loss:0.0005337750646293575\n",
      "train loss:0.0036565911093596488\n",
      "train loss:0.000225424740251511\n",
      "train loss:0.0018136559938698232\n",
      "train loss:0.0036912709206221154\n",
      "train loss:0.04518756755143663\n",
      "train loss:0.0005666676299355505\n",
      "train loss:0.0008086652608899879\n",
      "train loss:0.0022271197751779744\n",
      "train loss:0.0040673909533295955\n",
      "train loss:0.0012517191770702138\n",
      "train loss:0.0020322245438682894\n",
      "train loss:0.0010862630780699638\n",
      "train loss:0.0048885521770913225\n",
      "train loss:0.0015069099656730605\n",
      "train loss:0.0015146703723059062\n",
      "train loss:0.002292483198421106\n",
      "train loss:0.00030944179757873806\n",
      "train loss:0.0010858805899070722\n",
      "train loss:0.0009225768956170685\n",
      "train loss:0.0024788647248400035\n",
      "train loss:0.0029058625775272725\n",
      "train loss:0.0016731442130487322\n",
      "train loss:0.005145860848326613\n",
      "train loss:0.0009938851700417488\n",
      "train loss:0.0004451931114974759\n",
      "train loss:0.006044814894782429\n",
      "train loss:0.000985603554964197\n",
      "train loss:0.0032807002488841807\n",
      "train loss:0.001512979829118016\n",
      "train loss:0.0003832669865759064\n",
      "train loss:0.0007296296912139579\n",
      "train loss:0.0009176314404015573\n",
      "train loss:0.0005529918345785938\n",
      "train loss:0.002244128844010517\n",
      "train loss:0.0005379218565438689\n",
      "train loss:0.000433569262066431\n",
      "train loss:0.0017545444754292084\n",
      "train loss:0.00042781032515669127\n",
      "train loss:0.00034699253592416205\n",
      "train loss:0.0017739933923680143\n",
      "train loss:0.00274718620369687\n",
      "train loss:0.0014494263475762798\n",
      "train loss:0.00020965604789765967\n",
      "train loss:0.0003294491208138539\n",
      "train loss:0.0009939947298106037\n",
      "train loss:0.0005192573319809488\n",
      "train loss:0.0009824779950424226\n",
      "train loss:0.0019739855766511445\n",
      "train loss:0.0006741571304620217\n",
      "train loss:0.00011272920560594562\n",
      "train loss:0.00017283212495021488\n",
      "train loss:0.00035636298323227456\n",
      "train loss:0.0006872093556070405\n",
      "train loss:0.0009832096841520387\n",
      "train loss:0.00024955058019955606\n",
      "train loss:0.00119719947845153\n",
      "train loss:0.021288893860927253\n",
      "train loss:0.0001056149029106471\n",
      "train loss:0.0015758056720426754\n",
      "train loss:5.981925090330453e-05\n",
      "train loss:0.0007708156935041382\n",
      "train loss:0.00010344488964268769\n",
      "train loss:0.0007699090838020755\n",
      "train loss:0.009618788582591005\n",
      "train loss:0.0025315228659142193\n",
      "=== epoch:18, train acc:1.0, test acc:0.986 ===\n",
      "train loss:0.0007738130292016675\n",
      "train loss:0.0008777053255933621\n",
      "train loss:0.00028031768639944247\n",
      "train loss:0.0007517785935232291\n",
      "train loss:0.00012212041561300398\n",
      "train loss:0.001645607861625118\n",
      "train loss:0.0008130225047643092\n",
      "train loss:0.0003960384544742351\n",
      "train loss:0.0005079410438891283\n",
      "train loss:0.002432845828571564\n",
      "train loss:0.003584750899114208\n",
      "train loss:0.0016629037114665051\n",
      "train loss:0.00022669963323517772\n",
      "train loss:0.005707344505101104\n",
      "train loss:0.002806854936268684\n",
      "train loss:0.0012986170068060008\n",
      "train loss:0.0002879036534350748\n",
      "train loss:0.000904941892866338\n",
      "train loss:0.0005488671847093551\n",
      "train loss:0.0013306977063583516\n",
      "train loss:0.0004607282246325025\n",
      "train loss:0.0033718686850600556\n",
      "train loss:0.002051401109053036\n",
      "train loss:0.0011625118481396743\n",
      "train loss:3.165036477967417e-05\n",
      "train loss:0.0023367098615864248\n",
      "train loss:0.0009956607477663588\n",
      "train loss:0.01923637581491176\n",
      "train loss:0.0006279044001660884\n",
      "train loss:0.0038004577056622146\n",
      "train loss:0.000645920331648521\n",
      "train loss:0.0001581826803069898\n",
      "train loss:0.0025325724079044913\n",
      "train loss:0.005577467774840237\n",
      "train loss:0.003650674944575533\n",
      "train loss:8.343425273574833e-05\n",
      "train loss:0.0005145179442617777\n",
      "train loss:0.000522222793639286\n",
      "train loss:0.0013505308617661187\n",
      "train loss:0.003398115248707863\n",
      "train loss:0.0007668834627432743\n",
      "train loss:0.0011231083340950492\n",
      "train loss:0.0031361587295593396\n",
      "train loss:0.0014246669937987599\n",
      "train loss:0.0020841532258848907\n",
      "train loss:0.00022119089943716815\n",
      "train loss:0.0020485263512950653\n",
      "train loss:0.002648988355289521\n",
      "train loss:0.0008007899929665256\n",
      "train loss:0.00018485424970122858\n",
      "train loss:0.0026087751637482205\n",
      "train loss:0.0002645089338550092\n",
      "train loss:0.0058975051780740554\n",
      "train loss:0.00030241642554885273\n",
      "train loss:0.001982577752563778\n",
      "train loss:0.0017949779715458402\n",
      "train loss:0.006476787982634956\n",
      "train loss:0.00014060700484109322\n",
      "train loss:0.0012448770242371417\n",
      "train loss:0.0010270499107494664\n",
      "train loss:0.010394167985793996\n",
      "train loss:0.0026848935135285663\n",
      "train loss:0.011298084044522245\n",
      "train loss:0.0034088015521586035\n",
      "train loss:0.000687613017289227\n",
      "train loss:0.0057211460890766175\n",
      "train loss:0.00023383926915954923\n",
      "train loss:0.0002258707063736103\n",
      "train loss:0.0009759181118418499\n",
      "train loss:0.0007235112894258705\n",
      "train loss:0.001878764324468689\n",
      "train loss:4.8161663649116945e-05\n",
      "train loss:0.0016448644968853198\n",
      "train loss:0.0010348528192939963\n",
      "train loss:0.0865027595804507\n",
      "train loss:0.0035601269843131295\n",
      "train loss:0.00011532859916810462\n",
      "train loss:0.003263694944145442\n",
      "train loss:0.00031895390848680073\n",
      "train loss:0.002789893506569166\n",
      "train loss:0.0007523552961637381\n",
      "train loss:0.005006699483474244\n",
      "train loss:0.0001954885665489228\n",
      "train loss:0.0006119041422586893\n",
      "train loss:0.0009121371210846403\n",
      "train loss:0.0023948798726482843\n",
      "train loss:6.598996195158515e-05\n",
      "train loss:0.001138855710768403\n",
      "train loss:0.0018182931043164879\n",
      "train loss:0.0011887708848991416\n",
      "train loss:0.004448612513077213\n",
      "train loss:0.013765840726342602\n",
      "train loss:0.01456706974206528\n",
      "train loss:0.00022127590615129048\n",
      "train loss:0.020810281189399083\n",
      "train loss:0.0003184132631117167\n",
      "train loss:0.0016791846347093034\n",
      "train loss:0.0009765307330193328\n",
      "train loss:0.0022696950251943656\n",
      "train loss:0.0027480086545345916\n",
      "train loss:0.0104143632985863\n",
      "train loss:0.004568566446432871\n",
      "train loss:0.049834869706521935\n",
      "train loss:0.003862076633965916\n",
      "train loss:0.002564585017137948\n",
      "train loss:0.0005255174324496414\n",
      "train loss:0.008403178081106718\n",
      "train loss:0.0022900894965140854\n",
      "train loss:0.002031739601162841\n",
      "train loss:7.222104415162687e-05\n",
      "train loss:0.008824329510834086\n",
      "train loss:0.00885491724158918\n",
      "train loss:0.0008874989763656974\n",
      "train loss:0.0014886827448713405\n",
      "train loss:0.0018041565951448055\n",
      "train loss:0.0037988976082423275\n",
      "train loss:0.001850105340237753\n",
      "train loss:0.004991696472635058\n",
      "train loss:0.007939931928362143\n",
      "train loss:0.003649213609085243\n",
      "train loss:0.002681051074243163\n",
      "train loss:0.0017693637458192713\n",
      "train loss:0.031771176647072956\n",
      "train loss:0.00023158979647213484\n",
      "train loss:0.0010205637396790014\n",
      "train loss:0.0007903174595273467\n",
      "train loss:0.00016863996064583685\n",
      "train loss:0.0025914083060830954\n",
      "train loss:0.004578025901058429\n",
      "train loss:0.0008567961658090905\n",
      "train loss:0.0013313490391901137\n",
      "train loss:0.00020209089670365034\n",
      "train loss:0.0017405691204803234\n",
      "train loss:0.0034233920411531203\n",
      "train loss:0.000859520169046665\n",
      "train loss:0.0007062313063875211\n",
      "train loss:0.00031147431662201366\n",
      "train loss:0.015660784555833748\n",
      "train loss:0.0027326669624026384\n",
      "train loss:0.00017480218117890334\n",
      "train loss:0.008437940625240947\n",
      "train loss:0.000950395179406255\n",
      "train loss:0.003126049624785344\n",
      "train loss:0.01647603985267623\n",
      "train loss:0.0013742348088531858\n",
      "train loss:0.003188782030216688\n",
      "train loss:0.0020156129146563727\n",
      "train loss:0.0054557394611217235\n",
      "train loss:0.012322641944739024\n",
      "train loss:0.002520313633209686\n",
      "train loss:0.05504862613055006\n",
      "train loss:0.003159415952622955\n",
      "train loss:0.00035669050975242647\n",
      "train loss:0.0027742142930590307\n",
      "train loss:0.001111213653963259\n",
      "train loss:0.004304428453708057\n",
      "train loss:0.0013530696051783483\n",
      "train loss:6.994501961708214e-05\n",
      "train loss:0.015026478586795266\n",
      "train loss:0.004566392862369079\n",
      "train loss:0.0062502307493647455\n",
      "train loss:0.0007445425950564508\n",
      "train loss:0.06549990529540356\n",
      "train loss:0.010206042620171578\n",
      "train loss:0.0004936665802971185\n",
      "train loss:0.014983431525930273\n",
      "train loss:0.0023521962384098834\n",
      "train loss:0.002473870521775995\n",
      "train loss:0.0029399093106375496\n",
      "train loss:0.0015814638485480948\n",
      "train loss:0.0001949299809421559\n",
      "train loss:0.00015028079352957525\n",
      "train loss:0.0053798216085533535\n",
      "train loss:0.0003467711298119528\n",
      "train loss:0.0015877832944305816\n",
      "train loss:0.002046321718479392\n",
      "train loss:7.558316829158421e-05\n",
      "train loss:0.0020892285526650377\n",
      "train loss:0.0039915442007083775\n",
      "train loss:0.0024672206480213415\n",
      "train loss:0.003449767792942969\n",
      "train loss:0.0010716755855784375\n",
      "train loss:0.0014232645278768818\n",
      "train loss:0.0012068020672168398\n",
      "train loss:0.013997960379061494\n",
      "train loss:0.0006714273632503222\n",
      "train loss:0.005612741455199961\n",
      "train loss:0.00138860700997111\n",
      "train loss:0.004164664725026138\n",
      "train loss:0.0019794974124763445\n",
      "train loss:0.006509095810587554\n",
      "train loss:0.0034555750295436876\n",
      "train loss:0.00014821989775877045\n",
      "train loss:0.002687781529280215\n",
      "train loss:0.002360577161655155\n",
      "train loss:0.00042619785703977483\n",
      "train loss:0.0056413307022827675\n",
      "train loss:0.0009200132468847898\n",
      "train loss:0.0003578159225974219\n",
      "train loss:0.0005959519110647382\n",
      "train loss:2.5915711725124445e-05\n",
      "train loss:0.0011600493973615536\n",
      "train loss:0.0009381308107231434\n",
      "train loss:0.0003736137850659793\n",
      "train loss:0.00041303997638120345\n",
      "train loss:0.0009657752846773968\n",
      "train loss:0.0003229929987874885\n",
      "train loss:0.0029094432401517763\n",
      "train loss:6.131008726731262e-05\n",
      "train loss:0.0016898017062976225\n",
      "train loss:0.0022551131152977423\n",
      "train loss:0.0006477595766517029\n",
      "train loss:0.00012391651831953335\n",
      "train loss:0.004869263098546724\n",
      "train loss:0.0013710894111932115\n",
      "train loss:0.0003873270287609894\n",
      "train loss:0.0006495158684949217\n",
      "train loss:0.002064219670696846\n",
      "train loss:0.006409538312909135\n",
      "train loss:0.0014569548023379388\n",
      "train loss:0.0017303437432836673\n",
      "train loss:0.002792979925092986\n",
      "train loss:0.0012854988633585265\n",
      "train loss:0.002951705347852304\n",
      "train loss:0.00037776528435948335\n",
      "train loss:0.0010000397203447392\n",
      "train loss:0.0018597780645808106\n",
      "train loss:0.0027030714986910963\n",
      "train loss:0.0020890613326742156\n",
      "train loss:0.002873840281797239\n",
      "train loss:0.0004317947010682074\n",
      "train loss:0.0013782539101346984\n",
      "train loss:0.0015168740355577712\n",
      "train loss:0.0014852385172260992\n",
      "train loss:0.0035316581285081113\n",
      "train loss:0.0005322454067420259\n",
      "train loss:0.0029367443520717175\n",
      "train loss:0.00166845159584851\n",
      "train loss:0.000156941582026074\n",
      "train loss:0.0008100973670485164\n",
      "train loss:0.002594068989736788\n",
      "train loss:0.0014277978239190523\n",
      "train loss:0.004332675427243764\n",
      "train loss:0.00010494827327874065\n",
      "train loss:0.000566821646314232\n",
      "train loss:0.0016381947604009936\n",
      "train loss:0.0005060277899427115\n",
      "train loss:0.0025694715047076206\n",
      "train loss:0.00023782146743388512\n",
      "train loss:0.0014017067630411376\n",
      "train loss:0.003818042746996685\n",
      "train loss:0.0010424870059191564\n",
      "train loss:0.0006307311070651227\n",
      "train loss:0.0020945485274894405\n",
      "train loss:7.62395389461087e-05\n",
      "train loss:0.00018807236746178614\n",
      "train loss:0.00042607686659248246\n",
      "train loss:0.001343490680935097\n",
      "train loss:0.00032170282046320794\n",
      "train loss:0.0004562239904279274\n",
      "train loss:0.003070729059746292\n",
      "train loss:0.0026400108169635545\n",
      "train loss:0.000205267186690897\n",
      "train loss:0.001968378239352325\n",
      "train loss:7.588035709134742e-05\n",
      "train loss:7.384578329916505e-05\n",
      "train loss:0.000820635344032316\n",
      "train loss:0.0029833190828413986\n",
      "train loss:0.0036981921372621516\n",
      "train loss:0.000365467683405728\n",
      "train loss:0.00022413723362341547\n",
      "train loss:0.00043063275955023094\n",
      "train loss:0.017430333303891602\n",
      "train loss:0.002864078724707852\n",
      "train loss:0.0030531367857820463\n",
      "train loss:0.0007735648405436255\n",
      "train loss:0.00027063243182083395\n",
      "train loss:0.0021019469420182606\n",
      "train loss:0.0007487766254311408\n",
      "train loss:0.002094255503970356\n",
      "train loss:0.0020755582207061993\n",
      "train loss:0.0005442135378223666\n",
      "train loss:0.0026201661897427213\n",
      "train loss:0.007641062960386818\n",
      "train loss:0.0018635075398626966\n",
      "train loss:0.0012644676438227514\n",
      "train loss:0.0014724509875122876\n",
      "train loss:0.00015893107646056584\n",
      "train loss:0.0017976329123422555\n",
      "train loss:0.0006592826587932155\n",
      "train loss:0.002373670416994208\n",
      "train loss:0.0026416979475794695\n",
      "train loss:0.001734347581612904\n",
      "train loss:0.0020624303058103042\n",
      "train loss:0.00010094952010532788\n",
      "train loss:0.0036878401199882836\n",
      "train loss:0.0007539914790775466\n",
      "train loss:0.0030680977817645286\n",
      "train loss:0.0007660212427883901\n",
      "train loss:0.001708699719169561\n",
      "train loss:0.00039793066325680837\n",
      "train loss:0.002735724550614691\n",
      "train loss:0.0007431325945572066\n",
      "train loss:0.0012418643487812292\n",
      "train loss:0.0023100893369766007\n",
      "train loss:0.0027169862763434254\n",
      "train loss:0.001115262330011003\n",
      "train loss:0.0012865749058853409\n",
      "train loss:0.003850807899564012\n",
      "train loss:0.00020048035311887917\n",
      "train loss:0.0008311675909325024\n",
      "train loss:7.608934665134647e-05\n",
      "train loss:0.03746059603031825\n",
      "train loss:0.003406824334015415\n",
      "train loss:0.0015393762537561958\n",
      "train loss:7.200039835279863e-05\n",
      "train loss:0.0004888918526100985\n",
      "train loss:0.00012256055415933124\n",
      "train loss:0.00017068846780995056\n",
      "train loss:0.0001921506616214303\n",
      "train loss:0.0005139125256432156\n",
      "train loss:0.0022258530366161093\n",
      "train loss:0.00022703771668500044\n",
      "train loss:4.8905445027106976e-05\n",
      "train loss:0.00035574438744406757\n",
      "train loss:7.905867902946218e-05\n",
      "train loss:0.00040006095942376677\n",
      "train loss:4.82370367785819e-05\n",
      "train loss:0.0014895013491755095\n",
      "train loss:0.0024477588674633706\n",
      "train loss:0.001967124731653973\n",
      "train loss:0.0010610687988006962\n",
      "train loss:6.492440985104219e-05\n",
      "train loss:0.0018397861530682436\n",
      "train loss:0.0018452287606297976\n",
      "train loss:0.0007353539914075996\n",
      "train loss:0.0002343447574122524\n",
      "train loss:0.00024546390271171535\n",
      "train loss:0.0002445852176789379\n",
      "train loss:0.003543741494285314\n",
      "train loss:0.0009863073753280515\n",
      "train loss:2.134270005470457e-05\n",
      "train loss:0.00029180216665953564\n",
      "train loss:0.0010693460638481265\n",
      "train loss:0.00020367538925696865\n",
      "train loss:0.0015122654606430513\n",
      "train loss:0.0013287366043956078\n",
      "train loss:0.0007842335464503001\n",
      "train loss:0.0012384197191345134\n",
      "train loss:0.0005966393203443039\n",
      "train loss:0.0003067052896862008\n",
      "train loss:0.0011722083895344937\n",
      "train loss:9.123126290091232e-05\n",
      "train loss:0.00018808733136704486\n",
      "train loss:0.0006081747185834528\n",
      "train loss:9.407372745183304e-05\n",
      "train loss:0.0007435151270227879\n",
      "train loss:4.064025125426181e-05\n",
      "train loss:0.0023863476913353198\n",
      "train loss:0.00011196077701301839\n",
      "train loss:0.027007047026333834\n",
      "train loss:0.00029784170724320977\n",
      "train loss:9.88500467393111e-05\n",
      "train loss:6.451535947464272e-05\n",
      "train loss:0.0005204964455114632\n",
      "train loss:0.00013298031864773894\n",
      "train loss:0.0005564732436006602\n",
      "train loss:0.0003145188308752591\n",
      "train loss:0.00020139080317773943\n",
      "train loss:0.0005641284309693369\n",
      "train loss:0.006376841993260622\n",
      "train loss:0.0007120724846124478\n",
      "train loss:0.003034911103398202\n",
      "train loss:0.00012152544900403451\n",
      "train loss:0.0014394073350421296\n",
      "train loss:0.00042103347899832887\n",
      "train loss:0.0003061200784840061\n",
      "train loss:0.03821324399327521\n",
      "train loss:0.013236620764747044\n",
      "train loss:0.0008084873821122481\n",
      "train loss:0.0015275649621638097\n",
      "train loss:0.011036008143964729\n",
      "train loss:0.00019789390530014505\n",
      "train loss:0.0008808942468511479\n",
      "train loss:0.0002958769553391188\n",
      "train loss:0.0008052149250320727\n",
      "train loss:0.001328546314766845\n",
      "train loss:0.00047635628928073987\n",
      "train loss:0.001387633094869072\n",
      "train loss:0.0005938139882146809\n",
      "train loss:0.002026398084364137\n",
      "train loss:0.0012207441926535157\n",
      "train loss:5.7867702355594826e-05\n",
      "train loss:0.00014785686228898756\n",
      "train loss:0.0012000084039915683\n",
      "train loss:0.0003407549633771103\n",
      "train loss:0.00022127756240317747\n",
      "train loss:0.0014046997486868893\n",
      "train loss:0.002376939804298738\n",
      "train loss:7.575189151460965e-05\n",
      "train loss:0.0035271878194643113\n",
      "train loss:0.0029677385211064056\n",
      "train loss:0.008051115639089652\n",
      "train loss:0.0003376770315978141\n",
      "train loss:0.0003353062874701979\n",
      "train loss:0.000184765791819713\n",
      "train loss:0.0008775824858888045\n",
      "train loss:0.0006128914537680276\n",
      "train loss:0.020225313438870865\n",
      "train loss:0.00013887951705335907\n",
      "train loss:0.0022855598523017885\n",
      "train loss:0.00026091666344880775\n",
      "train loss:0.00032135554945716564\n",
      "train loss:0.0009499071007347933\n",
      "train loss:0.00011290387700775548\n",
      "train loss:0.030186741276087684\n",
      "train loss:0.0016568394076055045\n",
      "train loss:0.0011842162663087874\n",
      "train loss:0.0005406453163679844\n",
      "train loss:0.00037737458098420696\n",
      "train loss:0.0035121595105086106\n",
      "train loss:0.0004528437529922749\n",
      "train loss:0.009711398686823941\n",
      "train loss:6.338035522524576e-05\n",
      "train loss:0.0013090831291126645\n",
      "train loss:0.00035945227389016826\n",
      "train loss:0.001421173624690077\n",
      "train loss:0.0037316063613545085\n",
      "train loss:0.00166092087041098\n",
      "train loss:0.0001325791399789763\n",
      "train loss:0.0019846630899049296\n",
      "train loss:6.16372514969913e-05\n",
      "train loss:0.0002058907912966031\n",
      "train loss:0.0003582748183836053\n",
      "train loss:0.000506225651173915\n",
      "train loss:0.0002068330769811198\n",
      "train loss:3.419524988378246e-05\n",
      "train loss:0.0004144696401528483\n",
      "train loss:0.0023425816883254997\n",
      "train loss:0.0013299498641856102\n",
      "train loss:0.00076810832041505\n",
      "train loss:0.0026508805019247674\n",
      "train loss:0.0007154417706005766\n",
      "train loss:0.0007579834507739657\n",
      "train loss:0.00010766819979499038\n",
      "train loss:0.000625932389375463\n",
      "train loss:0.00024128687869609742\n",
      "train loss:0.0010451579151491185\n",
      "train loss:0.0006858741669260747\n",
      "train loss:0.0009358214133856154\n",
      "train loss:0.000615534635664078\n",
      "train loss:0.00014765260464413713\n",
      "train loss:0.0003759878125851221\n",
      "train loss:0.0005768147514082546\n",
      "train loss:0.0003662312029161743\n",
      "train loss:0.0034678274042007264\n",
      "train loss:0.0023163180754817263\n",
      "train loss:0.00075476463198155\n",
      "train loss:0.004286326638358773\n",
      "train loss:0.00012891746863526909\n",
      "train loss:0.003254899343145508\n",
      "train loss:0.00859523890888629\n",
      "train loss:0.005061331380394748\n",
      "train loss:0.0033023109590865993\n",
      "train loss:1.621746555353807e-05\n",
      "train loss:0.0008205225553839854\n",
      "train loss:0.000302277023624516\n",
      "train loss:0.0071377441754712865\n",
      "train loss:0.0008891492700561872\n",
      "train loss:0.0003846200773720749\n",
      "train loss:0.0001388581584038689\n",
      "train loss:0.0012076909037497363\n",
      "train loss:0.00019978515679152604\n",
      "train loss:0.0001409748485816169\n",
      "train loss:0.0008149850064709702\n",
      "train loss:0.0004347853042625666\n",
      "train loss:0.0004247313798556264\n",
      "train loss:1.6608187629339685e-05\n",
      "train loss:0.0012636335272666174\n",
      "train loss:0.000530290259218375\n",
      "train loss:0.0014345116023167057\n",
      "train loss:0.006351192884877217\n",
      "train loss:0.0006788703625355717\n",
      "train loss:0.00031894292821277987\n",
      "train loss:0.0013043890397542162\n",
      "train loss:0.002265808599489029\n",
      "train loss:0.0021361085440602047\n",
      "train loss:0.003171685124896394\n",
      "train loss:0.001501920577133232\n",
      "train loss:0.001922681662806484\n",
      "train loss:9.422970006257091e-05\n",
      "train loss:0.002225600315464134\n",
      "train loss:0.00013691781722373957\n",
      "train loss:0.0009505148898407541\n",
      "train loss:0.000907676739036699\n",
      "train loss:0.0006320672106391814\n",
      "train loss:0.0016752824469351956\n",
      "train loss:0.0016169622067586055\n",
      "train loss:0.00010943448677189164\n",
      "train loss:9.436686344439156e-05\n",
      "train loss:9.030271166720233e-06\n",
      "train loss:0.0001107409682259487\n",
      "train loss:0.001043619633864236\n",
      "train loss:0.00044402484879917824\n",
      "train loss:0.0020391456318272824\n",
      "train loss:0.004862409375528638\n",
      "train loss:0.0025274329450745247\n",
      "train loss:0.0007614040776848013\n",
      "train loss:0.006867902760587444\n",
      "train loss:0.00023369600082672948\n",
      "train loss:0.00027208301701932064\n",
      "train loss:0.0005363091250375836\n",
      "train loss:0.00012622256049712468\n",
      "train loss:6.948096572967096e-05\n",
      "train loss:0.0007645505175339017\n",
      "train loss:8.510530419695184e-05\n",
      "train loss:0.00024768934701571797\n",
      "train loss:0.000319985064542522\n",
      "train loss:0.0006602427508936642\n",
      "train loss:0.0002910060732856015\n",
      "train loss:0.0010035598892299252\n",
      "train loss:0.002365820652851991\n",
      "train loss:0.0002572248652599291\n",
      "train loss:5.6569858590873745e-05\n",
      "train loss:0.0002467852900499744\n",
      "train loss:0.0005195869848268607\n",
      "train loss:0.001463367421433113\n",
      "train loss:0.0008781249853638312\n",
      "train loss:0.00013728874668406688\n",
      "train loss:0.01721045144513442\n",
      "train loss:0.0012983908566733462\n",
      "train loss:0.003935212295216422\n",
      "train loss:0.0004984783145820067\n",
      "train loss:0.00046176792163928006\n",
      "train loss:0.0006163167470643917\n",
      "train loss:0.004872406320792205\n",
      "train loss:0.00028939587053090015\n",
      "train loss:0.00041308927288761154\n",
      "train loss:8.910796246477351e-05\n",
      "train loss:0.0001280258366679587\n",
      "train loss:0.00039117087541513414\n",
      "train loss:0.0006841618695204727\n",
      "train loss:0.0014867270233009672\n",
      "train loss:0.0004281059257849668\n",
      "train loss:0.00017735949063974802\n",
      "train loss:0.0005055330596164178\n",
      "train loss:0.0008150657611148807\n",
      "train loss:0.00020667088072400818\n",
      "train loss:0.004127002088291369\n",
      "train loss:6.098858983589646e-05\n",
      "train loss:3.706771863347153e-05\n",
      "train loss:0.00048263070503561415\n",
      "train loss:0.0013552026931439361\n",
      "train loss:0.0009645994332281905\n",
      "train loss:0.0007995133193876668\n",
      "train loss:0.0013740671013827864\n",
      "train loss:0.0003124974120248392\n",
      "train loss:0.017185881968819153\n",
      "train loss:0.0012721090793743003\n",
      "train loss:0.00020998600239642944\n",
      "train loss:0.0004925689954642822\n",
      "train loss:0.001874803952585985\n",
      "train loss:0.0009121341760148804\n",
      "train loss:0.002007117534032126\n",
      "train loss:0.0013278115716453967\n",
      "train loss:0.0020644122129488346\n",
      "train loss:0.0001916321632400503\n",
      "train loss:0.0003082828299968393\n",
      "train loss:0.00032849281413116114\n",
      "train loss:0.0010634491127856063\n",
      "train loss:0.0034424440065526627\n",
      "train loss:0.0037816702393755736\n",
      "train loss:0.00039614490161708716\n",
      "train loss:0.002320827748548654\n",
      "train loss:0.0008743680240222763\n",
      "train loss:8.598876150837245e-05\n",
      "train loss:0.008481624057932654\n",
      "train loss:0.0013734652542727107\n",
      "train loss:0.0015326289045401822\n",
      "train loss:0.0003572582062052926\n",
      "train loss:0.0012578062795277902\n",
      "train loss:0.0008362988293505888\n",
      "train loss:0.003839656776108957\n",
      "train loss:0.0015956454707023776\n",
      "train loss:0.0015663544925408582\n",
      "train loss:0.0008388589954288544\n",
      "train loss:0.0019625590125065184\n",
      "train loss:0.0010922093696396334\n",
      "train loss:0.0009728414102336165\n",
      "train loss:0.000695082085274542\n",
      "train loss:0.002581727033214426\n",
      "train loss:0.00011957238545041892\n",
      "train loss:0.00034443763324356144\n",
      "train loss:0.00017899332141313684\n",
      "train loss:0.0005479087337680199\n",
      "train loss:0.0005060842019454663\n",
      "train loss:2.5489415580728358e-05\n",
      "train loss:0.00013483288572248216\n",
      "train loss:0.006712081781362666\n",
      "train loss:0.0011794962336459502\n",
      "=== epoch:19, train acc:1.0, test acc:0.985 ===\n",
      "train loss:0.003151249436531324\n",
      "train loss:0.002404552968846161\n",
      "train loss:0.004982086408541916\n",
      "train loss:6.741334130319995e-05\n",
      "train loss:0.00044062697170971476\n",
      "train loss:0.0029988010458464466\n",
      "train loss:0.0001878889422590347\n",
      "train loss:0.0006954906836676182\n",
      "train loss:6.67930678552724e-05\n",
      "train loss:0.00041819735862346657\n",
      "train loss:0.00029484337121839384\n",
      "train loss:0.0008557484335654424\n",
      "train loss:0.0020015599138610324\n",
      "train loss:9.104830183896807e-05\n",
      "train loss:0.0005127443187128284\n",
      "train loss:0.001705580808494059\n",
      "train loss:0.001608288596547493\n",
      "train loss:0.00037963073972312395\n",
      "train loss:0.00011849564395909057\n",
      "train loss:0.0015002631909529815\n",
      "train loss:0.0023884717570963813\n",
      "train loss:0.00017170684598437932\n",
      "train loss:0.0035584494338839322\n",
      "train loss:0.0011950717318618916\n",
      "train loss:0.001877896917819372\n",
      "train loss:0.0010123730841742609\n",
      "train loss:0.00269137602085045\n",
      "train loss:0.00021749724951547714\n",
      "train loss:0.00023013065733183406\n",
      "train loss:0.0025298450082007345\n",
      "train loss:8.110308965450279e-05\n",
      "train loss:9.825010369955944e-05\n",
      "train loss:0.00023885210226901908\n",
      "train loss:0.0003341472981176405\n",
      "train loss:0.0006998206814119728\n",
      "train loss:0.001931248538013573\n",
      "train loss:0.028859671853808604\n",
      "train loss:0.006259278165274884\n",
      "train loss:4.432203971154037e-05\n",
      "train loss:0.0006006591128672333\n",
      "train loss:0.0018969082519010524\n",
      "train loss:0.0003163395044192822\n",
      "train loss:2.9197322928000633e-05\n",
      "train loss:0.001029733733113242\n",
      "train loss:9.772780338872264e-06\n",
      "train loss:0.0006238562098820607\n",
      "train loss:0.0002749380234372772\n",
      "train loss:0.0013800703364275112\n",
      "train loss:0.0010643514803870538\n",
      "train loss:0.0007869108865854309\n",
      "train loss:0.0003576460593272393\n",
      "train loss:0.00245165161040489\n",
      "train loss:0.0029587243986360728\n",
      "train loss:0.00021524773104937024\n",
      "train loss:0.0028635606987144225\n",
      "train loss:0.00020533058069655135\n",
      "train loss:0.05140076680260632\n",
      "train loss:0.0009499708749165901\n",
      "train loss:0.002082737511464288\n",
      "train loss:3.9798966930503975e-05\n",
      "train loss:0.0006952332293277134\n",
      "train loss:0.01592673928679217\n",
      "train loss:5.6981695207478054e-05\n",
      "train loss:0.0009442481248500552\n",
      "train loss:6.536463396315706e-05\n",
      "train loss:0.0002818599978337149\n",
      "train loss:0.00015836200469569712\n",
      "train loss:0.0019209918187385055\n",
      "train loss:0.0002005471023618627\n",
      "train loss:0.0004746123222506641\n",
      "train loss:0.0003632624659490252\n",
      "train loss:0.00026150011191834966\n",
      "train loss:0.002217866452825103\n",
      "train loss:9.940442553229786e-05\n",
      "train loss:0.0009725944654258965\n",
      "train loss:0.0006515080097995649\n",
      "train loss:0.00046696846427732487\n",
      "train loss:9.524280843866221e-05\n",
      "train loss:0.0017249444008111208\n",
      "train loss:0.001431711842850861\n",
      "train loss:0.000942613215204982\n",
      "train loss:0.0015705559924175936\n",
      "train loss:0.00024309212553332498\n",
      "train loss:0.008888759542528468\n",
      "train loss:0.0003757889995315187\n",
      "train loss:0.0001934561264007911\n",
      "train loss:0.0006136166631207509\n",
      "train loss:0.004544903560980187\n",
      "train loss:0.0006840735598961506\n",
      "train loss:0.0001620125037436984\n",
      "train loss:0.00027247414816864617\n",
      "train loss:0.0008467109598557941\n",
      "train loss:0.0002522440783289884\n",
      "train loss:0.00020361714000523314\n",
      "train loss:9.374485355759616e-05\n",
      "train loss:0.004025837761379968\n",
      "train loss:0.0007248426834826172\n",
      "train loss:0.0022126926880597347\n",
      "train loss:0.003435942525679283\n",
      "train loss:0.00019249500134778657\n",
      "train loss:0.0008339540710972222\n",
      "train loss:0.0014138368956905663\n",
      "train loss:0.009660548985545992\n",
      "train loss:0.0010289616836708022\n",
      "train loss:0.00011152257915559309\n",
      "train loss:0.00011226382958712325\n",
      "train loss:0.0002564253537405807\n",
      "train loss:0.0030775720696475796\n",
      "train loss:0.0013996815079662123\n",
      "train loss:0.0002748837943256557\n",
      "train loss:0.0019273192509519676\n",
      "train loss:0.00019268749680417994\n",
      "train loss:0.017625297262726325\n",
      "train loss:0.002673043725091176\n",
      "train loss:0.002228434522877428\n",
      "train loss:0.004659537448364499\n",
      "train loss:0.0009562232610910342\n",
      "train loss:0.00011196671266307572\n",
      "train loss:0.0006220088255373873\n",
      "train loss:0.0002225272338321145\n",
      "train loss:0.00012707110020114247\n",
      "train loss:0.00010172495251163149\n",
      "train loss:0.0003610736987370471\n",
      "train loss:0.0016437958940396058\n",
      "train loss:0.004103004857196575\n",
      "train loss:0.003025195103439745\n",
      "train loss:0.00018967871082318317\n",
      "train loss:6.6365513214564e-05\n",
      "train loss:0.00716674984517346\n",
      "train loss:0.00025019799710725865\n",
      "train loss:0.0005137181459795631\n",
      "train loss:0.0015428261444924958\n",
      "train loss:0.00012674676456398524\n",
      "train loss:0.00020359074999719198\n",
      "train loss:0.012696724635533847\n",
      "train loss:0.0007193513774961724\n",
      "train loss:0.001587618338267279\n",
      "train loss:0.000332445988461667\n",
      "train loss:0.001509694996805355\n",
      "train loss:0.0018210255934187707\n",
      "train loss:0.002548960028574094\n",
      "train loss:0.002306377763758206\n",
      "train loss:0.0004799100846497319\n",
      "train loss:0.0002891805667937449\n",
      "train loss:3.329200924887849e-05\n",
      "train loss:0.0009231599461340999\n",
      "train loss:0.0033283345969555146\n",
      "train loss:0.0005167199257352919\n",
      "train loss:0.001272945864482308\n",
      "train loss:0.00019070113191242802\n",
      "train loss:0.0003253103127118264\n",
      "train loss:0.00010436457357790812\n",
      "train loss:0.0021254239307424995\n",
      "train loss:0.0008711520200291654\n",
      "train loss:0.001948637705241623\n",
      "train loss:0.00031658984402070753\n",
      "train loss:0.0019139972952255968\n",
      "train loss:0.003547913263328696\n",
      "train loss:4.227634527447327e-05\n",
      "train loss:5.5131824197978934e-05\n",
      "train loss:0.00012349728428240008\n",
      "train loss:7.510686598124197e-05\n",
      "train loss:0.000846713492978707\n",
      "train loss:0.00035630602549235055\n",
      "train loss:0.0032738666628427766\n",
      "train loss:0.0018687736504905922\n",
      "train loss:0.00201890387263083\n",
      "train loss:0.0003152455679477256\n",
      "train loss:0.000604548607626458\n",
      "train loss:0.0026375760039529226\n",
      "train loss:0.00025099337702852475\n",
      "train loss:0.010250477680090343\n",
      "train loss:0.001654907802557922\n",
      "train loss:0.002246396831479086\n",
      "train loss:0.0007888285501580651\n",
      "train loss:0.0006847510100056682\n",
      "train loss:0.00013653050835820294\n",
      "train loss:0.0008231311466818972\n",
      "train loss:0.0020994713147368003\n",
      "train loss:0.0001685434005851022\n",
      "train loss:0.0013237585405706444\n",
      "train loss:0.0006158420847305364\n",
      "train loss:0.0006950011680449133\n",
      "train loss:0.0008787366273415773\n",
      "train loss:0.0010199784847794594\n",
      "train loss:0.004451030638859205\n",
      "train loss:0.001612599062214098\n",
      "train loss:0.0022760159342061393\n",
      "train loss:0.0002879159842868144\n",
      "train loss:0.0026128408188255663\n",
      "train loss:0.0013706413862759267\n",
      "train loss:0.000631517058988455\n",
      "train loss:0.00046182407555272705\n",
      "train loss:0.01566217055749964\n",
      "train loss:0.0009489376095345218\n",
      "train loss:0.001213293671345555\n",
      "train loss:0.0007475848106928355\n",
      "train loss:0.00520192263830343\n",
      "train loss:0.0019175776323498619\n",
      "train loss:0.0012153429855947463\n",
      "train loss:0.0009592979531374826\n",
      "train loss:0.031103148986607355\n",
      "train loss:0.0027839137792887913\n",
      "train loss:0.0016155361618955138\n",
      "train loss:0.0006863609773025814\n",
      "train loss:0.00018871619268073442\n",
      "train loss:0.00039227852197633404\n",
      "train loss:0.000931270397790185\n",
      "train loss:0.0016404301800990269\n",
      "train loss:0.006028866540637281\n",
      "train loss:0.0038340405797894746\n",
      "train loss:0.00025255684242284776\n",
      "train loss:0.00024334684950632588\n",
      "train loss:0.004036707957250307\n",
      "train loss:0.0010952700233602861\n",
      "train loss:0.0009240827471251095\n",
      "train loss:0.002231729459597394\n",
      "train loss:0.0003348415296526067\n",
      "train loss:0.004202944937189795\n",
      "train loss:0.0002119609771927972\n",
      "train loss:0.005326568121646364\n",
      "train loss:0.0005556602178614248\n",
      "train loss:0.0012953883097390748\n",
      "train loss:0.0039024651785225985\n",
      "train loss:0.0028063252142876568\n",
      "train loss:7.157455243458177e-05\n",
      "train loss:0.0005653588213995447\n",
      "train loss:0.0009315278246147964\n",
      "train loss:0.0008090585608009932\n",
      "train loss:0.0010624809865047173\n",
      "train loss:0.0005279540935061103\n",
      "train loss:0.002371377636423437\n",
      "train loss:0.006855504018363187\n",
      "train loss:0.0021321093855897256\n",
      "train loss:0.00046189333579401873\n",
      "train loss:0.00031405605670158223\n",
      "train loss:0.0003147690732175784\n",
      "train loss:0.00046468980745641814\n",
      "train loss:0.0009083331094403083\n",
      "train loss:0.008250787232932768\n",
      "train loss:0.0006685126343662242\n",
      "train loss:0.00040695031239511374\n",
      "train loss:0.0002168212503975542\n",
      "train loss:8.57957099421412e-05\n",
      "train loss:0.0072944495477600445\n",
      "train loss:0.00022719041509346866\n",
      "train loss:0.0018883987781000103\n",
      "train loss:0.0011874015064486097\n",
      "train loss:4.318653352220301e-05\n",
      "train loss:3.9993894422184464e-05\n",
      "train loss:0.00100373636814957\n",
      "train loss:0.0007945273181389789\n",
      "train loss:0.0005971072313480696\n",
      "train loss:0.00115942084849795\n",
      "train loss:8.630400575289703e-05\n",
      "train loss:0.0008449728739056621\n",
      "train loss:0.004963193017506296\n",
      "train loss:0.0009286541758663676\n",
      "train loss:0.0020426583475599623\n",
      "train loss:0.00010196616222232241\n",
      "train loss:4.13032013233453e-05\n",
      "train loss:0.0065416742796866436\n",
      "train loss:0.00039582728411288243\n",
      "train loss:0.00041243434229671375\n",
      "train loss:6.6698537600274645e-06\n",
      "train loss:0.0007299927743863468\n",
      "train loss:0.00027708590060496674\n",
      "train loss:0.001868639978951524\n",
      "train loss:0.0012784539655056283\n",
      "train loss:0.0026095404434534814\n",
      "train loss:0.0009285904213066017\n",
      "train loss:0.0030337858352343845\n",
      "train loss:1.3960599807626185e-05\n",
      "train loss:0.0005782267188082265\n",
      "train loss:0.0003590930766991397\n",
      "train loss:1.622040753065192e-05\n",
      "train loss:0.00030509425440806775\n",
      "train loss:0.00018299744662265058\n",
      "train loss:0.0012287770662771055\n",
      "train loss:0.0009310449109069456\n",
      "train loss:7.689812290944199e-05\n",
      "train loss:0.00010933946786657121\n",
      "train loss:0.0015673815350727043\n",
      "train loss:0.0010736017091118775\n",
      "train loss:0.0010126054957903278\n",
      "train loss:0.0019704700499829204\n",
      "train loss:0.00021760820604841585\n",
      "train loss:0.000542124924209979\n",
      "train loss:0.0026889237363577406\n",
      "train loss:0.0005236135739063608\n",
      "train loss:0.0004178064829864226\n",
      "train loss:0.0006337183234648243\n",
      "train loss:0.00015577559306582502\n",
      "train loss:0.0006964631934191214\n",
      "train loss:0.00023721631843658488\n",
      "train loss:0.005634850060669767\n",
      "train loss:0.0004969894890126901\n",
      "train loss:7.538154852958329e-05\n",
      "train loss:0.0003504254005938358\n",
      "train loss:0.007145244965099082\n",
      "train loss:0.0017181649611991868\n",
      "train loss:0.0008973605298966012\n",
      "train loss:0.0006633208798642723\n",
      "train loss:0.0012871289366060091\n",
      "train loss:0.0032910101286159365\n",
      "train loss:0.00012611461301231572\n",
      "train loss:0.005758367819824422\n",
      "train loss:5.049461787979035e-05\n",
      "train loss:0.007882636312745156\n",
      "train loss:0.000415465285320662\n",
      "train loss:0.00012480375588866006\n",
      "train loss:0.00027190966898254035\n",
      "train loss:0.003761785155414052\n",
      "train loss:0.00027319275861657734\n",
      "train loss:0.0029455639793484582\n",
      "train loss:0.0015886968834165594\n",
      "train loss:0.0025255132601541335\n",
      "train loss:6.585365668474976e-05\n",
      "train loss:0.002292958520743404\n",
      "train loss:0.03272756046563871\n",
      "train loss:0.0003971706864619088\n",
      "train loss:0.001760845226571532\n",
      "train loss:1.6988919227784e-05\n",
      "train loss:0.00015089082071580623\n",
      "train loss:0.0019286228107893066\n",
      "train loss:0.002221470625324469\n",
      "train loss:0.0003574294843725412\n",
      "train loss:0.0008751061544100519\n",
      "train loss:0.00047130175772076523\n",
      "train loss:0.00013691201239529737\n",
      "train loss:0.0034462482428433737\n",
      "train loss:0.0003238578283536579\n",
      "train loss:0.0008811439770566848\n",
      "train loss:0.0002538076721577169\n",
      "train loss:0.0027967587980297676\n",
      "train loss:0.0007372041236811877\n",
      "train loss:0.0014241597835851355\n",
      "train loss:0.00014144894540669885\n",
      "train loss:0.0010619023105271261\n",
      "train loss:0.0012882949756372044\n",
      "train loss:0.022260291950538916\n",
      "train loss:0.0029909552816011816\n",
      "train loss:0.0003476807605817435\n",
      "train loss:0.0012495913261209265\n",
      "train loss:0.0077652624777285705\n",
      "train loss:0.0018831633296135827\n",
      "train loss:7.344744981216934e-05\n",
      "train loss:0.0008351658900269534\n",
      "train loss:0.0009643982054612245\n",
      "train loss:0.003516578222838666\n",
      "train loss:0.002018817694395212\n",
      "train loss:0.0005085930113110395\n",
      "train loss:0.0009655385369920926\n",
      "train loss:0.0016583795183610461\n",
      "train loss:0.00033024682085301617\n",
      "train loss:0.023669632534566935\n",
      "train loss:0.003579944592542914\n",
      "train loss:0.002689312046627179\n",
      "train loss:0.0002445755717118829\n",
      "train loss:0.01281671309113027\n",
      "train loss:0.0013991115220942415\n",
      "train loss:0.002369131131863611\n",
      "train loss:0.0004528538260800552\n",
      "train loss:6.43424917038145e-05\n",
      "train loss:0.001281300585307173\n",
      "train loss:0.0020430338450413673\n",
      "train loss:0.0001317841829412743\n",
      "train loss:0.0015523553095484833\n",
      "train loss:0.0020466417558539984\n",
      "train loss:0.00438695818299463\n",
      "train loss:0.002729743165657762\n",
      "train loss:0.00022179934223068683\n",
      "train loss:4.6975813130251245e-05\n",
      "train loss:0.00045209943768281184\n",
      "train loss:0.00013176023115932137\n",
      "train loss:0.002969914894929815\n",
      "train loss:0.000872696058652867\n",
      "train loss:0.0003378808531812466\n",
      "train loss:0.0012237380343140822\n",
      "train loss:0.00024038112435028502\n",
      "train loss:0.0009585436423177234\n",
      "train loss:0.00019221712065871363\n",
      "train loss:0.0010463909770709594\n",
      "train loss:0.002346247631270318\n",
      "train loss:0.00157058626132001\n",
      "train loss:0.0038421192057088473\n",
      "train loss:0.019000326158462325\n",
      "train loss:0.0028880214062794902\n",
      "train loss:9.617966207309821e-05\n",
      "train loss:0.0034748193005700602\n",
      "train loss:0.0028791574605931554\n",
      "train loss:2.391785467059673e-05\n",
      "train loss:1.3323872351675908e-05\n",
      "train loss:0.0006150612259745069\n",
      "train loss:0.0004947576806077105\n",
      "train loss:0.0024571550981162633\n",
      "train loss:0.0028252845883566407\n",
      "train loss:0.002021371082018399\n",
      "train loss:0.0019288201629541124\n",
      "train loss:0.0024634895586763445\n",
      "train loss:0.0057197398460607455\n",
      "train loss:0.002507775381510363\n",
      "train loss:0.004780427357438853\n",
      "train loss:0.001028077661497903\n",
      "train loss:0.001736156329677516\n",
      "train loss:0.04649727935778132\n",
      "train loss:0.001056899910537195\n",
      "train loss:0.0006664294956561146\n",
      "train loss:0.0005435866222277289\n",
      "train loss:0.0016908500628334603\n",
      "train loss:0.00036684815292326674\n",
      "train loss:0.0002645312132939888\n",
      "train loss:0.005687985240571092\n",
      "train loss:0.004640420917505627\n",
      "train loss:0.0011132617020924492\n",
      "train loss:0.0017057318132266137\n",
      "train loss:0.004920003826691887\n",
      "train loss:0.0005018771927157936\n",
      "train loss:0.014582409427942573\n",
      "train loss:0.015570942201759759\n",
      "train loss:0.001561332209826241\n",
      "train loss:0.0002996983734266113\n",
      "train loss:0.00020853724247258805\n",
      "train loss:0.003622715017350988\n",
      "train loss:0.00020724359549944443\n",
      "train loss:0.0016728019798821147\n",
      "train loss:0.00038987831084989045\n",
      "train loss:0.0008061111778737636\n",
      "train loss:0.005467242640165263\n",
      "train loss:0.010351151634927664\n",
      "train loss:0.0018042909959783842\n",
      "train loss:0.00038068729269397555\n",
      "train loss:0.00014789177275832677\n",
      "train loss:0.00041827773029366397\n",
      "train loss:0.0003483549325480849\n",
      "train loss:0.00020751300197519626\n",
      "train loss:0.0036478183505199674\n",
      "train loss:0.0003133299945652828\n",
      "train loss:0.0024489844710833364\n",
      "train loss:0.0020103024671127116\n",
      "train loss:0.0027841871670223365\n",
      "train loss:0.0013787023200419084\n",
      "train loss:0.005419287455709314\n",
      "train loss:0.0004032608764481621\n",
      "train loss:0.0012007627488710767\n",
      "train loss:0.0036441139627431856\n",
      "train loss:0.002924958148328394\n",
      "train loss:0.0013620182322833204\n",
      "train loss:0.0008690403739754309\n",
      "train loss:0.005610272867135913\n",
      "train loss:0.022121401577724575\n",
      "train loss:0.0013350757620283634\n",
      "train loss:0.0004408498846025195\n",
      "train loss:0.016587243350528802\n",
      "train loss:0.01774667828063057\n",
      "train loss:0.00014077970185138886\n",
      "train loss:0.0057240027077093865\n",
      "train loss:0.000572692294361459\n",
      "train loss:8.560309927705464e-05\n",
      "train loss:0.0055218368977608505\n",
      "train loss:0.002353367855044278\n",
      "train loss:0.0015400770026753323\n",
      "train loss:0.0025802666852131773\n",
      "train loss:0.00034014993433502324\n",
      "train loss:0.003042840676662506\n",
      "train loss:0.003534406442511821\n",
      "train loss:0.0016924035707113125\n",
      "train loss:0.0015105961400875801\n",
      "train loss:0.0028218151318788946\n",
      "train loss:0.003618361343103517\n",
      "train loss:0.0001895740088730615\n",
      "train loss:0.000728846317774308\n",
      "train loss:0.0020995580530542724\n",
      "train loss:0.005015585973789581\n",
      "train loss:0.002252046157898507\n",
      "train loss:0.0034354582483117923\n",
      "train loss:0.0012806388356288922\n",
      "train loss:0.0030000277567013116\n",
      "train loss:0.0012715325182659765\n",
      "train loss:0.0009852357957552457\n",
      "train loss:0.0034395195972095286\n",
      "train loss:0.001692671518112248\n",
      "train loss:0.00038040508318570117\n",
      "train loss:0.0007763746807604294\n",
      "train loss:0.02435951252728283\n",
      "train loss:0.0006904180433933834\n",
      "train loss:0.0007628172828326224\n",
      "train loss:0.0004662727297823949\n",
      "train loss:3.361980907386603e-05\n",
      "train loss:0.0008572018232971785\n",
      "train loss:0.001145585542967439\n",
      "train loss:8.963577656030111e-05\n",
      "train loss:0.0005213264653787555\n",
      "train loss:0.0012098505193405432\n",
      "train loss:0.010479108910238058\n",
      "train loss:0.0008005006991083181\n",
      "train loss:0.00032321086480938924\n",
      "train loss:0.0002533084830347486\n",
      "train loss:0.00046696876986186404\n",
      "train loss:0.0051268015793527475\n",
      "train loss:4.339074548399357e-05\n",
      "train loss:7.700255589352707e-05\n",
      "train loss:0.0006241851891105131\n",
      "train loss:0.0009733679289966548\n",
      "train loss:0.0031244000039851155\n",
      "train loss:0.002621855805461218\n",
      "train loss:0.0015978317445401644\n",
      "train loss:0.0012069520257342642\n",
      "train loss:0.007157556218021939\n",
      "train loss:0.0006785201366309685\n",
      "train loss:0.0008677399961317785\n",
      "train loss:0.00066423230649294\n",
      "train loss:9.308540387218473e-05\n",
      "train loss:0.0004430480764707644\n",
      "train loss:0.005734895809937353\n",
      "train loss:0.002949893972917361\n",
      "train loss:0.0003056344757089229\n",
      "train loss:0.0035144862534546937\n",
      "train loss:0.0008184331737643454\n",
      "train loss:4.491625888284254e-05\n",
      "train loss:0.00036261047557347597\n",
      "train loss:3.819541408328348e-05\n",
      "train loss:0.015256207718111585\n",
      "train loss:0.00178900890434331\n",
      "train loss:0.00012768014112953883\n",
      "train loss:0.001283877239365111\n",
      "train loss:0.005007849859430532\n",
      "train loss:0.000296499518872133\n",
      "train loss:0.0006148460505483069\n",
      "train loss:0.005332879470088101\n",
      "train loss:0.0035558815087081775\n",
      "train loss:0.0029163682283195164\n",
      "train loss:0.0007791275028798315\n",
      "train loss:0.0020183433719805337\n",
      "train loss:0.0010191702127744224\n",
      "train loss:0.0016848464817305648\n",
      "train loss:0.0002920029338420329\n",
      "train loss:0.010212677791713814\n",
      "train loss:0.00013823527447789065\n",
      "train loss:0.0015039496407285133\n",
      "train loss:0.0009205084159312665\n",
      "train loss:0.0023617153796153608\n",
      "train loss:0.0007359903094728558\n",
      "train loss:0.0026997237857710288\n",
      "train loss:0.0014409592441191241\n",
      "train loss:4.5962446154142464e-05\n",
      "train loss:0.002262513852357774\n",
      "train loss:0.002271897553073623\n",
      "train loss:0.00015075679066547213\n",
      "train loss:0.009979862292775636\n",
      "train loss:0.001194749650375353\n",
      "train loss:0.00020612005963544896\n",
      "train loss:0.0001402338520230066\n",
      "train loss:0.0035069874623588817\n",
      "train loss:0.0033952454588443088\n",
      "train loss:0.0020574750819809602\n",
      "train loss:0.0037377487106264712\n",
      "train loss:0.005321814723553766\n",
      "train loss:0.0011574572619847975\n",
      "train loss:0.003298336180849663\n",
      "train loss:0.006211293416313186\n",
      "train loss:0.0001752142095826562\n",
      "train loss:5.8605691870491697e-05\n",
      "train loss:0.002324058456928812\n",
      "train loss:0.0022439023155423655\n",
      "train loss:0.0006041181774310256\n",
      "train loss:0.0003931161553019527\n",
      "train loss:0.000591366469322427\n",
      "train loss:0.00022237680508914112\n",
      "train loss:0.00044405244823004467\n",
      "train loss:0.00019630710154269139\n",
      "train loss:0.001187766566190771\n",
      "train loss:0.0016996697928659719\n",
      "train loss:0.0012044043849483703\n",
      "train loss:0.001963625457424317\n",
      "train loss:0.004872190001894984\n",
      "train loss:0.0005790933454739872\n",
      "train loss:0.0006882183163756393\n",
      "train loss:0.0020737078346551286\n",
      "train loss:0.002298588533365446\n",
      "train loss:0.003456414704015828\n",
      "train loss:0.004707192448122308\n",
      "train loss:0.0036555019391610864\n",
      "train loss:0.015309268266435494\n",
      "train loss:0.0016687711704612051\n",
      "train loss:0.0020416519761604105\n",
      "train loss:0.002061445623895467\n",
      "train loss:0.0008171910765211966\n",
      "train loss:0.0015810857086871995\n",
      "train loss:0.0006272222210030671\n",
      "train loss:0.0013843436520076984\n",
      "train loss:0.00691223103664059\n",
      "train loss:0.014221125418845303\n",
      "train loss:0.00020156795822396813\n",
      "train loss:0.00032025185918219546\n",
      "train loss:0.014260445960259232\n",
      "train loss:0.0006024365005850933\n",
      "train loss:0.00011608678834453646\n",
      "train loss:0.0032655227257790136\n",
      "train loss:0.00038225155165065415\n",
      "=== epoch:20, train acc:0.999, test acc:0.984 ===\n",
      "train loss:0.00026564271197003064\n",
      "train loss:0.016563393281707636\n",
      "train loss:0.0020720922083800834\n",
      "train loss:0.0027720338970374796\n",
      "train loss:0.0006458240274161653\n",
      "train loss:0.0019341534929350308\n",
      "train loss:7.394818223218169e-05\n",
      "train loss:0.00014568205194503804\n",
      "train loss:0.004483415780895582\n",
      "train loss:0.0030537864600422592\n",
      "train loss:0.0049438426616454124\n",
      "train loss:0.00019633498781551463\n",
      "train loss:0.004333672906607469\n",
      "train loss:0.001418851820501054\n",
      "train loss:0.0015787323283706712\n",
      "train loss:0.0009844184536942106\n",
      "train loss:0.005721766351272501\n",
      "train loss:0.0020800723498088265\n",
      "train loss:0.003426962547128164\n",
      "train loss:0.001169233511791552\n",
      "train loss:0.0007598114564676922\n",
      "train loss:0.0012401375260997203\n",
      "train loss:5.614111794024509e-05\n",
      "train loss:0.008242484529040433\n",
      "train loss:0.00018314285725595436\n",
      "train loss:1.4292384654494217e-05\n",
      "train loss:0.0017133720745450458\n",
      "train loss:0.0010524168749138215\n",
      "train loss:0.00011428738215219385\n",
      "train loss:0.0023563309962536735\n",
      "train loss:0.008427966474122922\n",
      "train loss:0.0014782285391037884\n",
      "train loss:0.013347397376158608\n",
      "train loss:0.000833874330297326\n",
      "train loss:0.0008457560037451817\n",
      "train loss:0.0002345471763362249\n",
      "train loss:0.0013855130628699663\n",
      "train loss:0.0005466264108577634\n",
      "train loss:0.0005920466366311584\n",
      "train loss:0.0005908534618675917\n",
      "train loss:0.006455083235266873\n",
      "train loss:0.0009848952948188928\n",
      "train loss:0.0030036286992363025\n",
      "train loss:0.0006932724337237456\n",
      "train loss:0.0013402617624804303\n",
      "train loss:6.956694069019736e-05\n",
      "train loss:0.000587821346689342\n",
      "train loss:0.0005761655108813321\n",
      "train loss:0.0028688255972863573\n",
      "train loss:0.0010287820944552203\n",
      "train loss:0.00025723070285799885\n",
      "train loss:0.004116622493565775\n",
      "train loss:0.0011205234752553186\n",
      "train loss:0.0021515089409411556\n",
      "train loss:0.0005567356122235375\n",
      "train loss:0.001447231997602241\n",
      "train loss:0.0050912697593185445\n",
      "train loss:0.00022642248091526586\n",
      "train loss:0.00012357278036260512\n",
      "train loss:0.00023714352663325216\n",
      "train loss:0.0003058944264609591\n",
      "train loss:0.0014614226041466038\n",
      "train loss:0.00020865289149774445\n",
      "train loss:0.0003798354400359708\n",
      "train loss:0.0007895044051361211\n",
      "train loss:0.0011128737865942278\n",
      "train loss:0.0001269683576956829\n",
      "train loss:0.0010308423980035437\n",
      "train loss:0.0023275492371303903\n",
      "train loss:0.0002645160536298256\n",
      "train loss:0.00010648933914463074\n",
      "train loss:0.001495398576860122\n",
      "train loss:4.525878710371166e-05\n",
      "train loss:0.0013913518976585216\n",
      "train loss:0.004058506904423298\n",
      "train loss:0.001057901752524427\n",
      "train loss:0.0008625216340642821\n",
      "train loss:0.001938848751362558\n",
      "train loss:0.00034844087377678925\n",
      "train loss:0.0016373555198835234\n",
      "train loss:0.00016448719095889946\n",
      "train loss:0.0005209948354546731\n",
      "train loss:1.4110031034545241e-05\n",
      "train loss:0.00032101850590816297\n",
      "train loss:0.00017738098697098165\n",
      "train loss:0.00015052499115320958\n",
      "train loss:0.0007384513613007089\n",
      "train loss:0.000861077410776111\n",
      "train loss:0.0008852574291495255\n",
      "train loss:0.013456342213855561\n",
      "train loss:0.00032798043756341135\n",
      "train loss:0.00437909393263604\n",
      "train loss:0.002083963375273401\n",
      "train loss:0.00017833955950766154\n",
      "train loss:0.0012828247948459392\n",
      "train loss:0.0029753016862125066\n",
      "train loss:0.00019871720582064067\n",
      "train loss:0.015973207955997618\n",
      "train loss:0.001138291075778444\n",
      "train loss:0.00012730446814839203\n",
      "train loss:0.000122176851294025\n",
      "train loss:0.0029497182044512103\n",
      "train loss:0.0009232437291441316\n",
      "train loss:0.0031094927987113407\n",
      "train loss:0.0017787530725440076\n",
      "train loss:0.0013724956160929588\n",
      "train loss:0.01560147691468961\n",
      "train loss:0.0012241326763351205\n",
      "train loss:0.0005812790510551524\n",
      "train loss:0.00031526408382527603\n",
      "train loss:0.00023128555386350627\n",
      "train loss:0.0033008554968662228\n",
      "train loss:0.0003044388187238418\n",
      "train loss:0.0054554196149958515\n",
      "train loss:0.001530745090623338\n",
      "train loss:0.006772263584694991\n",
      "train loss:0.0001087086876146684\n",
      "train loss:6.475303299568715e-05\n",
      "train loss:0.010284052144745492\n",
      "train loss:0.0013840080314482068\n",
      "train loss:0.00038923603072659797\n",
      "train loss:0.00016065382944911857\n",
      "train loss:0.00045750612750209807\n",
      "train loss:0.003664946162101112\n",
      "train loss:0.012355879863400938\n",
      "train loss:0.00033764358370674927\n",
      "train loss:0.00018302931609328937\n",
      "train loss:0.0003695227257690077\n",
      "train loss:0.005271027289949819\n",
      "train loss:0.00566984341359962\n",
      "train loss:0.00311715204112279\n",
      "train loss:0.0008019181161472559\n",
      "train loss:0.003207969108740206\n",
      "train loss:0.00020044274134840078\n",
      "train loss:0.0008384444113334259\n",
      "train loss:0.0017304878506760413\n",
      "train loss:0.005748176040342058\n",
      "train loss:0.013362574075789592\n",
      "train loss:0.0012101629646545412\n",
      "train loss:0.0012526093921154913\n",
      "train loss:0.00046477010462160254\n",
      "train loss:0.0009001707099764785\n",
      "train loss:0.00042604326645048415\n",
      "train loss:0.00036190778250758733\n",
      "train loss:0.0009193330584995244\n",
      "train loss:0.0003311735982528149\n",
      "train loss:0.004402396747713464\n",
      "train loss:9.749871423115296e-05\n",
      "train loss:0.0006542484325443816\n",
      "train loss:0.0002695411229857416\n",
      "train loss:0.00035902924401533433\n",
      "train loss:0.007914232352233668\n",
      "train loss:0.01951410908505512\n",
      "train loss:0.0009553334719955683\n",
      "train loss:0.005307198707527874\n",
      "train loss:0.0026076596083928535\n",
      "train loss:0.0007261228978649142\n",
      "train loss:0.0036977033323176197\n",
      "train loss:0.00055697283576028\n",
      "train loss:0.000605408802381234\n",
      "train loss:0.000250219804990532\n",
      "train loss:0.00016654777927566019\n",
      "train loss:0.0044023774448923615\n",
      "train loss:0.0019876542752577595\n",
      "train loss:0.001411989584074886\n",
      "train loss:0.0032264252829253266\n",
      "train loss:0.0004006074043742976\n",
      "train loss:0.000771601371479414\n",
      "train loss:0.04903789371442279\n",
      "train loss:0.003634362136928243\n",
      "train loss:0.003081481150318731\n",
      "train loss:0.0036336646487625706\n",
      "train loss:0.0013229119471493374\n",
      "train loss:0.0014571476658950704\n",
      "train loss:0.0005998931294648331\n",
      "train loss:0.001708078481360721\n",
      "train loss:0.00032447067164147767\n",
      "train loss:0.0009613222814748253\n",
      "train loss:0.0003700952996486836\n",
      "train loss:0.007183887809987878\n",
      "train loss:0.000570603775574381\n",
      "train loss:2.3343648694747078e-05\n",
      "train loss:0.008079406811413425\n",
      "train loss:0.006441753669534854\n",
      "train loss:0.0049939909561300365\n",
      "train loss:0.00044639527988013744\n",
      "train loss:0.0006568440212467679\n",
      "train loss:0.0003725218559953219\n",
      "train loss:0.0003595810309265777\n",
      "train loss:0.00033076215424555764\n",
      "train loss:0.0020742440305214203\n",
      "train loss:0.005117565384516512\n",
      "train loss:0.0009090156865375762\n",
      "train loss:0.004179442000991494\n",
      "train loss:0.00018047922759947886\n",
      "train loss:0.0003576152442577545\n",
      "train loss:0.001262566875642894\n",
      "train loss:0.004833177427115663\n",
      "train loss:0.0291814886705966\n",
      "train loss:0.0048282259745008015\n",
      "train loss:0.026689481962584694\n",
      "train loss:1.1661100011166687e-05\n",
      "train loss:0.00010016829547878782\n",
      "train loss:0.003371271136491562\n",
      "train loss:0.0007236245884073957\n",
      "train loss:0.0035771747333386427\n",
      "train loss:8.150829567876985e-05\n",
      "train loss:0.0011504669658236586\n",
      "train loss:0.00032202276448716567\n",
      "train loss:0.0024204037482360853\n",
      "train loss:0.003570881977130869\n",
      "train loss:0.0009253703791032516\n",
      "train loss:5.037348051895333e-05\n",
      "train loss:0.0006503744221062956\n",
      "train loss:0.0007707147795552206\n",
      "train loss:0.001552034328122828\n",
      "train loss:0.0007198939928693459\n",
      "train loss:0.0007970165831268315\n",
      "train loss:0.004835729159972152\n",
      "train loss:0.00020168357381436766\n",
      "train loss:0.0002346673762106246\n",
      "train loss:0.0031790094783333395\n",
      "train loss:0.0011208167356315595\n",
      "train loss:0.002296401296669862\n",
      "train loss:0.0027913494639339813\n",
      "train loss:0.0001680462811640892\n",
      "train loss:0.00033043833276805943\n",
      "train loss:0.005949964145802579\n",
      "train loss:0.0006511403648390048\n",
      "train loss:0.0012728480452446852\n",
      "train loss:0.00019846310642417114\n",
      "train loss:0.0001453147669601528\n",
      "train loss:0.0010887895106460558\n",
      "train loss:0.001772805833685563\n",
      "train loss:0.001371351667323618\n",
      "train loss:0.00038333453547964575\n",
      "train loss:0.0012785350406044576\n",
      "train loss:0.001915233168181841\n",
      "train loss:0.0014236081795863072\n",
      "train loss:0.0031151947008099734\n",
      "train loss:0.0012408376985625985\n",
      "train loss:0.002592229526199874\n",
      "train loss:0.00029210870773144063\n",
      "train loss:3.7040133889770105e-05\n",
      "train loss:0.0010437862274749298\n",
      "train loss:0.0001015181667949552\n",
      "train loss:0.0006449452958201395\n",
      "train loss:5.947643549005791e-05\n",
      "train loss:0.0015363548261610393\n",
      "train loss:0.0024544204116080325\n",
      "train loss:0.0022969864259483456\n",
      "train loss:0.0005409616124055204\n",
      "train loss:0.00017190728913255157\n",
      "train loss:0.0009154122799081263\n",
      "train loss:0.0011372536457346137\n",
      "train loss:9.076925286374226e-05\n",
      "train loss:0.00022997580800371772\n",
      "train loss:4.4113791538562054e-05\n",
      "train loss:0.00014735592380496666\n",
      "train loss:0.00016513365382409158\n",
      "train loss:1.1479188500120132e-05\n",
      "train loss:0.0008277383920469489\n",
      "train loss:0.0005368596130589788\n",
      "train loss:0.001462183435209942\n",
      "train loss:0.00018961217927812595\n",
      "train loss:0.001532000275453644\n",
      "train loss:0.00022588340643177923\n",
      "train loss:0.0009270712696493501\n",
      "train loss:0.003501098469592337\n",
      "train loss:0.003918846405683396\n",
      "train loss:0.00025840638722663637\n",
      "train loss:0.0009642891728057827\n",
      "train loss:0.00019110366727576568\n",
      "train loss:0.000154417218407064\n",
      "train loss:0.0009595189498492214\n",
      "train loss:5.84839593044162e-05\n",
      "train loss:0.0022169068111151896\n",
      "train loss:0.00023891034137361018\n",
      "train loss:0.005190411897520214\n",
      "train loss:0.0005183388959659784\n",
      "train loss:0.0009700987220798093\n",
      "train loss:0.0018397796247513081\n",
      "train loss:0.0004658503108458633\n",
      "train loss:4.576806765711751e-05\n",
      "train loss:0.0006255544235277426\n",
      "train loss:0.005301857888453357\n",
      "train loss:0.0006405020498665804\n",
      "train loss:0.00024801116059467733\n",
      "train loss:0.0005375879222851097\n",
      "train loss:9.02654543697835e-05\n",
      "train loss:0.00014503004842534456\n",
      "train loss:0.0090450114261181\n",
      "train loss:0.0004601851077818355\n",
      "train loss:0.001060014314929899\n",
      "train loss:0.0010647423162656168\n",
      "train loss:0.00013349573918608327\n",
      "train loss:0.0002677445371179399\n",
      "train loss:0.0006604484691283888\n",
      "train loss:0.00308658181981184\n",
      "train loss:0.0006862144800525536\n",
      "train loss:0.0007647772676098428\n",
      "train loss:0.0021507564302227487\n",
      "train loss:3.304790415182981e-05\n",
      "train loss:0.001042917481506143\n",
      "train loss:0.001379742170781325\n",
      "train loss:0.0005560942115876411\n",
      "train loss:0.0008493562632572107\n",
      "train loss:0.00010590066300646852\n",
      "train loss:0.0015170893462317719\n",
      "train loss:1.2765779415297779e-05\n",
      "train loss:0.0002059072970239044\n",
      "train loss:0.000827782106973443\n",
      "train loss:0.0023137599517563325\n",
      "train loss:0.0001394058215320593\n",
      "train loss:0.004316876928274534\n",
      "train loss:0.01329298061232351\n",
      "train loss:0.005771973492268752\n",
      "train loss:0.00018179875869451232\n",
      "train loss:0.0028974698968702263\n",
      "train loss:0.0007273506376717788\n",
      "train loss:0.0023029127094139597\n",
      "train loss:0.0024899489614377125\n",
      "train loss:0.00041335879776975706\n",
      "train loss:0.0016568278749824216\n",
      "train loss:0.0013661125230402655\n",
      "train loss:0.0011746795026265837\n",
      "train loss:0.0007374944747245414\n",
      "train loss:0.0008985426371675002\n",
      "train loss:0.0021603806132795157\n",
      "train loss:5.3741739914084876e-05\n",
      "train loss:9.319690017740363e-05\n",
      "train loss:0.00027242799477814294\n",
      "train loss:0.0009874238648507375\n",
      "train loss:0.04749830118339919\n",
      "train loss:0.03345049239622174\n",
      "train loss:9.192640197289005e-05\n",
      "train loss:0.0012434371199020002\n",
      "train loss:0.0024607608353531376\n",
      "train loss:0.00031168045945422396\n",
      "train loss:0.000136882916990889\n",
      "train loss:0.0007051269541316058\n",
      "train loss:0.001426877139204085\n",
      "train loss:0.000429747754725594\n",
      "train loss:0.0014229205012621302\n",
      "train loss:0.0024970550484419676\n",
      "train loss:4.486492059762792e-05\n",
      "train loss:0.0011436710017878\n",
      "train loss:0.0017924440540332208\n",
      "train loss:0.005764623476984386\n",
      "train loss:0.00028100498367643513\n",
      "train loss:0.00044183686942266014\n",
      "train loss:0.0006959602387186618\n",
      "train loss:0.0002549290213308563\n",
      "train loss:0.0008722711719147795\n",
      "train loss:0.002092755182150866\n",
      "train loss:0.003375117826524038\n",
      "train loss:0.003840022561917574\n",
      "train loss:0.00034562204962521796\n",
      "train loss:0.0034828714192361365\n",
      "train loss:6.831305159914143e-05\n",
      "train loss:0.0007014822444146541\n",
      "train loss:0.00010711966470986522\n",
      "train loss:0.002405932044210994\n",
      "train loss:0.0008362274022027384\n",
      "train loss:3.6256805515003135e-05\n",
      "train loss:0.00028013973752399814\n",
      "train loss:0.002405801577751828\n",
      "train loss:0.0007488714219730753\n",
      "train loss:0.00039170901827824433\n",
      "train loss:6.845149853019674e-05\n",
      "train loss:0.0015089890164597874\n",
      "train loss:0.0005255126891562447\n",
      "train loss:0.00014108415678481284\n",
      "train loss:0.0003274411449158896\n",
      "train loss:0.00150116027104627\n",
      "train loss:2.2425646388915425e-05\n",
      "train loss:0.00931676030305431\n",
      "train loss:2.7006820250154137e-05\n",
      "train loss:0.0021138018452038147\n",
      "train loss:7.353856593529206e-05\n",
      "train loss:0.008748284380600414\n",
      "train loss:0.0163403000642351\n",
      "train loss:0.002753226815879036\n",
      "train loss:0.00010422291129750216\n",
      "train loss:0.001242142994759552\n",
      "train loss:0.0009304329120966529\n",
      "train loss:0.0007263791451930135\n",
      "train loss:0.0008687597666537694\n",
      "train loss:0.0023602896140803814\n",
      "train loss:0.0027750035283180875\n",
      "train loss:0.00010718283106728593\n",
      "train loss:0.001026632733723719\n",
      "train loss:2.2470178615624007e-05\n",
      "train loss:0.019632756338572355\n",
      "train loss:4.827203304135694e-05\n",
      "train loss:0.0002607808402245496\n",
      "train loss:0.0023989044111170977\n",
      "train loss:0.005201003192732759\n",
      "train loss:6.0183660803514476e-05\n",
      "train loss:0.02293496073391678\n",
      "train loss:0.0007068844684933095\n",
      "train loss:0.00010137928652136651\n",
      "train loss:0.0031582420110010632\n",
      "train loss:0.009386259468489156\n",
      "train loss:0.01115568436839229\n",
      "train loss:0.0009942970605523083\n",
      "train loss:0.07393962373931032\n",
      "train loss:0.0007114008355422138\n",
      "train loss:0.000414598227546546\n",
      "train loss:0.00019463931814779284\n",
      "train loss:9.470868971138562e-05\n",
      "train loss:3.107344068185273e-05\n",
      "train loss:0.002078370707518427\n",
      "train loss:0.00015445188697917766\n",
      "train loss:0.004247361766525235\n",
      "train loss:0.000298130879308817\n",
      "train loss:0.0004024105235482843\n",
      "train loss:0.00363723981672832\n",
      "train loss:0.002463698709124258\n",
      "train loss:0.0008898736788431723\n",
      "train loss:0.002255867300425537\n",
      "train loss:0.006862501275174457\n",
      "train loss:0.0006857867281061912\n",
      "train loss:0.0010578830304406558\n",
      "train loss:0.001578150430955001\n",
      "train loss:0.0016267612576460733\n",
      "train loss:0.004603038937537221\n",
      "train loss:0.0014621892937723513\n",
      "train loss:0.0005993976837041946\n",
      "train loss:0.004884568216164032\n",
      "train loss:0.002417980948914642\n",
      "train loss:0.0008379332099117741\n",
      "train loss:0.00015080256711113882\n",
      "train loss:0.0003492924820062673\n",
      "train loss:0.0016274865762746606\n",
      "train loss:0.0010561030823126483\n",
      "train loss:0.0005725916606141222\n",
      "train loss:0.0010682539400271385\n",
      "train loss:0.0006229312012101809\n",
      "train loss:0.0002049847774871038\n",
      "train loss:0.008241025722911091\n",
      "train loss:0.00018658057803247726\n",
      "train loss:4.941487366082948e-05\n",
      "train loss:0.0007695733468818654\n",
      "train loss:0.003395754655405911\n",
      "train loss:0.013488897607430106\n",
      "train loss:0.0007482402071311823\n",
      "train loss:0.0001871402759502367\n",
      "train loss:0.011538521548140732\n",
      "train loss:7.026021595677237e-05\n",
      "train loss:0.00018427055205850256\n",
      "train loss:0.0008870019011337874\n",
      "train loss:0.0009094374257949756\n",
      "train loss:0.0007700375148148967\n",
      "train loss:0.0008185299872034304\n",
      "train loss:0.0022485424916921172\n",
      "train loss:0.0009558101757013957\n",
      "train loss:0.0008438058288363779\n",
      "train loss:0.002806691205326137\n",
      "train loss:0.001966794039241891\n",
      "train loss:0.000841185671808212\n",
      "train loss:7.95010320001361e-05\n",
      "train loss:0.0018042808732210258\n",
      "train loss:0.002180988324196234\n",
      "train loss:0.0011613370160259056\n",
      "train loss:0.0008762907593258025\n",
      "train loss:0.00018861538234458855\n",
      "train loss:0.0005700363229555095\n",
      "train loss:0.004261220609090896\n",
      "train loss:0.00039595798742684206\n",
      "train loss:0.00018664302437465755\n",
      "train loss:0.0017059292849472703\n",
      "train loss:0.0039955661979391545\n",
      "train loss:0.0003965799343339163\n",
      "train loss:0.0006141155061249192\n",
      "train loss:0.004241246598158586\n",
      "train loss:8.035810457642085e-05\n",
      "train loss:0.0001231250655728404\n",
      "train loss:0.0001411563716158682\n",
      "train loss:0.0021645805747664646\n",
      "train loss:0.0016596301110979588\n",
      "train loss:0.00010484615192566884\n",
      "train loss:0.0002449308295704284\n",
      "train loss:0.0002569209895407763\n",
      "train loss:0.0002901918422985935\n",
      "train loss:0.0027054579464913468\n",
      "train loss:0.00034608514188669347\n",
      "train loss:0.0004862247289299941\n",
      "train loss:0.002829614355426278\n",
      "train loss:0.0018316268842682695\n",
      "train loss:0.00011257888336008944\n",
      "train loss:0.0008087589632158365\n",
      "train loss:0.0024790143743436245\n",
      "train loss:6.412455193518412e-05\n",
      "train loss:0.000696371584081969\n",
      "train loss:0.00022288408152795704\n",
      "train loss:5.321893141203993e-06\n",
      "train loss:0.0010455677177631647\n",
      "train loss:0.0010021664615940606\n",
      "train loss:0.0006953925696887496\n",
      "train loss:0.0017257904805390869\n",
      "train loss:0.001645735222362659\n",
      "train loss:0.0004149399149687398\n",
      "train loss:0.0002025393824717183\n",
      "train loss:0.000351318324531924\n",
      "train loss:0.0007390877442170493\n",
      "train loss:0.001114547665876732\n",
      "train loss:0.00010436582665400287\n",
      "train loss:0.0016709774316458097\n",
      "train loss:0.0007738156931952016\n",
      "train loss:0.0008566665202894787\n",
      "train loss:2.3589650515129225e-05\n",
      "train loss:0.0023138294936775983\n",
      "train loss:6.716828808894151e-05\n",
      "train loss:0.00019239090883348839\n",
      "train loss:0.0006922288786130003\n",
      "train loss:0.0026161515614189245\n",
      "train loss:0.0001643897359591479\n",
      "train loss:0.0041489135087809685\n",
      "train loss:0.0003582969930915996\n",
      "train loss:0.0011785899894623238\n",
      "train loss:3.276979749262035e-05\n",
      "train loss:6.304811167115721e-05\n",
      "train loss:9.825157289588684e-05\n",
      "train loss:0.0006075268284987421\n",
      "train loss:0.0011474473997547137\n",
      "train loss:0.00030479175829727627\n",
      "train loss:0.0036577119419711544\n",
      "train loss:0.0004575872346993291\n",
      "train loss:0.0011234523144922738\n",
      "train loss:0.0008991094048543242\n",
      "train loss:0.001081423982968762\n",
      "train loss:0.0001588766486401234\n",
      "train loss:2.901195886310226e-05\n",
      "train loss:5.621217355706507e-05\n",
      "train loss:4.598312372014221e-05\n",
      "train loss:7.835167816412721e-05\n",
      "train loss:0.0002066591705053978\n",
      "train loss:0.00042907696530462835\n",
      "train loss:0.007859918269551243\n",
      "train loss:0.015073800443099208\n",
      "train loss:0.0008097068312684478\n",
      "train loss:0.0005976737583090422\n",
      "train loss:0.0008228659983241966\n",
      "train loss:0.0002353818997605277\n",
      "train loss:0.00016537470263353174\n",
      "train loss:5.539860324700903e-05\n",
      "train loss:4.7549709035173695e-05\n",
      "train loss:0.001961975901282129\n",
      "train loss:0.0018863447456937135\n",
      "train loss:0.001182341108214051\n",
      "train loss:0.00038508117308044647\n",
      "train loss:0.0011968911912714634\n",
      "train loss:0.017232132246401438\n",
      "train loss:0.0017190473583586544\n",
      "train loss:0.0001946916975994173\n",
      "train loss:0.0025693087237749516\n",
      "train loss:0.02736608522142975\n",
      "train loss:0.0007353452709599204\n",
      "train loss:0.0006356079553966347\n",
      "train loss:0.00027267513286802463\n",
      "train loss:0.0014557353278869983\n",
      "train loss:0.000367862474104228\n",
      "train loss:0.0004298639289091915\n",
      "train loss:0.00011300985225220719\n",
      "train loss:0.0005415644464765182\n",
      "train loss:0.00034694885840132943\n",
      "train loss:0.003314312280750498\n",
      "train loss:0.0008921015840377089\n",
      "train loss:0.00039807560128921407\n",
      "train loss:0.00023168689197468497\n",
      "train loss:0.0002099437392890426\n",
      "train loss:0.000514452985377567\n",
      "train loss:0.0011502243719735234\n",
      "train loss:0.00013723696714098204\n",
      "train loss:6.155673079626971e-05\n",
      "train loss:6.536964672248546e-05\n",
      "train loss:0.0032001001198462557\n",
      "train loss:0.00029441270905696584\n",
      "train loss:0.00032879668070432506\n",
      "train loss:0.0030220497853068163\n",
      "train loss:0.00039561897568480013\n",
      "train loss:0.0009703040090389547\n",
      "train loss:0.0014135919614183098\n",
      "train loss:0.0007140088793709592\n",
      "train loss:0.0007274951446047499\n",
      "train loss:0.004654169426909694\n",
      "train loss:0.0011513859779654057\n",
      "train loss:2.0572382134078008e-05\n",
      "train loss:0.0014834016849251707\n",
      "train loss:0.010563794753722412\n",
      "train loss:2.6160023293883305e-05\n",
      "train loss:0.00019448640748561416\n",
      "train loss:8.705038057555852e-05\n",
      "train loss:0.0004658766480273245\n",
      "train loss:0.001461135643677476\n",
      "train loss:0.00041719437862692383\n",
      "train loss:0.00012789274097607905\n",
      "train loss:7.971137074113854e-05\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9892\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 108;\n                var nbb_unformatted_code = \"from mnist import load_mnist\\nfrom trainer import Trainer\\n\\n(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\\n\\nmax_epochs = 20\\n\\nnetwork = SimpleConvNet(\\n    input_dim=(1, 28, 28),\\n    conv_param={\\\"filter_num\\\": 30, \\\"filter_size\\\": 5, \\\"pad\\\": 0, \\\"stride\\\": 1},\\n    hidden_size=100,\\n    output_size=10,\\n    weight_init_std=0.01,\\n)\\n\\ntrainer = Trainer(\\n    network,\\n    x_train,\\n    t_train,\\n    x_test,\\n    t_test,\\n    epochs=max_epochs,\\n    mini_batch_size=100,\\n    optimizer=\\\"Adam\\\",\\n    optimizer_param={\\\"lr\\\": 0.001},\\n    evaluate_sample_num_per_epoch=1000,\\n)\\ntrainer.train()\";\n                var nbb_formatted_code = \"from mnist import load_mnist\\nfrom trainer import Trainer\\n\\n(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\\n\\nmax_epochs = 20\\n\\nnetwork = SimpleConvNet(\\n    input_dim=(1, 28, 28),\\n    conv_param={\\\"filter_num\\\": 30, \\\"filter_size\\\": 5, \\\"pad\\\": 0, \\\"stride\\\": 1},\\n    hidden_size=100,\\n    output_size=10,\\n    weight_init_std=0.01,\\n)\\n\\ntrainer = Trainer(\\n    network,\\n    x_train,\\n    t_train,\\n    x_test,\\n    t_test,\\n    epochs=max_epochs,\\n    mini_batch_size=100,\\n    optimizer=\\\"Adam\\\",\\n    optimizer_param={\\\"lr\\\": 0.001},\\n    evaluate_sample_num_per_epoch=1000,\\n)\\ntrainer.train()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mnist import load_mnist\n",
    "from trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(\n",
    "    input_dim=(1, 28, 28),\n",
    "    conv_param={\"filter_num\": 30, \"filter_size\": 5, \"pad\": 0, \"stride\": 1},\n",
    "    hidden_size=100,\n",
    "    output_size=10,\n",
    "    weight_init_std=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    network,\n",
    "    x_train,\n",
    "    t_train,\n",
    "    x_test,\n",
    "    t_test,\n",
    "    epochs=max_epochs,\n",
    "    mini_batch_size=100,\n",
    "    optimizer=\"Adam\",\n",
    "    optimizer_param={\"lr\": 0.001},\n",
    "    evaluate_sample_num_per_epoch=1000,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 109;\n                var nbb_unformatted_code = \"def save_params(network, file_name=\\\"params.pkl\\\"):\\n    params = {}\\n    for key, val in network.params.items():\\n        params[key] = val \\n    with open(file_name, \\\"wb\\\") as f:\\n        pickle.dump(params, f)\\n\\ndef load_params(network, file_name=\\\"params.pkl\\\"):\\n    with open(file_name, \\\"rb\\\") as f:\\n        params = pickle.load(f)\\n    for key, val in params.items():\\n        network.params[key] = val \\n\\n    for i, key in enumerate([\\\"Conv1\\\", \\\"Affine1\\\", \\\"Affine2\\\"]):\\n        network.layers[key].w = network.params['w' + str(i + 1)]\\n        network.layers[key].b = network.params['b' + str(i + 1)]\";\n                var nbb_formatted_code = \"def save_params(network, file_name=\\\"params.pkl\\\"):\\n    params = {}\\n    for key, val in network.params.items():\\n        params[key] = val\\n    with open(file_name, \\\"wb\\\") as f:\\n        pickle.dump(params, f)\\n\\n\\ndef load_params(network, file_name=\\\"params.pkl\\\"):\\n    with open(file_name, \\\"rb\\\") as f:\\n        params = pickle.load(f)\\n    for key, val in params.items():\\n        network.params[key] = val\\n\\n    for i, key in enumerate([\\\"Conv1\\\", \\\"Affine1\\\", \\\"Affine2\\\"]):\\n        network.layers[key].w = network.params[\\\"w\\\" + str(i + 1)]\\n        network.layers[key].b = network.params[\\\"b\\\" + str(i + 1)]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_params(network, file_name=\"params.pkl\"):\n",
    "    params = {}\n",
    "    for key, val in network.params.items():\n",
    "        params[key] = val \n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(params, f)\n",
    "\n",
    "def load_params(network, file_name=\"params.pkl\"):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        params = pickle.load(f)\n",
    "    for key, val in params.items():\n",
    "        network.params[key] = val \n",
    "\n",
    "    for i, key in enumerate([\"Conv1\", \"Affine1\", \"Affine2\"]):\n",
    "        network.layers[key].w = network.params['w' + str(i + 1)]\n",
    "        network.layers[key].b = network.params['b' + str(i + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 110;\n                var nbb_unformatted_code = \"save_params(network)\";\n                var nbb_formatted_code = \"save_params(network)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_params(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 113;\n                var nbb_unformatted_code = \"def show_filter(filters, nx=8, margin=3, scale=10):\\n    FN, C, FH, FW = filters.shape\\n    ny = int(np.ceil(FN / nx))\\n\\n    fig = plt.figure()\\n    fig.subplots_adjust(left=0, bottom=0, top=1, hspace=0.05, wspace=0.05)\\n    for i in range(FN):\\n        ax = fig.add_subplot(ny, nx, i + 1, xticks=[], yticks=[])\\n        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation=\\\"nearest\\\")\\n    plt.show()\\n\\nshow_filter(network.params[\\\"w1\\\"])\";\n                var nbb_formatted_code = \"def show_filter(filters, nx=8, margin=3, scale=10):\\n    FN, C, FH, FW = filters.shape\\n    ny = int(np.ceil(FN / nx))\\n\\n    fig = plt.figure()\\n    fig.subplots_adjust(left=0, bottom=0, top=1, hspace=0.05, wspace=0.05)\\n    for i in range(FN):\\n        ax = fig.add_subplot(ny, nx, i + 1, xticks=[], yticks=[])\\n        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation=\\\"nearest\\\")\\n    plt.show()\\n\\n\\nshow_filter(network.params[\\\"w1\\\"])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_filter(filters, nx=8, margin=3, scale=10):\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    plt.show()\n",
    "\n",
    "show_filter(network.params[\"w1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e05b10c8b0164a5a86602a296dece25bd56b72cc470e07d5f2c76b87d4ea3cf2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
